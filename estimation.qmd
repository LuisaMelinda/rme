---
title: "Estimation"
date: last-modified
date-format: "[Last modified:] YYYY-MM-DD: H:mm:ss (A)"
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

# Estimands, estimates, and estimators

{{< include macros.qmd >}}

\index{estimand}

:::{#def-estimand}

## Estimand 

An **estimand** is an unknown quantity whose value we want to know. 

::::{#exm-estimand}

## Mean height of students

If we are trying to determine the mean height of students at our school, then the population mean is our estimand.

::::

In statistical contexts, most estimands are parameters of probabilistic models, or functions of model parameters.

::::{.callout-note}

## Notation for estimands

Model paramaters and other estimands are often symbolized using lower-case Greek letters: $\alpha, \beta, \gamma, \delta$, etc.

::::

:::

 \index{estimate} \index{estimated value}
 
:::{#def-estimate}

## Estimate/estimated value

In statistics, in **estimate** or **estimated value** is an informed guess of an estimand's value, based on observed data.

::::{#exm-estimate}

## Mean height of students

Suppose we measure the heights of 50 random students from our school, and that the the sample mean was 175cm. We might use 175cm as an estimate of the population mean.

::::

:::

\index{estimator}

:::{#def-estimator}

## Estimator 

An **estimator** is a function $\hth(X_1,...X_n)$, whose input is a set of random variables $X_1,...,X_n$, and whose corresponding observed value $\hth(x_1,...x_n)$ is used as an estimate.

:::

:::{.callout-note}
### Notation for estimators

Estimators are often symbolized by placing a "hat" (circumflex) diacritic symbol $\hat\ $ on top of the corresponding estimand; for example, $\hth$.
:::

:::{#exm-estimator}

## Mean height of students

If we want to estimate the mean height of students at our university, which we will represent as $\mu$, we might measure the heights of $n= 50$ randomly sampled students as random variables $X_1,...,X_n$. Then we could use the function

$$\hat\mu(X_1,...,X_n) = \frac{1}{n} \sum_{i=1}^n X_i \eqdef \bar X$$

as an *estimator* to produce an *estimate* $\hat\mu = \bar x$ of $\mu$. 

Another estimator would be just the height of the first student sampled: 

$$\hat\mu^{(2)}(X_1,...,X_n) = X_1$$

A third possible estimator would be the mean of all sampled students' heights, except for the two most extreme; that is, if we re-order the observations $X_{(1)} = \min_{i\in 1:n} X_i$, $X_{(2)} = \min_{i\in \{1:n\} - \arg X_{(1)}} X_i$, ..., $X_{(n)} = \max_{i\in 1:n} X_i$, then we could define the estimator:

$$\hat\mu^{(3)}(X_1,...,X_n) = \frac{1}{n}\sum_{i=2}^{n-1} X_{(i)}$$

Which of these estimators is best? It depends on how we evaluate them (see @sec-est-accuracy below).

:::

:::{.callout-note}

## Contrasting estimands, estimates, and estimators

It's helpful to keep in mind the mathematical type of each estimation concept:

* estimands are numbers (or vector of numbers)
* estimates are also numbers (or vectors)
* estimators are functions of random variables, so they are also random variables

:::

# Accuracy of estimators {#sec-est-accuracy}

To determine which estimator is best, we need to define *best*. "Accuracy" is usually most important; easy computation is usually secondary.

:::{#def-accuracy}
## Accuracy

The **accuracy** of an estimator for a given estimand does not have a consensus formal definition, but all of the usual candidates are related to the distributions of the *errors* made by the resulting estimates.

::::{#def-error}
## Error

The **error** of an estimate $\hth$, often denoted $\e(\hth)$, is the difference between the estimate and its estimand $\theta$; that is:

$$\e(\hth) \eqdef \hth - \th$$

::::

Two frequently-used options for accuracy are:

::::{#def-mse}

### Mean squared error

The **mean squared error** of an estimator is the expectation of the squared errors:

$$
\begin{aligned}
\mselr{\hth} 
&\eqdef \E{(\epsilon(\hth))^2}\\
&= \E{(\hth - \theta)^2}
\end{aligned}
$$

::::

::::{#def-mae}

### Mean absolute error

The **mean absolute error** of an estimator is the expectation of the absolute values of the errors:


$$
\begin{aligned}
\maelr{\hth} 
&\eqdef \E{\abs{\epsilon(\hth)}}\\
&= \E{\abs{\hth - \theta}}
\end{aligned}
$$

::::

::::{#def-bias}

## Bias

The **bias** of an estimator $\hth$ for an estimand $\theta$ is the expected value of the error:

$$
\begin{aligned}
\bias{\hth} 
&\eqdef \E{\epsilon(\hth)}\\
&= \E{\hth - \theta}\\
&=\E{\hth} - \E{\theta}\\
&=\E{\hth} - \theta
\end{aligned}
$$ {#eq-bias-def}

The third equality in @eq-bias-def is by the linearity of expectation.

::::

:::

:::{#def-precision}

## Precision

The **precision** of an estimator $\hth$, often denoted $\tau(\hth)$, is the inverse of that estimator's variance; that is:

$$\tau(\hth) \eqdef \inv{\Var{\hth}}$$

:::

:::{#thm-mse-bias-variance}

## MSE = Bias^2^ + Variance

For any one-dimensional estimator $\hth$:

$$\mselr{\hth} = \sqf{\bias{\hth}} + \var{\hth}$$

::::{.proof}

It's easiest to work backwards:

$$
\ba
\sqf{\bias{\hth}}
&=\sqf{\E{\e(\hth)}}\\
&=\sqf{\E{\hth - \th}}\\
&=\sqf{\E{\hth} - \th}\\
&=\sqf{\E{\hth}} - 2\E{\hth}\th+\th^2\\
\var{\hth}
&=\E{\hth^2} - \sqf{\E{\hth}}\\
\sqf{\bias{\hth}} + \var{\hth}
&=\sqf{\E{\hth}} - 2\E{\hth}\th+\th^2 + \E{\hth^2} - \sqf{\E{\hth}}\\
&=\E{\hth^2} - 2\E{\hth}\th+\th^2\\
&=\E{\hth^2} - 2\E{\hth}\th+\E{\th^2}\\
&= \E{\hth^2 - 2\hth\th - \th^2}\\
&= \E{(\hth - \th)^2}\\
&= \E{(\e(\hth))^2}\\
&=\mselr{\hth} 
\ea
$$
::::

:::
