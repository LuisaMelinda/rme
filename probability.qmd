# Probability

---

{{< include shared-config.qmd >}}

## Random variables

### Types of random variables

:::{#def-binary}

#### binary variable

A **binary variable** is a random variable which has only two possible values in its range.

:::


::: {#exr-binary-examples}


#### Examples of binary variables

What are some examples of binary variables in the health sciences?
:::

----

:::: {.solution}

{{< include binary-outcome-examples.qmd >}}


::::

---


## Key probability distributions

### The Bernoulli distribution

:::::{#def-bernoulli}
#### Bernoulli distribution
{{< include def-bernoulli.qmd >}}
:::::

### The Poisson distribution

:::::{#def-poisson}
#### Poisson distribution

$$P(Y=y) = \frac{\lambda^{y} e^{-\lambda}}{y!}$$
:::::


## Characteristics of probability distributions

:::{#def-density}
### Density function

The density function $f(t)$ or $\p(T=t)$ for a random variable $T$ at value $t$ can be defined as the derivative of the cumulative probability function $P(T\le t)$; that is:

$$f(t) \eqdef \deriv{t} \Pr(T\le t)$$

:::

:::{#def-hazard}

{{< include _def-hazard.qmd >}}

:::

:::{#def-expectation}
### Expectation, expected value, population mean \index{expectation} \index{expected value}

The **expectation**, **expected value**, or **population mean** of a *continuous* random variable $X$, denoted $\E{X}$, $\mu(X)$, or $\mu_X$, is the weighted mean of $X$'s possible values, weighted by the probability density function of those values:

$$\E{X} = \int_{x\in \range{X}} x \cdot \p(X=x)dx$$

The **expectation**, **expected value**, or **population mean** of a *discrete* random variable $X$, denoted $\E{X}$, $\mu(X)$, or $\mu_X$, is the mean of $X$'s possible values, weighted by the probability mass function of those values:

$$\E{X} = \sum_{x \in \range{X}} x \cdot \P(X=x)$$

(c.f. <https://en.wikipedia.org/wiki/Expected_value>)

:::

---

:::{#thm-bernoulli-mean}
#### Expectation of the Bernoulli distribution

The expectation of a Bernoulli random variable with parameter $\pi$ is:

$$\E{X} = \pi$$
:::

---

:::{.proof}

$$
\ba
\E{X}
&= \sum_{x\in \range{X}} x \cd \P(X=x)
\\&= \sum_{x\in \set{0,1}} x \cd \P(X=x)
\\&= \paren{0 \cd \P(X=0)} + \paren{1 \cd \P(X=1)}
\\&= \paren{0 \cd (1-\pi)} + \paren{1 \cd \pi}
\\&= 0 + \pi
\\&= \pi
\ea
$$

:::

---

### Variance and related characteristics

:::{#def-variance}
#### Variance

The variance of a random variable $X$ is the expectation of the squared difference between $X$ and $\E{X}$; that is:

$$
\Var{X} \eqdef \E{(X-\E{X})^2}
$$

:::

---

:::{#thm-variance}
#### Alternative expression for variance

$$\Var{X}=\E{X^2} - \sqf{\E{X}}$$

---

::::{.proof}
By linearity of expectation, we have:

$$
\begin{aligned}
\Var{X} 
&\eqdef \E{(X-\E{X})^2}\\
&=\E{X^2 - 2X\E{X} + \sqf{\E{X}}}\\
&=\E{X^2} - \E{2X\E{X}} + \E{\sqf{\E{X}}}\\
&=\E{X^2} - 2\E{X}\E{X} + \sqf{\E{X}}\\
&=\E{X^2} - \sqf{\E{X}}\\
\end{aligned}
$$
::::

:::

---

::: {#def-precision}
#### Precision

The **precision** of a random variable $X$, often denoted $\tau(X)$, $\tau_X$, or shorthanded as $\tau$, is
the inverse of that random variable's variance; that is:

$$\tau(X) \eqdef \inv{\Var{X}}$$
:::

::: {#def-sd}

#### Standard deviation

The standard deviation of a random variable $X$ is the square-root of the variance of $X$:

$$\SD{X} \eqdef \sqrt{\Var{X}}$$

:::

---

:::{#def-cov}
#### Covariance

For any two one-dimensional random variables, $X,Y$:

$$\Cov{X,Y} \eqdef \Expf{(X - \E X)(Y - \E Y)}$$

:::

---

:::{#thm-alt-cov}
$$\Cov{X,Y}= \E{XY} - \E{X} \E{Y}$$
:::

---

:::{.proof}
Left to the reader.
:::

---

:::{#lem-cov-xx}

##### The covariance of a variable with itself is its variance

For any random variable $X$:

$$\Cov{X,X} = \Var{X}$$

:::

:::{.proof}
$$
\ba
\Cov{X,X} &= E[XX] - E[X]E[X] 
\\ &= E[X^2]-(E[X])^2
\\ &= \Var{X}
\ea
$$
:::

---

:::{#def-cov-vec-x}

#### Variance/covariance of a $p \times 1$ random vector

For a $p \times 1$ dimensional random vector $X$,

$$
\begin{aligned}
\text{Var}(X) 
&\eqdef \text{Cov}(X)\\
&\eqdef E[ \left( X - E\lbrack X\rbrack \right)^{\top}\left( X - E\lbrack X\rbrack \right) ]\\
\ea
$$

:::

---

:::{#thm-vcov-vec}

#### Alternate expression for variance of a random vector

$$
\ba
\Var{X} 
&= E[ X^{\top}X ] - {E\lbrack X\rbrack}^{\top}E\lbrack X\rbrack
\end{aligned}
$$
:::

---

:::{.proof}
$$
\ba
\Var{X} 
&= E[ \left( X^{\top} - E\lbrack X\rbrack^{\top} \right)\left( X - E\lbrack X\rbrack \right) ]\\
&= E[ X^{\top}X - E\lbrack X\rbrack^{\top}X - X^{\top}E\lbrack X\rbrack + E\lbrack X\rbrack^{\top}E\lbrack X\rbrack ]\\
&= E[ X^{\top}X ] - E\lbrack X\rbrack^{\top}E\lbrack X\rbrack - {E\lbrack X\rbrack}^{\top}E\lbrack X\rbrack + E\lbrack X\rbrack^{\top}E\lbrack X\rbrack\\
&= E[ X^{\top}X ] - 2{E\lbrack X\rbrack}^{\top}E\lbrack X\rbrack + E\lbrack X\rbrack^{\top}E\lbrack X\rbrack\\
&= E[ X^{\top}X ] - {E\lbrack X\rbrack}^{\top}E\lbrack X\rbrack
\end{aligned}
$$
:::

---

:::{#thm-var-lincom}

#### Variance of a linear combination

For any set of random variables $\Xin$ and corresponding constants $a_1, ... ,a_n$:

$$\Var{\sumin a_i X_i} = \sumin \sumn{j} a_i a_j \Cov{X_i,X_j}$$
:::

---

:::{.proof}

Left to the reader...

:::

---

:::{#lem-var-lincom2}

For any two random variables $X$ and $Y$ and scalars $a$ and $b$:

$$\Var{aX + bY} = a^2 \Var{X} + b^2 \Var{Y} + 2(a \cd b) \Cov{X,Y}$$

:::

---

:::{.proof}

Apply @thm-var-lincom with $n=2$, $X_1 = X$, and $X_2 = Y$.

Or, see <https://statproofbook.github.io/P/var-lincomb.html>

:::

---

:::{#def-homosked}
### homoskedastic, heteroskedastic

A random variable $Y$ is **homoskedastic** (with respect to covariates $X$) if the variance of $Y$ does not vary with $X$: 

$$\Varr(Y|X=x) = \ss, \forall x$$

Otherwise it is **heteroskedastic**.

:::

---

:::{#def-indpt}

### Statistical independence

A set of random variables $\X1n$ are **statistically independent** 
if their joint probability is equal to the product of their marginal probabilities:

$$\Pr(\Xx1n) = \prodi1n{\Pr(X_i=x_i)}$$

:::
