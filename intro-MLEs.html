<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.551">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Regression Models for Epidemiology - Appendix C — Introduction to Maximum Likelihood Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./common-mistakes.html" rel="next">
<link href="./estimation.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-W625DGE908"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-W625DGE908', { 'anonymize_ip': true});
</script><script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false,
  "enableExperimentalNewNoteButton": true
}
</script><script async="" src="https://hypothes.is/embed.js"></script><script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="custom.scss">
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./probability.html">Appendices</a></li><li class="breadcrumb-item"><a href="./intro-MLEs.html"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Introduction to Maximum Likelihood Inference</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Regression Models for Epidemiology</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/d-morrison/rme" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Regression-Models-for-Epidemiology.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro-to-GLMs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./glms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Generalized Linear Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Linear-models-overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear (Gaussian) Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./logistic-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Models for Binary Outcomes (Logistic regression and variations)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./count-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Models for Count Outcomes (Poisson regression and variations)</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./time-to-event-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Time to Event Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro-to-survival-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introduction to Survival Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./proportional-hazards-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Proportional Hazards Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./coxph-model-building.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Building Cox Proportional Hazards models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./parametric-survival-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Parametric survival models</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro-MLEs.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Introduction to Maximum Likelihood Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./common-mistakes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Common Mistakes</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#maximum-likelihood-inference-for-univariate-gaussian-models" id="toc-maximum-likelihood-inference-for-univariate-gaussian-models" class="nav-link active" data-scroll-target="#maximum-likelihood-inference-for-univariate-gaussian-models"><span class="header-section-number">C.1</span> Maximum likelihood inference for univariate Gaussian models</a>
  <ul class="collapse">
<li><a href="#mle-of-mu" id="toc-mle-of-mu" class="nav-link" data-scroll-target="#mle-of-mu"><span class="header-section-number">C.1.1</span> MLE of <span class="math inline">\(\mu\)</span></a></li>
  <li><a href="#mle-of-sigma2" id="toc-mle-of-sigma2" class="nav-link" data-scroll-target="#mle-of-sigma2"><span class="header-section-number">C.1.2</span> MLE of <span class="math inline">\(\sigma^{2}\)</span></a></li>
  <li><a href="#covariance-matrix-of-mles" id="toc-covariance-matrix-of-mles" class="nav-link" data-scroll-target="#covariance-matrix-of-mles"><span class="header-section-number">C.1.3</span> Covariance matrix of MLEs</a></li>
  <li><a href="#confidence-intervals-for-mles" id="toc-confidence-intervals-for-mles" class="nav-link" data-scroll-target="#confidence-intervals-for-mles"><span class="header-section-number">C.1.4</span> Confidence intervals for MLEs</a></li>
  <li><a href="#p-values-and-hypothesis-tests-for-mles" id="toc-p-values-and-hypothesis-tests-for-mles" class="nav-link" data-scroll-target="#p-values-and-hypothesis-tests-for-mles"><span class="header-section-number">C.1.5</span> p-values and hypothesis tests for MLEs</a></li>
  <li><a href="#likelihood-ratio-tests-for-mles" id="toc-likelihood-ratio-tests-for-mles" class="nav-link" data-scroll-target="#likelihood-ratio-tests-for-mles"><span class="header-section-number">C.1.6</span> Likelihood ratio tests for MLEs</a></li>
  <li><a href="#prediction-intervals-for-mles" id="toc-prediction-intervals-for-mles" class="nav-link" data-scroll-target="#prediction-intervals-for-mles"><span class="header-section-number">C.1.7</span> Prediction intervals for MLEs</a></li>
  </ul>
</li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/d-morrison/rme/edit/main/intro-MLEs.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/d-morrison/rme/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./probability.html">Appendices</a></li><li class="breadcrumb-item"><a href="./intro-MLEs.html"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Introduction to Maximum Likelihood Inference</span></a></li></ol></nav><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Appendix C — Introduction to Maximum Likelihood Inference</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">Last modified: 2024-03-31: 18:14:45 (PM)</p>
    </div>
  </div>
  
    
  </div>
  


</header><p>These notes are derived primarily from <span class="citation" data-cites="dobson2018introduction">Dobson and Barnett (<a href="references.html#ref-dobson2018introduction" role="doc-biblioref">2018</a>)</span> (mostly chapters 3-5).</p>
<p>Some material was also taken from <span class="citation" data-cites="mclachlan2007algorithm">McLachlan and Krishnan (<a href="references.html#ref-mclachlan2007algorithm" role="doc-biblioref">2007</a>)</span> and <span class="citation" data-cites="CaseBerg01">Casella and Berger (<a href="references.html#ref-CaseBerg01" role="doc-biblioref">2002</a>)</span>.</p>
<section id="maximum-likelihood-inference-for-univariate-gaussian-models" class="level2" data-number="C.1"><h2 data-number="C.1" class="anchored" data-anchor-id="maximum-likelihood-inference-for-univariate-gaussian-models">
<span class="header-section-number">C.1</span> Maximum likelihood inference for univariate Gaussian models</h2>
<p>Suppose <span class="math inline">\(X_{1},\ldots,X_{n} \sim_{iid}N\left( \mu,\ \sigma^{2} \right)\)</span>. Let <span class="math inline">\(X = \left( X_{1},\ldots,X_{n} \right)^{\top}\)</span> be these random variables in vector format. Let <span class="math inline">\(x_{i}\)</span> and <span class="math inline">\(x\)</span> denote the corresponding observed data. Let <span class="math inline">\(\theta = \left( \mu,\sigma^{2} \right)^{\top}\)</span> be the vector of parameters. Let <span class="math inline">\(\Theta\)</span> denote the parameters as a random vector.</p>
<p>Then the log-likelihood <span class="math inline">\(\ell \stackrel{\text{def}}{=}\ell(X;\theta) \stackrel{\text{def}}{=}p\left( X = x \mid \Theta = \theta \right)\)</span> is:</p>
<p><span class="math display">\[
\begin{aligned}
\ell
&amp;\propto - \frac{n}{2}\text{log}\left\{\sigma^{2}\right\} - \frac{1}{2}\sum_{i = 1}^{n}\frac{\left( x_{i} - \mu \right)^{2}}{\sigma^{2}}\\
&amp;= - \frac{n}{2}\text{log}\left\{\sigma^{2}\right\} - \frac{1}{2\sigma^{2}}\sum_{i = 1}^{n}{x_{i}^{2} - 2x_{i}\mu + \mu^{2}}
\end{aligned}
\]</span></p>
<section id="mle-of-mu" class="level3" data-number="C.1.1"><h3 data-number="C.1.1" class="anchored" data-anchor-id="mle-of-mu">
<span class="header-section-number">C.1.1</span> MLE of <span class="math inline">\(\mu\)</span>
</h3>
<p>Then:</p>
<p><span class="math display">\[\frac{d\ell}{d\mu} = - \frac{1}{2}\sum_{i = 1}^{n}\frac{- 2\left( x_{i} - \mu \right)}{\sigma^{2}}\]</span></p>
<p><span class="math display">\[= \frac{1}{\sigma^{2}}\left\lbrack \left( \sum_{i = 1}^{n}x_{i} \right) - n\mu \right\rbrack\]</span></p>
<p>If <span class="math inline">\(\frac{d\ell}{d\mu} = 0\)</span>, then <span class="math inline">\(\mu = \overline{x} \stackrel{\text{def}}{=}\frac{1}{n}\sum_{i = 1}^{n}x_{i}\)</span>.</p>
<p><span class="math display">\[\frac{d^{2}\ell}{(d\mu)^{2}} = \frac{- n}{\sigma^{2}} &lt; 0\]</span></p>
<p>So <span class="math inline">\({\widehat{\mu}}_{ML} = \overline{x}\)</span>.</p>
</section><section id="mle-of-sigma2" class="level3" data-number="C.1.2"><h3 data-number="C.1.2" class="anchored" data-anchor-id="mle-of-sigma2">
<span class="header-section-number">C.1.2</span> MLE of <span class="math inline">\(\sigma^{2}\)</span>
</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reparametrizing the Gaussian distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>When solving for <span class="math inline">\({\widehat{\sigma}}_{ML}\)</span>, you can treat <span class="math inline">\(\sigma^{2}\)</span> as an atomic variable (don’t differentiate with respect to <span class="math inline">\(\sigma\)</span> or things get messy). In fact, you can replace <span class="math inline">\(\sigma^{2}\)</span> with <span class="math inline">\(1/\tau\)</span> and differentiate with respect to <span class="math inline">\(\tau\)</span> instead, and the process might be even easier.</p>
</div>
</div>
<p><span class="math display">\[\frac{d\ell}{d\sigma^{2}} = \frac{d}{d\sigma^{2}}\left( - \frac{n}{2}\text{log}\left\{\sigma^{2}\right\} - \frac{1}{2}\sum_{i = 1}^{n}\frac{\left( x_{i} - \mu \right)^{2}}{\sigma^{2}} \right)\ \]</span></p>
<p><span class="math display">\[= - \frac{n}{2}\left( \sigma^{2} \right)^{- 1} + \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2}\]</span></p>
<p>If <span class="math inline">\(\frac{d\ell}{d\sigma^{2}} = 0\)</span>, then:</p>
<p><span class="math display">\[\frac{n}{2}\left( \sigma^{2} \right)^{- 1} = \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2}\]</span></p>
<p><span class="math display">\[\sigma^{2} = \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2}\]</span></p>
<p>We plug in <span class="math inline">\({\widehat{\mu}}_{ML} = \overline{x}\)</span> to maximize globally (a technique called profiling):</p>
<p><span class="math display">\[\sigma_{ML}^{2} = \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}\]</span></p>
<p>Now:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{d^{2}\ell}{\left( d\sigma^{2} \right)^{2}}
&amp;= \frac{d}{d\sigma^{2}}\left\{ - \frac{n}{2}\left( \sigma^{2} \right)^{- 1} + \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right\}\\
&amp;= \left\{ - \frac{n}{2}\frac{d}{d\sigma^{2}}\left( \sigma^{2} \right)^{- 1} + \frac{1}{2}\frac{d}{d\sigma^{2}}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right\}\\
&amp;= \left\{ \frac{n}{2}\left( \sigma^{2} \right)^{- 2} - \left( \sigma^{2} \right)^{- 3}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right\}\\
&amp;= \left( \sigma^{2} \right)^{- 2}\left\{ \frac{n}{2} - \left( \sigma^{2} \right)^{- 1}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right\}
\end{aligned}
\]</span></p>
<p>Evaluated at <span class="math inline">\(\mu = \overline{x},\sigma^{2} = \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}\)</span>, we have:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{d^{2}\ell}{\left( d\sigma^{2} \right)^{2}}
&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}\left\{ \frac{n}{2} - \left( {\widehat{\sigma}}^{2} \right)^{- 1}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2} \right\}\\
&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}\left\{ \frac{n}{2} - \left( {\widehat{\sigma}}^{2} \right)^{- 1}n{\widehat{\sigma}}^{2} \right\}\\
&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}\left\{ \frac{n}{2} - n \right\}\\
&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}n\left\{ \frac{1}{2} - 1 \right\}\\
&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}n\left( - \frac{1}{2} \right) &lt; 0
\end{aligned}
\]</span></p>
<p>Finally, we have:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{d^{2}\ell}{d\mu\ d\sigma^{2}}
&amp;= \frac{d}{d\mu}\left\{ - \frac{n}{2}\left( \sigma^{2} \right)^{- 1} + \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right\}\\
&amp;= \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\frac{d}{d\mu}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2}\\
&amp;= \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}{- 2(x_{i} - \mu)}\\
&amp;= - \left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}{(x_{i} - \mu)}
\end{aligned}
\]</span></p>
<p>Evaluated at <span class="math inline">\(\mu = \widehat{\mu} = \overline{x},\sigma^{2} = {\widehat{\sigma}}^{2} = \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}\)</span>, we have:</p>
<p><span class="math display">\[\frac{d^{2}\ell}{d\mu\ d\sigma^{2}} = - \left( {\widehat{\sigma}}^{2} \right)^{- 2}\left( n\overline{x} - n\overline{x} \right) = 0\]</span></p>
</section><section id="covariance-matrix-of-mles" class="level3" data-number="C.1.3"><h3 data-number="C.1.3" class="anchored" data-anchor-id="covariance-matrix-of-mles">
<span class="header-section-number">C.1.3</span> Covariance matrix of MLEs</h3>
<section id="the-score-function" class="level4"><h4 class="anchored" data-anchor-id="the-score-function">The score function</h4>
<p>Let <span class="math inline">\(\theta\)</span> be the vector of all parameters; here, <span class="math inline">\(\theta = \left( \mu,\sigma^{2} \right)^{\top}\)</span>.</p>
<p>Let <span class="math inline">\(\ell^{'}(x,\theta) \stackrel{\text{def}}{=}\frac{d}{d\theta}\ell(x,\theta) = \left( \begin{array}{r}
\frac{d}{d\mu}\ell(\theta;x) \\
\frac{d}{d\sigma^{2}}\ell(\theta;x)
\end{array} \right) = \left( \begin{array}{r}
\ell_{\mu}^{'}(\theta;x) \\
\ell_{\sigma^{2}}^{'}(\theta;x)
\end{array} \right)\)</span>.</p>
<p><span class="math inline">\(\ell^{'}(x,\theta)\)</span> is the function we set equal to 0 and solve to find the MLE:</p>
<p><span class="math display">\[{\widehat{\theta}}_{ML} = \left\{ \theta:\ell^{'}(x,\theta) = 0 \right\}\]</span></p>
<p>The function <span class="math inline">\(\ell^{'}(x,\theta)\)</span> is so central that it has its own name, the “score” or “gradient” function. Statisticians also often skip writing the arguments <span class="math inline">\((x,\theta)\)</span>, so <span class="math inline">\(\ell^{'} \stackrel{\text{def}}{=}\ell^{'}(x,\theta)\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Some statisticians use <span class="math inline">\(U\)</span> or <span class="math inline">\(S\)</span> instead of <span class="math inline">\(\ell^{'}\)</span>. I prefer <span class="math inline">\(\ell^{'}\)</span>. Why use up extra letters?</p>
</section><section id="the-fisher-expected-information-matrix" class="level4"><h4 class="anchored" data-anchor-id="the-fisher-expected-information-matrix">The (Fisher) (expected) information matrix</h4>
<p>The variance of <span class="math inline">\(\ell^{'}(x,\theta)\)</span>, <span class="math inline">\({Cov}\left\{ \ell^{'}(x,\theta) \right\}\)</span>, is also very important; we call it the “expected information matrix”, “Fisher information matrix”, or just “information matrix”, and we represent it using the symbol <span class="math inline">\(\mathfrak{I}\)</span> (<code>\frakturI</code> in Unicode, <code>\mathfrak{I}</code> in LaTeX).</p>
<section id="review-of-variances-and-covariances" class="level5"><h5 class="anchored" data-anchor-id="review-of-variances-and-covariances">Review of variances and covariances</h5>
<section id="variances-and-covariances-of-one-dimensional-random-variables" class="level6"><h6 class="anchored" data-anchor-id="variances-and-covariances-of-one-dimensional-random-variables">Variances and covariances of one-dimensional random variables</h6>
<p>For a one-dimensional random variables <span class="math inline">\(X\)</span>,</p>
<p><span class="math display">\[{Var}(X) \stackrel{\text{def}}{=}Ε\left\lbrack \left( X - Ε\lbrack X\rbrack \right)^{2} \right\rbrack = Ε\left\lbrack X^{2} \right\rbrack - \left( Ε\lbrack X\rbrack \right)^{2}\]</span></p>
<p>For any two-dimensional random variables, <span class="math inline">\(X,Y\)</span>:</p>
<p><span class="math display">\[{Cov}(X,Y) = E\left\lbrack (X - ΕX)(Y - ΕY) \right\rbrack = Ε\lbrack XY\rbrack - E\lbrack X\rbrack E\lbrack Y\rbrack\]</span></p>
<p>Therefore, <span class="math inline">\({Var}{(X)} = {Cov}(X,X) = Ε\lbrack XX\rbrack - E\lbrack X\rbrack E\lbrack X\rbrack = Ε\left\lbrack X^{2} \right\rbrack - \left( E\lbrack X\rbrack \right)^{2}\)</span></p>
<p><span class="math display">\[\mathfrak{I \stackrel{\text{def}}{=}I(}\theta) \stackrel{\text{def}}{=}{Cov}\left( \ell^{'}|\theta \right) = Ε\left\lbrack \ell^{'}{\ell^{'}}^{\top} \right\rbrack - Ε\left\lbrack \ell^{'} \right\rbrack\ Ε\left\lbrack \ell^{'} \right\rbrack^{\top}\]</span></p>
<p>Sometimes we write <span class="math inline">\({Cov}{(X)} \stackrel{\text{def}}{=}{Cov}(X,X) = {Var}(X)\)</span>.</p>
</section><section id="variances-and-covariances-of-p-times-1-random-vectors" class="level6"><h6 class="anchored" data-anchor-id="variances-and-covariances-of-p-times-1-random-vectors">Variances and covariances of <span class="math inline">\(p \times 1\)</span> random vectors</h6>
<p>Now, for a <span class="math inline">\(p \times 1\)</span> dimensional random vector <span class="math inline">\(X\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
\text{Var}(X)
&amp;\stackrel{\text{def}}{=}\text{Cov}(X)\\
&amp;\stackrel{\text{def}}{=}E\left\lbrack \left( X - E\lbrack X\rbrack \right)^{\top}\left( X - E\lbrack X\rbrack \right) \right\rbrack\\
&amp;= E\left\lbrack \left( X^{\top} - E\lbrack X\rbrack^{\top} \right)\left( X - E\lbrack X\rbrack \right) \right\rbrack\\
&amp;= E\left\lbrack X^{\top}X - E\lbrack X\rbrack^{\top}X - X^{\top}E\lbrack X\rbrack + E\lbrack X\rbrack^{\top}E\lbrack X\rbrack \right\rbrack\\
&amp;= E\left\lbrack X^{\top}X \right\rbrack - E\lbrack X\rbrack^{\top}E\lbrack X\rbrack - {E\lbrack X\rbrack}^{\top}E\lbrack X\rbrack + E\lbrack X\rbrack^{\top}E\lbrack X\rbrack\\
&amp;= E\left\lbrack X^{\top}X \right\rbrack - 2{E\lbrack X\rbrack}^{\top}E\lbrack X\rbrack + E\lbrack X\rbrack^{\top}E\lbrack X\rbrack\\
&amp;= E\left\lbrack X^{\top}X \right\rbrack - {E\lbrack X\rbrack}^{\top}E\lbrack X\rbrack
\end{aligned}
\]</span></p>
<p>The elements of <span class="math inline">\(\mathfrak{I}\)</span> are:</p>
<p><span class="math display">\[\left\{ \mathfrak{I}_{ij} \stackrel{\text{def}}{=}{Cov}\left( {\ell^{'}}_{i},{\ell^{'}}_{j} \right) = Ε\left\lbrack \ell_{i}^{'}\ell_{j}^{'} \right\rbrack - Ε\left\lbrack {\ell^{'}}_{i} \right\rbrack Ε\left\lbrack {\ell^{'}}_{j} \right\rbrack \right\}\]</span></p>
<p>In our motivating example, <span class="math inline">\(i,j \in \left\{ \mu,\sigma^{2} \right\}\)</span>. Here,</p>
<p><span class="math display">\[
\begin{aligned}
Ε[\ell']
&amp;\stackrel{\text{def}}{=}\int_{x \in \mathcal R(x)}{\ell'(x,\theta) p(X = x | \theta)dx}\\
&amp;= \int_{x\in \mathcal R(x)}{\left( \frac{d}{d\theta}\text{log}\left\{p\left( X = x \mid \theta \right)\right\} \right)\ p\left( X = x \mid \theta \right)\ dx}\\
&amp;= \int_{x \in \mathcal R(x)}{\frac{\frac{d}{d\theta}p\left( X = x \mid \theta \right)}{p\left( X = x \mid \theta \right)}p\left( X = x \mid \theta \right)\ dx
}\\
&amp;= \int_{x \in \mathcal R(x)}{\frac{d}{d\theta}p\left( X = x \mid \theta \right)\ dx}
\end{aligned}
\]</span></p>
<p>And similarly</p>
<p><span class="math display">\[Ε\left\lbrack \ell^{'}{\ell^{'}}^{\top} \right\rbrack \stackrel{\text{def}}{=}\int_{x \in R(x)}^{}{\ell^{'}(x,\theta)\ell^{'}(x,\theta)^{\top}\ p\left( X = x \mid \theta \right)\ dx}\]</span></p>
<p>Note that <span class="math inline">\(Ε\left\lbrack \ell^{'} \right\rbrack\)</span> and <span class="math inline">\(Ε\left\lbrack \ell^{'}{\ell^{'}}^{\top} \right\rbrack\)</span> are functions of <span class="math inline">\(\theta\)</span> but not of <span class="math inline">\(x\)</span>; the expectation operator removed <span class="math inline">\(x\)</span>.</p>
<p>Also note that for most of the distributions you are familiar with (including Gaussian, binomial, Poisson, exponential),</p>
<p><span class="math display">\[Ε\left\lbrack \ell^{'} \right\rbrack = 0\]</span></p>
<p>So</p>
<p><span class="math display">\[\mathfrak{I =}Ε\left\lbrack \ell^{'}{\ell^{'}}^{\top} \right\rbrack\]</span></p>
<p>Moreover, for those distributions (called the “exponential family”), we have:</p>
<p><span class="math display">\[\mathfrak{I = -}Ε\left\lbrack \ell^{''} \right\rbrack = Ε\left\lbrack - \ell^{''} \right\rbrack\]</span></p>
<p>(see Dobson and Barnett 4e, §3.17), where</p>
<p><span class="math display">\[\ell^{''} \stackrel{\text{def}}{=}\frac{d}{d\theta}\ell^{'(x,\theta)^{\top}} = \frac{d}{d\theta}\frac{d}{d\theta^{\top}}\ell(x,\theta)\]</span></p>
<p>is the <span class="math inline">\(p \times p\)</span> matrix whose elements are:</p>
<p><span class="math display">\[\ell_{ij}^{''} \stackrel{\text{def}}{=}\frac{d}{d\theta_{i}}\frac{d}{d\theta_{j}}\text{log}\left\{ p\left( X = x \mid \theta \right)\right\}\]</span></p>
<p><span class="math inline">\(\ell^{''}\)</span> could be called the “Hessian” of the log-likelihood function.</p>
<p>Sometimes, we use <span class="math inline">\(I(\theta;x) \stackrel{\text{def}}{=}- \ell^{''}\)</span> (note the standard-font “I” here). <span class="math inline">\(I(\theta;x)\)</span> is the observed information matrix (Negative Hessian).</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key point
</div>
</div>
<div class="callout-body-container callout-body">
<p>The asymptotics of MLEs gives us <span class="math inline">\({\widehat{\theta}}_{ML} \sim N\left( \theta,\mathfrak{I}^{- 1}(\theta) \right)\)</span>, approximately, for large sample sizes.</p>
</div>
</div>
<p>We can estimate <span class="math inline">\(\mathfrak{I}^{- 1}(\theta)\)</span> by working out <span class="math inline">\(- Ε\left\lbrack \ell^{''} \right\rbrack\)</span> or <span class="math inline">\(Ε\left\lbrack \ell^{'}{\ell^{'}}^{\top} \right\rbrack\)</span> and plugging in <span class="math inline">\({\widehat{\theta}}_{ML}\)</span>, but sometimes we instead use <span class="math inline">\(I\left( {\widehat{\theta}}_{ML};x \right)\)</span> for convenience; there are some cases where it’s provably better according to some criteria (Efron &amp; Hinkley 1978).</p>
<p>Note that later, when we are trying to find MLEs for likelihoods that we can’t easily differentiate, we will “hill-climb” using the Newton-Raphson algorithm:</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\theta}
&amp;\leftarrow \widehat{\theta} + \left\lbrack I\left( \widehat{\theta},y \right) \right\rbrack^{- 1}\ell^{'}\left( y,\widehat{\theta} \right)\\
&amp;= \widehat{\theta} - \left\lbrack \ell^{''}\left( y,\widehat{\theta} \right) \right\rbrack^{- 1}\ell^{'}\left( y,\widehat{\theta} \right)
\end{aligned}
\]</span></p>
<p>Here, for computational simplicity, we will sometimes use <span class="math inline">\(\mathfrak{I}^{- 1}(\theta)\)</span> in place of <span class="math inline">\(I\left( \widehat{\theta},y \right)\)</span>; doing so is called “Fisher scoring” or the “method of scoring”. Note that this is the opposite of the substitution that we are making for estimating the variance of the MLE; this time we should technically use the observed information but we use the expected information instead.</p>
<p>There’s also an “empirical information matrix” (see McLachlan and Krishnan 2007).</p>
<p><span class="math display">\[I_{e}(\theta,y) = \sum_{i = 1}^{n}{\ell_{i}^{'}\ {\ell_{i}^{'}}^{\top}} - \frac{1}{n}\ell^{'}{\ell^{'}}^{\top}\]</span></p>
<p>where <span class="math inline">\(\ell_{i}\)</span> is the log-likelihood of the ith observation. Note that <span class="math inline">\(\ell^{'} = \sum_{i = 1}^{n}\ell_{i}^{'}\)</span>.</p>
<p><span class="math inline">\(\frac{1}{n}I_{e}(\theta,y)\)</span> is the sample equivalent of</p>
<p><span class="math display">\[\mathfrak{I \stackrel{\text{def}}{=}I(}\theta) \stackrel{\text{def}}{=}{Cov}\left( \ell^{'}|\theta \right) = Ε\left\lbrack \ell^{'}{\ell^{'}}^{\top} \right\rbrack - Ε\left\lbrack \ell^{'} \right\rbrack\ Ε\left\lbrack \ell^{'} \right\rbrack^{\top}\]</span></p>
<p><span class="math display">\[\left\{ \mathfrak{I}_{jk} \stackrel{\text{def}}{=}{Cov}\left( {\ell^{'}}_{j},{\ell^{'}}_{k} \right) = Ε\left\lbrack \ell_{j}^{'}\ell_{k}^{'} \right\rbrack - Ε\left\lbrack {\ell^{'}}_{j} \right\rbrack Ε\left\lbrack {\ell^{'}}_{k} \right\rbrack \right\}\]</span></p>
<p><span class="math inline">\(I_{e}(\theta,y)\)</span> is sometimes computationally easier to compute for Newton-Raphson-type maximization algorithms.</p>
<p>Back to our Gaussian example:</p>
<p><span class="math display">\[I = \begin{bmatrix}
\frac{n}{\sigma^{2}} &amp; 0 \\
0 &amp; \left( {\widehat{\sigma}}^{2} \right)^{- 2}n\left( - \frac{1}{2} \right)
\end{bmatrix} = \begin{bmatrix}
a &amp; 0 \\
0 &amp; d
\end{bmatrix}\]</span></p>
<p>So:</p>
<p><span class="math display">\[I^{- 1} = \frac{1}{ad}\begin{bmatrix}
d &amp; 0 \\
0 &amp; a
\end{bmatrix} = \begin{bmatrix}
\frac{1}{a} &amp; 0 \\
0 &amp; \frac{1}{d}
\end{bmatrix}\]</span></p>
<p><span class="math display">\[I^{- 1} = \begin{bmatrix}
\frac{{\widehat{\sigma}}^{2}}{n} &amp; 0 \\
0 &amp; \frac{{2\left( {\widehat{\sigma}}^{2} \right)}^{2}}{n}
\end{bmatrix}\]</span></p>
<p>See <span class="citation" data-cites="CaseBerg01">Casella and Berger (<a href="references.html#ref-CaseBerg01" role="doc-biblioref">2002</a>)</span> p322, example 7.2.12.</p>
<p>To prove it’s a maximum, need:</p>
<ul>
<li><p><span class="math inline">\(\ell^{'} = 0\)</span></p></li>
<li><p>At least one diagonal element of <span class="math inline">\(\mathcal{l''}\)</span> is negative.</p></li>
<li><p>Determinant of <span class="math inline">\(\mathcal{l''}\)</span> is positive.</p></li>
</ul></section></section></section></section><section id="confidence-intervals-for-mles" class="level3" data-number="C.1.4"><h3 data-number="C.1.4" class="anchored" data-anchor-id="confidence-intervals-for-mles">
<span class="header-section-number">C.1.4</span> Confidence intervals for MLEs</h3>
</section><section id="p-values-and-hypothesis-tests-for-mles" class="level3" data-number="C.1.5"><h3 data-number="C.1.5" class="anchored" data-anchor-id="p-values-and-hypothesis-tests-for-mles">
<span class="header-section-number">C.1.5</span> p-values and hypothesis tests for MLEs</h3>
</section><section id="likelihood-ratio-tests-for-mles" class="level3" data-number="C.1.6"><h3 data-number="C.1.6" class="anchored" data-anchor-id="likelihood-ratio-tests-for-mles">
<span class="header-section-number">C.1.6</span> Likelihood ratio tests for MLEs</h3>
<p>[We haven’t gone over this yet]</p>
<p>log(likelihood ratio) tests <span class="citation" data-cites="dobson2018introduction">(c.f. <a href="references.html#ref-dobson2018introduction" role="doc-biblioref">Dobson and Barnett 2018, sec. 5.7</a>)</span>:</p>
<p><span class="math display">\[2\left( \mathcal{l -}\ell_{0} \right) \sim \chi^{2}(p - q)\]</span></p>
</section><section id="prediction-intervals-for-mles" class="level3" data-number="C.1.7"><h3 data-number="C.1.7" class="anchored" data-anchor-id="prediction-intervals-for-mles">
<span class="header-section-number">C.1.7</span> Prediction intervals for MLEs</h3>
<p><span class="math display">\[\overline{X} \in \left\lbrack \widehat{\mu} \pm z_{1 - \alpha\text{/}2}\frac{\sigma}{m} \right\rbrack\]</span></p>
<p>Where <span class="math inline">\(m\)</span> is the sample size of the new data to be predicted (typically 1, except for binary outcomes, where it needs to be bigger for prediction intervals to make sense)</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-CaseBerg01" class="csl-entry" role="listitem">
Casella, George, and Roger Berger. 2002. <em>Statistical Inference</em>. 2nd ed. Cengage Learning. <a href="https://www.cengage.com/c/statistical-inference-2e-casella-berger/9780534243128/">https://www.cengage.com/c/statistical-inference-2e-casella-berger/9780534243128/</a>.
</div>
<div id="ref-dobson2018introduction" class="csl-entry" role="listitem">
Dobson, Annette J, and Adrian G Barnett. 2018. <em>An Introduction to Generalized Linear Models</em>. 4th ed. CRC press. <a href="https://doi.org/10.1201/9781315182780">https://doi.org/10.1201/9781315182780</a>.
</div>
<div id="ref-mclachlan2007algorithm" class="csl-entry" role="listitem">
McLachlan, Geoffrey J, and Thriyambakam Krishnan. 2007. <em>The EM Algorithm and Extensions</em>. 2nd ed. John Wiley &amp; Sons. <a href="https://doi.org/10.1002/9780470191613">https://doi.org/10.1002/9780470191613</a>.
</div>
</div>
</section></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>I might sometimes switch the order of <span class="math inline">\(x,\)</span> <span class="math inline">\(\theta\)</span>; this is unintentional and not meaningful.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./estimation.html" class="pagination-link" aria-label="Estimation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Estimation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./common-mistakes.html" class="pagination-link" aria-label="Common Mistakes">
        <span class="nav-page-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Common Mistakes</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction to Maximum Likelihood Inference</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>{{&lt; include shared-config.qmd &gt;}}</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>These notes are derived primarily from @dobson2018introduction (mostly chapters 3-5).</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>Some material was also taken from @mclachlan2007algorithm and @CaseBerg01.</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">## Maximum likelihood inference for univariate Gaussian models</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>Suppose $X_{1},\ldots,X_{n} \sim_{iid}N\left( \mu,\ \sigma^{2} \right)$.</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>Let $X = \left( X_{1},\ldots,X_{n} \right)^{\top}$ be these random</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>variables in vector format. Let $x_{i}$ and $x$ denote the corresponding</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>observed data. Let $\theta = \left( \mu,\sigma^{2} \right)^{\top}$ be</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>the vector of parameters. Let $\Theta$ denote the parameters as a random</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>vector.</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>Then the log-likelihood</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>$\ell \eqdef \ell(X;\theta) \eqdef p\left( X = x \mid \Theta = \theta \right)$</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>is:</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>\ell </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>&amp;\propto - \frac{n}{2}\log{\sigma^{2}} - \frac{1}{2}\sum_{i = 1}^{n}\frac{\left( x_{i} - \mu \right)^{2}}{\sigma^{2}}<span class="sc">\\</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>&amp;= - \frac{n}{2}\log{\sigma^{2}} - \frac{1}{2\sigma^{2}}\sum_{i = 1}^{n}{x_{i}^{2} - 2x_{i}\mu + \mu^{2}}</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="fu">### MLE of $\mu$</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>Then:</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>$$\frac{d\ell}{d\mu} = - \frac{1}{2}\sum_{i = 1}^{n}\frac{- 2\left( x_{i} - \mu \right)}{\sigma^{2}}$$</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>$$= \frac{1}{\sigma^{2}}\left\lbrack \left( \sum_{i = 1}^{n}x_{i} \right) - n\mu \right\rbrack$$</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>If $\frac{d\ell}{d\mu} = 0$, then</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>$\mu = \overline{x} \eqdef \frac{1}{n}\sum_{i = 1}^{n}x_{i}$.</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>$$\frac{d^{2}\ell}{(d\mu)^{2}} = \frac{- n}{\sigma^{2}} &lt; 0$$</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>So ${\widehat{\mu}}_{ML} = \overline{x}$.</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="fu">### MLE of $\sigma^{2}$</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>:::{.callout-tip}</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="fu">### Reparametrizing the Gaussian distribution</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>When solving for ${\widehat{\sigma}}_{ML}$, you can treat</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>$\sigma^{2}$ as an atomic variable (don’t differentiate with respect to</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>$\sigma$ or things get messy). In fact, you can replace $\sigma^{2}$</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>with $1/\tau$ and differentiate with respect to $\tau$ instead, and the</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>process might be even easier.</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>$$\frac{d\ell}{d\sigma^{2}} = \frac{d}{d\sigma^{2}}\left( - \frac{n}{2}\log{\sigma^{2}} - \frac{1}{2}\sum_{i = 1}^{n}\frac{\left( x_{i} - \mu \right)^{2}}{\sigma^{2}} \right)\ $$</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>$$= - \frac{n}{2}\left( \sigma^{2} \right)^{- 1} + \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2}$$</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>If $\frac{d\ell}{d\sigma^{2}} = 0$, then:</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>$$\frac{n}{2}\left( \sigma^{2} \right)^{- 1} = \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2}$$</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>$$\sigma^{2} = \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2}$$</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>We plug in ${\widehat{\mu}}_{ML} = \overline{x}$ to maximize globally (a</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>technique called profiling):</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>$$\sigma_{ML}^{2} = \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}$$</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>Now:</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>\frac{d^{2}\ell}{\left( d\sigma^{2} \right)^{2}} </span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{d}{d\sigma^{2}}\left<span class="sc">\{</span> - \frac{n}{2}\left( \sigma^{2} \right)^{- 1} + \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right<span class="sc">\}\\</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>&amp;= \left<span class="sc">\{</span> - \frac{n}{2}\frac{d}{d\sigma^{2}}\left( \sigma^{2} \right)^{- 1} + \frac{1}{2}\frac{d}{d\sigma^{2}}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right<span class="sc">\}\\</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>&amp;= \left<span class="sc">\{</span> \frac{n}{2}\left( \sigma^{2} \right)^{- 2} - \left( \sigma^{2} \right)^{- 3}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right<span class="sc">\}\\</span></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>&amp;= \left( \sigma^{2} \right)^{- 2}\left<span class="sc">\{</span> \frac{n}{2} - \left( \sigma^{2} \right)^{- 1}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right<span class="sc">\}</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>Evaluated at</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>$\mu = \overline{x},\sigma^{2} = \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}$,</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>we have:</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>\frac{d^{2}\ell}{\left( d\sigma^{2} \right)^{2}} </span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}\left<span class="sc">\{</span> \frac{n}{2} - \left( {\widehat{\sigma}}^{2} \right)^{- 1}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2} \right<span class="sc">\}\\</span></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}\left<span class="sc">\{</span> \frac{n}{2} - \left( {\widehat{\sigma}}^{2} \right)^{- 1}n{\widehat{\sigma}}^{2} \right<span class="sc">\}\\</span></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}\left<span class="sc">\{</span> \frac{n}{2} - n \right<span class="sc">\}\\</span></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}n\left<span class="sc">\{</span> \frac{1}{2} - 1 \right<span class="sc">\}\\</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}n\left( - \frac{1}{2} \right) &lt; 0</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>Finally, we have:</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>\frac{d^{2}\ell}{d\mu\ d\sigma^{2}} </span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{d}{d\mu}\left<span class="sc">\{</span> - \frac{n}{2}\left( \sigma^{2} \right)^{- 1} + \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right<span class="sc">\}\\</span></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\frac{d}{d\mu}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2}<span class="sc">\\</span></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}{- 2(x_{i} - \mu)}<span class="sc">\\</span></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>&amp;= - \left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}{(x_{i} - \mu)}</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>Evaluated at</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>$\mu = \widehat{\mu} = \overline{x},\sigma^{2} = {\widehat{\sigma}}^{2} = \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}$,</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>we have:</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>$$\frac{d^{2}\ell}{d\mu\ d\sigma^{2}} = - \left( {\widehat{\sigma}}^{2} \right)^{- 2}\left( n\overline{x} - n\overline{x} \right) = 0$$</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a><span class="fu">### Covariance matrix of MLEs</span></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The score function</span></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>Let $\theta$ be the vector of all parameters; here,</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>$\theta = \left( \mu,\sigma^{2} \right)^{\top}$.</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>Let</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>$\ell^{'}(x,\theta) \eqdef \frac{d}{d\theta}\ell(x,\theta) = \left( \begin{array}{r}</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>\frac{d}{d\mu}\ell(\theta;x) <span class="sc">\\</span></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>\frac{d}{d\sigma^{2}}\ell(\theta;x)</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>\end{array} \right) = \left( \begin{array}{r}</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>\ell_{\mu}^{'}(\theta;x) <span class="sc">\\</span></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>\ell_{\sigma^{2}}^{'}(\theta;x)</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>\end{array} \right)$.</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>$\ell^{'}(x,\theta)$ is the function we set equal to 0 and solve</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>to find the MLE:</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>$${\widehat{\theta}}_{ML} = \left<span class="sc">\{</span> \theta:\ell^{'}(x,\theta) = 0 \right<span class="sc">\}</span>$$</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>The function $\ell^{'}(x,\theta)$ is so central that it has its</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>own name, the "score" or "gradient" function. Statisticians also often</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>skip writing the arguments $(x,\theta)$, so</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>$\ell^{'} \eqdef \ell^{'}(x,\theta)$.<span class="ot">[^1]</span> Some statisticians</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>use $U$ or $S$ instead of $\ell^{'}$. I prefer $\ell^{'}$.</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>Why use up extra letters?</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The (Fisher) (expected) information matrix</span></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>The variance of $\ell^{'}(x,\theta)$,</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>${Cov}\left<span class="sc">\{</span> \ell^{'}(x,\theta) \right<span class="sc">\}</span>$, is also very</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>important; we call it the "expected information matrix", "Fisher</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>information matrix", or just "information matrix", and we represent it</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>using the symbol $\mathfrak{I}$ (<span class="in">`\frakturI`</span> in Unicode, <span class="in">`\mathfrak{I}`</span> in LaTeX).</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a><span class="fu">##### Review of variances and covariances</span></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a><span class="fu">###### Variances and covariances of one-dimensional random variables</span></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>For a one-dimensional random variables $X$,</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>$${Var}(X) \eqdef Ε\left\lbrack \left( X - Ε\lbrack X\rbrack \right)^{2} \right\rbrack = Ε\left\lbrack X^{2} \right\rbrack - \left( Ε\lbrack X\rbrack \right)^{2}$$</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>For any two-dimensional random variables, $X,Y$:</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>$${Cov}(X,Y) = E\left\lbrack (X - ΕX)(Y - ΕY) \right\rbrack = Ε\lbrack XY\rbrack - E\lbrack X\rbrack E\lbrack Y\rbrack$$</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>Therefore,</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>${Var}{(X)} = {Cov}(X,X) = Ε\lbrack XX\rbrack - E\lbrack X\rbrack E\lbrack X\rbrack = Ε\left\lbrack X^{2} \right\rbrack - \left( E\lbrack X\rbrack \right)^{2}$</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>$$\mathfrak{I \eqdef I(}\theta) \eqdef {Cov}\left( \ell^{'}|\theta \right) = Ε\left\lbrack \ell^{'}{\ell^{'}}^{\top} \right\rbrack - Ε\left\lbrack \ell^{'} \right\rbrack\ Ε\left\lbrack \ell^{'} \right\rbrack^{\top}$$</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>Sometimes we write ${Cov}{(X)} \eqdef {Cov}(X,X) = {Var}(X)$.</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a><span class="fu">###### Variances and covariances of $p \times 1$ random vectors</span></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>Now, for a $p \times 1$ dimensional random vector $X$,</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>\text{Var}(X) </span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>&amp;\eqdef \text{Cov}(X)<span class="sc">\\</span></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>&amp;\eqdef E\left\lbrack \left( X - E\lbrack X\rbrack \right)^{\top}\left( X - E\lbrack X\rbrack \right) \right\rbrack<span class="sc">\\</span></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>&amp;= E\left\lbrack \left( X^{\top} - E\lbrack X\rbrack^{\top} \right)\left( X - E\lbrack X\rbrack \right) \right\rbrack<span class="sc">\\</span></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>&amp;= E\left\lbrack X^{\top}X - E\lbrack X\rbrack^{\top}X - X^{\top}E\lbrack X\rbrack + E\lbrack X\rbrack^{\top}E\lbrack X\rbrack \right\rbrack<span class="sc">\\</span></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>&amp;= E\left\lbrack X^{\top}X \right\rbrack - E\lbrack X\rbrack^{\top}E\lbrack X\rbrack - {E\lbrack X\rbrack}^{\top}E\lbrack X\rbrack + E\lbrack X\rbrack^{\top}E\lbrack X\rbrack<span class="sc">\\</span></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>&amp;= E\left\lbrack X^{\top}X \right\rbrack - 2{E\lbrack X\rbrack}^{\top}E\lbrack X\rbrack + E\lbrack X\rbrack^{\top}E\lbrack X\rbrack<span class="sc">\\</span></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>&amp;= E\left\lbrack X^{\top}X \right\rbrack - {E\lbrack X\rbrack}^{\top}E\lbrack X\rbrack</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>The elements of $\mathfrak{I}$ are:</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>$$\left<span class="sc">\{</span> \mathfrak{I}_{ij} \eqdef {Cov}\left( {\ell^{'}}_{i},{\ell^{'}}_{j} \right) = Ε\left\lbrack \ell_{i}^{'}\ell_{j}^{'} \right\rbrack - Ε\left\lbrack {\ell^{'}}_{i} \right\rbrack Ε\left\lbrack {\ell^{'}}_{j} \right\rbrack \right<span class="sc">\}</span>$$</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>In our motivating example, $i,j \in \left<span class="sc">\{</span> \mu,\sigma^{2} \right<span class="sc">\}</span>$. Here, </span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>Ε<span class="co">[</span><span class="ot">\ell'</span><span class="co">]</span></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>&amp;\eqdef \int_{x \in \mathcal R(x)}{\ell'(x,\theta) p(X = x | \theta)dx}<span class="sc">\\</span></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>&amp;= \int_{x\in \mathcal R(x)}{\left( \frac{d}{d\theta}\log{p\left( X = x \mid \theta \right)} \right)\ p\left( X = x \mid \theta \right)\ dx}<span class="sc">\\</span></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>&amp;= \int_{x \in \mathcal R(x)}{\frac{\frac{d}{d\theta}p\left( X = x \mid \theta \right)}{p\left( X = x \mid \theta \right)}p\left( X = x \mid \theta \right)\ dx</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>}<span class="sc">\\</span></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>&amp;= \int_{x \in \mathcal R(x)}{\frac{d}{d\theta}p\left( X = x \mid \theta \right)\ dx}</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>And similarly</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>$$Ε\left\lbrack \ell^{'}{\ell^{'}}^{\top} \right\rbrack \eqdef \int_{x \in R(x)}^{}{\ell^{'}(x,\theta)\ell^{'}(x,\theta)^{\top}\ p\left( X = x \mid \theta \right)\ dx}$$</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>Note that $Ε\left\lbrack \ell^{'} \right\rbrack$ and</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>$Ε\left\lbrack \ell^{'}{\ell^{'}}^{\top} \right\rbrack$</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>are functions of $\theta$ but not of $x$; the expectation operator</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>removed $x$.</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>Also note that for most of the distributions you are familiar with</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>(including Gaussian, binomial, Poisson, exponential),</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>$$Ε\left\lbrack \ell^{'} \right\rbrack = 0$$</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>So</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>$$\mathfrak{I =}Ε\left\lbrack \ell^{'}{\ell^{'}}^{\top} \right\rbrack$$</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>Moreover, for those distributions (called the "exponential family"), we</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>have:</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>$$\mathfrak{I = -}Ε\left\lbrack \ell^{''} \right\rbrack = Ε\left\lbrack - \ell^{''} \right\rbrack$$</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>(see Dobson and Barnett 4e, §3.17), where</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>$$\ell^{''} \eqdef \frac{d}{d\theta}\ell^{'(x,\theta)^{\top}} = \frac{d}{d\theta}\frac{d}{d\theta^{\top}}\ell(x,\theta)$$</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>is the $p \times p$ matrix whose elements are:</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>$$\ell_{ij}^{''} \eqdef \frac{d}{d\theta_{i}}\frac{d}{d\theta_{j}}\log{ p\left( X = x \mid \theta \right)}$$</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>$\ell^{''}$ could be called the "Hessian" of the log-likelihood</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>function.</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>Sometimes, we use $I(\theta;x) \eqdef - \ell^{''}$ (note the</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>standard-font "I" here). $I(\theta;x)$ is the observed information</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>matrix (Negative Hessian).</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>:::{.callout-important}</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key point</span></span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a> The asymptotics of MLEs gives us</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>${\widehat{\theta}}_{ML} \sim N\left( \theta,\mathfrak{I}^{- 1}(\theta) \right)$,</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>approximately, for large sample sizes.</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>We can estimate $\mathfrak{I}^{- 1}(\theta)$ by working out</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>$- Ε\left\lbrack \ell^{''} \right\rbrack$ or</span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>$Ε\left\lbrack \ell^{'}{\ell^{'}}^{\top} \right\rbrack$</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>and plugging in ${\widehat{\theta}}_{ML}$, but sometimes we instead use</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>$I\left( {\widehat{\theta}}_{ML};x \right)$ for convenience; there are</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>some cases where it’s provably better according to some criteria (Efron</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>&amp; Hinkley 1978).</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>Note that later, when we are trying to find MLEs for likelihoods that we</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>can’t easily differentiate, we will "hill-climb" using the</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>Newton-Raphson algorithm:</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>\widehat{\theta} </span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>&amp;\leftarrow \widehat{\theta} + \left\lbrack I\left( \widehat{\theta},y \right) \right\rbrack^{- 1}\ell^{'}\left( y,\widehat{\theta} \right)<span class="sc">\\</span></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a>&amp;= \widehat{\theta} - \left\lbrack \ell^{''}\left( y,\widehat{\theta} \right) \right\rbrack^{- 1}\ell^{'}\left( y,\widehat{\theta} \right)</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>Here, for computational simplicity, we will sometimes use</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>$\mathfrak{I}^{- 1}(\theta)$ in place of</span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>$I\left( \widehat{\theta},y \right)$; doing so is called "Fisher</span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>scoring" or the "method of scoring". Note that this is the opposite of</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a>the substitution that we are making for estimating the variance of the</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>MLE; this time we should technically use the observed information but we</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a>use the expected information instead.</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a>There’s also an "empirical information matrix" (see McLachlan and</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>Krishnan 2007).</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a>$$I_{e}(\theta,y) = \sum_{i = 1}^{n}{\ell_{i}^{'}\ {\ell_{i}^{'}}^{\top}} - \frac{1}{n}\ell^{'}{\ell^{'}}^{\top}$$</span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>where $\ell_{i}$ is the log-likelihood of the ith observation.</span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>Note that $\ell^{'} = \sum_{i = 1}^{n}\ell_{i}^{'}$.</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>$\frac{1}{n}I_{e}(\theta,y)$ is the sample equivalent of</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>$$\mathfrak{I \eqdef I(}\theta) \eqdef {Cov}\left( \ell^{'}|\theta \right) = Ε\left\lbrack \ell^{'}{\ell^{'}}^{\top} \right\rbrack - Ε\left\lbrack \ell^{'} \right\rbrack\ Ε\left\lbrack \ell^{'} \right\rbrack^{\top}$$</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>$$\left<span class="sc">\{</span> \mathfrak{I}_{jk} \eqdef {Cov}\left( {\ell^{'}}_{j},{\ell^{'}}_{k} \right) = Ε\left\lbrack \ell_{j}^{'}\ell_{k}^{'} \right\rbrack - Ε\left\lbrack {\ell^{'}}_{j} \right\rbrack Ε\left\lbrack {\ell^{'}}_{k} \right\rbrack \right<span class="sc">\}</span>$$</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>$I_{e}(\theta,y)$ is sometimes computationally easier to compute for</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>Newton-Raphson-type maximization algorithms.</span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>Back to our Gaussian example:</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a>$$I = \begin{bmatrix}</span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>\frac{n}{\sigma^{2}} &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>0 &amp; \left( {\widehat{\sigma}}^{2} \right)^{- 2}n\left( - \frac{1}{2} \right)</span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>\end{bmatrix} = \begin{bmatrix}</span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>a &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>0 &amp; d</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}$$</span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a>So:</span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>$$I^{- 1} = \frac{1}{ad}\begin{bmatrix}</span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>d &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>0 &amp; a</span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>\end{bmatrix} = \begin{bmatrix}</span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a>\frac{1}{a} &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>0 &amp; \frac{1}{d}</span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}$$</span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a>$$I^{- 1} = \begin{bmatrix}</span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a>\frac{{\widehat{\sigma}}^{2}}{n} &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a>0 &amp; \frac{{2\left( {\widehat{\sigma}}^{2} \right)}^{2}}{n}</span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}$$</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>See @CaseBerg01 p322, example 7.2.12.</span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>To prove it’s a maximum, need:</span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\ell^{'} = 0$</span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>At least one diagonal element of $\mathcal{l''}$ is negative.</span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Determinant of $\mathcal{l''}$ is positive.</span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a><span class="fu">### Confidence intervals for MLEs</span></span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a><span class="fu">### p-values and hypothesis tests for MLEs</span></span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a><span class="fu">### Likelihood ratio tests for MLEs</span></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a><span class="sc">\[</span>We haven’t gone over this yet<span class="sc">\]</span></span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a>log(likelihood ratio) tests <span class="co">[</span><span class="ot">c.f. @dobson2018introduction §5.7</span><span class="co">]</span>:</span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a>$$2\left( \mathcal{l -}\ell_{0} \right) \sim \chi^{2}(p - q)$$</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a><span class="fu">### Prediction intervals for MLEs</span></span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a>$$\overline{X} \in \left\lbrack \widehat{\mu} \pm z_{1 - \alpha\text{/}2}\frac{\sigma}{m} \right\rbrack$$</span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a>Where $m$ is the sample size of the new data to be predicted (typically</span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a>1, except for binary outcomes, where it needs to be bigger for</span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a>prediction intervals to make sense)</span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a><span class="ot">[^1]: </span>I might sometimes switch the order of $x,$ $\theta$; this is</span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a>    unintentional and not meaningful.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
<li class="nav-item">
 Copyright 2023, Douglas Ezra Morrison
  </li>  
</ul>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/d-morrison/rme/edit/main/intro-MLEs.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/d-morrison/rme/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>


</body></html>