<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Regression Models for Epidemiology - Appendix C — Introduction to Maximum Likelihood Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./common-mistakes.html" rel="next">
<link href="./estimation.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-W625DGE908"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-W625DGE908', { 'anonymize_ip': true});
</script><script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false,
  "enableExperimentalNewNoteButton": true
}
</script><script async="" src="https://hypothes.is/embed.js"></script><script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script><link href="site_libs/table1-1.0/table1_defaults.css" rel="stylesheet">
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="custom.scss">
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./probability.html">Appendices</a></li><li class="breadcrumb-item"><a href="./intro-MLEs.html"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Introduction to Maximum Likelihood Inference</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Regression Models for Epidemiology</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/d-morrison/rme" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Regression-Models-for-Epidemiology.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro-to-GLMs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./glms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Generalized Linear Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Linear-models-overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear (Gaussian) Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./logistic-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Models for Binary Outcomes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./count-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Models for Count Outcomes</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./time-to-event-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Time to Event Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro-to-survival-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introduction to Survival Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./proportional-hazards-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Proportional Hazards Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./coxph-model-building.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Building Cox Proportional Hazards models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./parametric-survival-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Parametric survival models</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro-MLEs.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Introduction to Maximum Likelihood Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./common-mistakes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Common Mistakes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Notation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro-to-R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Statistical computing in R</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#overview-of-maximum-likelihood-estimation" id="toc-overview-of-maximum-likelihood-estimation" class="nav-link active" data-scroll-target="#overview-of-maximum-likelihood-estimation"><span class="header-section-number">C.1</span> Overview of maximum likelihood estimation</a>
  <ul class="collapse">
<li><a href="#the-likelihood-function" id="toc-the-likelihood-function" class="nav-link" data-scroll-target="#the-likelihood-function"><span class="header-section-number">C.1.1</span> The likelihood function</a></li>
  <li><a href="#the-maximum-likelihood-estimate" id="toc-the-maximum-likelihood-estimate" class="nav-link" data-scroll-target="#the-maximum-likelihood-estimate"><span class="header-section-number">C.1.2</span> The maximum likelihood estimate</a></li>
  <li><a href="#finding-the-maximum-of-a-function" id="toc-finding-the-maximum-of-a-function" class="nav-link" data-scroll-target="#finding-the-maximum-of-a-function"><span class="header-section-number">C.1.3</span> Finding the maximum of a function</a></li>
  <li><a href="#directly-maximizing-the-likelihood-function-for-iid-data" id="toc-directly-maximizing-the-likelihood-function-for-iid-data" class="nav-link" data-scroll-target="#directly-maximizing-the-likelihood-function-for-iid-data"><span class="header-section-number">C.1.4</span> Directly maximizing the likelihood function for <em>iid</em> data</a></li>
  <li><a href="#the-log-likelihood-function" id="toc-the-log-likelihood-function" class="nav-link" data-scroll-target="#the-log-likelihood-function"><span class="header-section-number">C.1.5</span> The log-likelihood function</a></li>
  <li><a href="#the-score-function" id="toc-the-score-function" class="nav-link" data-scroll-target="#the-score-function"><span class="header-section-number">C.1.6</span> The score function</a></li>
  <li><a href="#asymptotic-distribution-of-the-maximum-likelihood-estimate" id="toc-asymptotic-distribution-of-the-maximum-likelihood-estimate" class="nav-link" data-scroll-target="#asymptotic-distribution-of-the-maximum-likelihood-estimate"><span class="header-section-number">C.1.7</span> Asymptotic distribution of the maximum likelihood estimate</a></li>
  <li><a href="#the-fisher-expected-information-matrix" id="toc-the-fisher-expected-information-matrix" class="nav-link" data-scroll-target="#the-fisher-expected-information-matrix"><span class="header-section-number">C.1.8</span> The (Fisher) (expected) information matrix</a></li>
  <li><a href="#gradient-ascent" id="toc-gradient-ascent" class="nav-link" data-scroll-target="#gradient-ascent"><span class="header-section-number">C.1.9</span> Gradient ascent</a></li>
  </ul>
</li>
  <li>
<a href="#example-maximum-likelihood-for-tropical-cyclones-in-australia" id="toc-example-maximum-likelihood-for-tropical-cyclones-in-australia" class="nav-link" data-scroll-target="#example-maximum-likelihood-for-tropical-cyclones-in-australia"><span class="header-section-number">C.2</span> Example: Maximum likelihood for Tropical Cyclones in Australia</a>
  <ul class="collapse">
<li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data"><span class="header-section-number">C.2.1</span> Data</a></li>
  <li><a href="#exploratory-analysis" id="toc-exploratory-analysis" class="nav-link" data-scroll-target="#exploratory-analysis"><span class="header-section-number">C.2.2</span> Exploratory analysis</a></li>
  <li><a href="#model" id="toc-model" class="nav-link" data-scroll-target="#model"><span class="header-section-number">C.2.3</span> Model</a></li>
  <li><a href="#estimating-the-model-parameters-using-maximum-likelihood" id="toc-estimating-the-model-parameters-using-maximum-likelihood" class="nav-link" data-scroll-target="#estimating-the-model-parameters-using-maximum-likelihood"><span class="header-section-number">C.2.4</span> Estimating the model parameters using maximum likelihood</a></li>
  </ul>
</li>
  <li>
<a href="#maximum-likelihood-inference-for-univariate-gaussian-models" id="toc-maximum-likelihood-inference-for-univariate-gaussian-models" class="nav-link" data-scroll-target="#maximum-likelihood-inference-for-univariate-gaussian-models"><span class="header-section-number">C.3</span> Maximum likelihood inference for univariate Gaussian models</a>
  <ul class="collapse">
<li><a href="#the-score-function-1" id="toc-the-score-function-1" class="nav-link" data-scroll-target="#the-score-function-1"><span class="header-section-number">C.3.1</span> The score function</a></li>
  <li><a href="#mle-of-mu" id="toc-mle-of-mu" class="nav-link" data-scroll-target="#mle-of-mu"><span class="header-section-number">C.3.2</span> MLE of <span class="math inline">\(\mu\)</span></a></li>
  <li><a href="#mle-of-sigma2" id="toc-mle-of-sigma2" class="nav-link" data-scroll-target="#mle-of-sigma2"><span class="header-section-number">C.3.3</span> MLE of <span class="math inline">\(\sigma^{2}\)</span></a></li>
  <li><a href="#covariance-matrix" id="toc-covariance-matrix" class="nav-link" data-scroll-target="#covariance-matrix"><span class="header-section-number">C.3.4</span> Covariance matrix</a></li>
  </ul>
</li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/d-morrison/rme/edit/main/intro-MLEs.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/d-morrison/rme/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./probability.html">Appendices</a></li><li class="breadcrumb-item"><a href="./intro-MLEs.html"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Introduction to Maximum Likelihood Inference</span></a></li></ol></nav><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-intro-MLEs" class="quarto-section-identifier">Appendix C — Introduction to Maximum Likelihood Inference</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">Last modified: 2024-04-22: 22:18:36 (PM)</p>
    </div>
  </div>
  
    
  </div>
  


</header><p>These notes are derived primarily from <span class="citation" data-cites="dobson4e">Dobson and Barnett (<a href="references.html#ref-dobson4e" role="doc-biblioref">2018</a>)</span> (mostly chapters 1-5).</p>
<p>Some material was also taken from <span class="citation" data-cites="mclachlan2007algorithm">McLachlan and Krishnan (<a href="references.html#ref-mclachlan2007algorithm" role="doc-biblioref">2007</a>)</span> and <span class="citation" data-cites="CaseBerg01">Casella and Berger (<a href="references.html#ref-CaseBerg01" role="doc-biblioref">2002</a>)</span>.</p>
<section id="overview-of-maximum-likelihood-estimation" class="level2" data-number="C.1"><h2 data-number="C.1" class="anchored" data-anchor-id="overview-of-maximum-likelihood-estimation">
<span class="header-section-number">C.1</span> Overview of maximum likelihood estimation</h2>
<section id="the-likelihood-function" class="level3" data-number="C.1.1"><h3 data-number="C.1.1" class="anchored" data-anchor-id="the-likelihood-function">
<span class="header-section-number">C.1.1</span> The likelihood function</h3>
<div id="def-lik" class="theorem definition">
<p><span class="theorem-title"><strong>Definition C.1 (Likelihood)</strong></span> Let <span class="math inline">\(\tilde{x}\)</span> be a dataset with corresponding random variable <span class="math inline">\(\tilde{X}\)</span>. Let <span class="math inline">\(\text{p}_{\Theta}(\tilde{X})\)</span> be a probability model for the distribution of <span class="math inline">\(\tilde{X}\)</span> with parameter vector <span class="math inline">\(\Theta\)</span>.</p>
<p>Then the <strong>likelihood</strong> of parameter value <span class="math inline">\(\theta\)</span>, for model <span class="math inline">\(\text{p}_{\Theta}(X)\)</span> and data <span class="math inline">\(\tilde{X}= \tilde{x}\)</span>, is the <em>joint probability</em> of <span class="math inline">\(\tilde{x}\)</span> given <span class="math inline">\(\Theta= \theta\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal{L}(\theta) &amp;\stackrel{\text{def}}{=}p_{\theta}(\tilde{X}= \tilde{x})
\\&amp;=p_{\theta}(X_1=x_1, ..., X_n = x_n)
\end{aligned}
\]</span></p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation for the likelihood function
</div>
</div>
<div class="callout-body-container callout-body">
<p>The likelihood function can be written as:</p>
<ul>
<li><span class="math inline">\(\mathcal{L}(\theta)\)</span></li>
<li><span class="math inline">\(\mathcal{L}(\tilde{x};\theta)\)</span></li>
<li><span class="math inline">\(\mathcal{L}(\theta; \tilde{x})\)</span></li>
<li><span class="math inline">\(\mathcal{L}_{\tilde{x}}(\theta)\)</span></li>
<li><span class="math inline">\(\mathcal{L}_{\theta}(\tilde{x})\)</span></li>
<li><span class="math inline">\(\mathcal{L}(\tilde{x} | \theta)\)</span></li>
</ul>
<p>All of these notations mean the same thing.</p>
</div>
</div>
<div class="notes">
<p>The likelihood is a function that takes <span class="math inline">\(\theta\)</span> (and implicitly, <span class="math inline">\(\tilde{X}\)</span>) as inputs and outputs a single number, the joint probability of <span class="math inline">\(\tilde{x}\)</span> for model <span class="math inline">\(p_\Theta(\tilde{X}=\tilde{x})\)</span> with <span class="math inline">\(\Theta = \theta\)</span>.</p>
</div>
<hr>
<div id="thm-lik-iid" class="theorem">
<p><span class="theorem-title"><strong>Theorem C.1 (Likelihood of an independent sample)</strong></span> For <a href="./probability.html#def-indpt">mutually independent</a> data <span class="math inline">\(X_1, ..., X_n\)</span>:</p>
<p><span id="eq-Lik"><span class="math display">\[\mathcal{L}(\tilde{x}|\theta) = \prod_{i=1}^n \text{p}(X_i=x_i|\theta) \tag{C.1}\]</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{aligned}
\mathcal{L}(\tilde{x}|\theta)
&amp;\stackrel{\text{def}}{=}\text{p}(X_1 = x_1, …,X_n =x_n|\theta)
\\&amp;= \prod_{i=1}^n \text{p}(X_i=x_i|\theta)
\end{aligned}
\]</span> The second equality is by the definition of statistical independence.</p>
</div>
</section><section id="the-maximum-likelihood-estimate" class="level3" data-number="C.1.2"><h3 data-number="C.1.2" class="anchored" data-anchor-id="the-maximum-likelihood-estimate">
<span class="header-section-number">C.1.2</span> The maximum likelihood estimate</h3>
<div id="def-mle" class="theorem definition">
<p><span class="theorem-title"><strong>Definition C.2 (Maximum likelihood estimate)</strong></span> The <strong>maximum likelihood estimate</strong> of a parameter vector <span class="math inline">\(\Theta\)</span>, denoted <span class="math inline">\(\hat\theta_{\text{ML}}\)</span>, is the value of <span class="math inline">\(\Theta\)</span> that maximizes the likelihood:</p>
<p><span id="eq-mle"><span class="math display">\[
\hat\theta_{\text{ML}}\stackrel{\text{def}}{=}\arg \max_\Theta\mathcal{L}(\Theta)
\tag{C.2}\]</span></span></p>
</div>
</section><section id="finding-the-maximum-of-a-function" class="level3" data-number="C.1.3"><h3 data-number="C.1.3" class="anchored" data-anchor-id="finding-the-maximum-of-a-function">
<span class="header-section-number">C.1.3</span> Finding the maximum of a function</h3>
<p>Recall from calculus: the maxima of a continuous function <span class="math inline">\(f(x)\)</span> over a range of input values <span class="math inline">\(\mathcal{R}\left(x\right)\)</span> can be found either:</p>
<ul>
<li>at the edges of the range of input values, <em>OR</em>:</li>
<li>where the function is flat (i.e.&nbsp;where the gradient function <span class="math inline">\(f'(x) = 0\)</span>) <em>AND</em> the second derivative is negative definite (<span class="math inline">\(f''(x) &lt; 0\)</span>).</li>
</ul></section><section id="directly-maximizing-the-likelihood-function-for-iid-data" class="level3" data-number="C.1.4"><h3 data-number="C.1.4" class="anchored" data-anchor-id="directly-maximizing-the-likelihood-function-for-iid-data">
<span class="header-section-number">C.1.4</span> Directly maximizing the likelihood function for <em>iid</em> data</h3>
<p>To find the maximizer(s) of the likelihood function, we need to solve <span class="math inline">\(\mathcal{L}'(\theta) = 0\)</span> for <span class="math inline">\(\theta\)</span>. However, even for mutually independent data, we quickly run into a problem:</p>
<p><span id="eq-deriv-Lik"><span class="math display">\[
\begin{aligned}
\mathcal{L}'(\theta)
&amp;= \frac{d}{d\theta} \mathcal{L}(\theta)
\\ &amp;= \frac{d}{d\theta} \prod_{i=1}^n p(X_i=x_i|\theta)
\end{aligned}
\tag{C.3}\]</span></span></p>
<p>The derivative of the likelihood of independent data is the derivative of a product. We will have to perform a massive application of the product rule to evaluate this derivative.</p>
</section><section id="the-log-likelihood-function" class="level3" data-number="C.1.5"><h3 data-number="C.1.5" class="anchored" data-anchor-id="the-log-likelihood-function">
<span class="header-section-number">C.1.5</span> The log-likelihood function</h3>
<p>It is typically easier to work with the log of the likelihood function:</p>
<div id="def-loglik" class="theorem definition">
<p><span class="theorem-title"><strong>Definition C.3 (Log-likelihood)</strong></span> The <strong>log-likelihood</strong> of parameter value <span class="math inline">\(\theta\)</span>, for model <span class="math inline">\(\text{p}_{\Theta}(\tilde{X})\)</span> and data <span class="math inline">\(\tilde{X}= \tilde{x}\)</span>, is the natural logarithm of the likelihood<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>:</p>
<p><span class="math display">\[\ell(\theta) \stackrel{\text{def}}{=}\text{log}\left\{\mathcal{L}(\theta)\right\}\]</span></p>
</div>
<div id="thm-mle-use-log" class="theorem">
<p><span class="theorem-title"><strong>Theorem C.2</strong></span> The likelihood and log-likelihood have the same maximizer:</p>
<p><span class="math display">\[
\arg \max_\theta\mathcal{L}(\theta) = \arg \max_\theta\ell(\theta)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Left to the reader.</p>
</div>
<hr>
<div id="thm-llik-iid" class="theorem">
<p><span class="theorem-title"><strong>Theorem C.3 (Log-likelihood of an iid sample)</strong></span> For iid data <span class="math inline">\(X_1, ..., X_n\)</span> with shared distribution <span class="math inline">\(\text{p}(X=x)\)</span>: <span id="eq-loglik"><span class="math display">\[
\begin{aligned}
\ell(x|\theta)
&amp;\stackrel{\text{def}}{=}\text{log}\left\{\mathcal L(x|\theta)\right\}
\\&amp;= \sum_{i=1}^n \text{log}\left\{p(X=x_i|\theta)\right\}
\end{aligned}
\tag{C.4}\]</span></span></p>
</div>
<section id="derivative-of-the-log-likelihood-function-for-iid-data" class="level4"><h4 class="anchored" data-anchor-id="derivative-of-the-log-likelihood-function-for-iid-data">Derivative of the log-likelihood function for <em>iid</em> data</h4>
<p>For iid data, we will have a much easier time taking the derivative of the log-likelihood:</p>
<p><span id="eq-deriv-llik"><span class="math display">\[
\begin{aligned}
\ell'(\theta)
&amp;= \frac{d}{d\theta} \ell(\theta)
\\ &amp;= \frac{d}{d\theta} \sum_{i=1}^n \text{log}\left\{\text{p}(X=x_i|\theta)\right\}
\\ &amp;= \sum_{i=1}^n \frac{d}{d\theta} \text{log}\left\{\text{p}(X=x_i|\theta)\right\}
\end{aligned}
\tag{C.5}\]</span></span></p>
</section></section><section id="the-score-function" class="level3" data-number="C.1.6"><h3 data-number="C.1.6" class="anchored" data-anchor-id="the-score-function">
<span class="header-section-number">C.1.6</span> The score function</h3>
<p>The first derivative of the log-likelihood, <span class="math inline">\(\ell'(\theta)\)</span>, is important enough to have its own name and symbol: the <em>score function</em>, <span class="math inline">\(U(\theta)\)</span>.</p>
<div id="def-score" class="theorem definition">
<p><span class="theorem-title"><strong>Definition C.4 (Score function)</strong></span> The <strong>score function</strong> of a statistical model <span class="math inline">\(\text{p}(\tilde{X}=\tilde{x})\)</span> is the gradient (i.e., first derivative) of the log-likelihood of that model:</p>
<p><span class="math display">\[\ell'(\theta) \stackrel{\text{def}}{=}\frac{d}{d\theta} \ell(\theta)\]</span></p>
</div>
<div class="notes">
<p>We often skip writing the arguments <span class="math inline">\(x\)</span> and/or <span class="math inline">\(\theta)\)</span>, so <span class="math inline">\(\ell^{'} \stackrel{\text{def}}{=}\ell^{'}(\tilde{x};\theta) \ell^{'}(\theta)\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> Some statisticians use <span class="math inline">\(U\)</span> or <span class="math inline">\(S\)</span> instead of <span class="math inline">\(\ell^{'}\)</span>. I prefer <span class="math inline">\(\ell^{'}\)</span>. Why use up extra letters?</p>
</div>
</section><section id="asymptotic-distribution-of-the-maximum-likelihood-estimate" class="level3" data-number="C.1.7"><h3 data-number="C.1.7" class="anchored" data-anchor-id="asymptotic-distribution-of-the-maximum-likelihood-estimate">
<span class="header-section-number">C.1.7</span> Asymptotic distribution of the maximum likelihood estimate</h3>
<div class="notes">
<p>We learned how to quantify our uncertainty about these maximum likelihood estimates; with sufficient sample size, <span class="math inline">\(\hat\theta_{\text{ML}}\)</span> has the approximate distribution:</p>
</div>
<p><span class="math display">\[
\hat\theta_{ML} \dot \sim N(\theta,\mathcal I(\theta)^{-1})
\]</span></p>
<p>Recall:</p>
<ul>
<li><span class="math inline">\(\mathcal{I}(\theta) \stackrel{\text{def}}{=}\mathbb{E}\left[I(\tilde{X};\theta)\right]\)</span></li>
<li><span class="math inline">\(I(\tilde{X},\theta) \stackrel{\text{def}}{=}-\ell''(\tilde{X};\theta)\)</span></li>
</ul>
<p>We can estimate <span class="math inline">\(\mathcal{I}(\theta)\)</span> using either <span class="math inline">\(\mathcal{I}(\hat\theta_{\text{ML}})\)</span> or <span class="math inline">\(I(\tilde{x}; \hat\theta_{\text{ML}})\)</span>.</p>
<p>So we can estimate the standard error of <span class="math inline">\(\hat\theta_k\)</span> as:</p>
<p><span class="math display">\[
\widehat{\text{SE}}\left(\hat\theta_k\right) = \sqrt{\left[\left(\hat{\mathcal{I}}\left(\hat\theta_{\text{ML}}\right)\right)^{-1}\right]_{kk}}
\]</span></p>
</section><section id="the-fisher-expected-information-matrix" class="level3" data-number="C.1.8"><h3 data-number="C.1.8" class="anchored" data-anchor-id="the-fisher-expected-information-matrix">
<span class="header-section-number">C.1.8</span> The (Fisher) (expected) information matrix</h3>
<p>The variance of <span class="math inline">\(\ell^{'}(x,\theta)\)</span>, <span class="math inline">\({Cov}\left\{ \ell^{'}(x,\theta) \right\}\)</span>, is also very important; we call it the “expected information matrix”, “Fisher information matrix”, or just “information matrix”, and we represent it using the symbol <span class="math inline">\(\mathfrak{I}\)</span> (<code>\frakturI</code> in Unicode, <code>\mathfrak{I}</code> in LaTeX).</p>
<p><span class="math display">\[\mathfrak{I \stackrel{\text{def}}{=}I(}\theta) \stackrel{\text{def}}{=}{Cov}\left( \ell^{'}|\theta \right) = E\left\lbrack \ell^{'}{\ell^{'}}^{\top} \right\rbrack - E\left\lbrack \ell^{'} \right\rbrack\ E\left\lbrack \ell^{'} \right\rbrack^{\top}
\]</span></p>
<p>The elements of <span class="math inline">\(\mathfrak{I}\)</span> are:</p>
<p><span class="math display">\[\left\{ \mathfrak{I}_{ij} \stackrel{\text{def}}{=}{Cov}\left( {\ell^{'}}_{i},{\ell^{'}}_{j} \right) = E\left\lbrack \ell_{i}^{'}\ell_{j}^{'} \right\rbrack - E\left\lbrack {\ell^{'}}_{i} \right\rbrack E\left\lbrack {\ell^{'}}_{j} \right\rbrack \right\}\]</span></p>
<p>Here,</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[\ell'\right]
&amp;\stackrel{\text{def}}{=}\int_{x \in \mathcal{R}\left(x\right)}
{
\ell'(x,\theta) \text{p}(X = x | \theta) dx
}
\\ &amp;= \int_{x \in \mathcal{R}\left(X\right)}
{
\left(
\frac{d}{d\theta}
\text{log}\left\{\text{p}(X = x | \theta)\right\}
\right)
\text{p}(X = x | \theta) dx
}
\\ &amp;=
\int_{x \in \mathcal{R}\left(X\right)}
{
\frac
{\frac{d}{d\theta} \text{p}(X = x | \theta)}
{\text{p}(X = x | \theta)}
\text{p}(X = x | \theta) dx
}
\\ &amp;=
\int_{x \in \mathcal{R}\left(X\right)}
{
\frac{d}{d\theta} \text{p}(X = x | \theta) dx
}
\end{aligned}
\]</span></p>
<p>And similarly</p>
<p><span class="math display">\[
\mathbb{E}\left[\ell' \ell'^{\top}\right]
\stackrel{\text{def}}{=}
\int_{x \in R(x)}
{\ell'(x,\theta)\ell'(x,\theta)^{\top}\
\text{p}\left(X = x | \theta\right)\ dx}
\]</span></p>
<p>Note that <span class="math inline">\(\mathbb{E}\left[\ell'\right]\)</span> and <span class="math inline">\(\mathbb{E}\left[\ell'{\ell'}^{\top}\right]\)</span> are functions of <span class="math inline">\(\theta\)</span> but not of <span class="math inline">\(x\)</span>; the expectation operator removed <span class="math inline">\(x\)</span>.</p>
<p>Also note that for most of the distributions you are familiar with (including Gaussian, binomial, Poisson, exponential):</p>
<p><span class="math display">\[\mathbb{E}\left[\ell^{'}\right] = 0\]</span></p>
<p>So</p>
<p><span class="math display">\[\mathcal{I}\left(\theta\right) = \mathbb{E}\left[\ell^{'}{\ell^{'}}^{\top} \right]\]</span></p>
<p>Moreover, for those distributions (called the “exponential family”), we have:</p>
<p><span class="math display">\[
\mathfrak{I} = -\mathbb{E}\left[\ell''\right]
= \mathbb{E}\left[- \ell''\right]
\]</span></p>
<p>(see <span class="citation" data-cites="dobson4e">Dobson and Barnett (<a href="references.html#ref-dobson4e" role="doc-biblioref">2018</a>)</span>, §3.17), where</p>
<p><span class="math display">\[\ell^{''} \stackrel{\text{def}}{=}\frac{d}{d\theta}\ell^{'(x,\theta)^{\top}} = \frac{d}{d\theta}\frac{d}{d\theta^{\top}}\ell(x,\theta)\]</span></p>
<p>is the <span class="math inline">\(p \times p\)</span> matrix whose elements are:</p>
<p><span class="math display">\[\ell_{ij}'' \stackrel{\text{def}}{=}\frac{d}{d\theta_{i}}\frac{d}{d\theta_{j}}\text{log}\left\{ p\left( X = x \mid \theta \right)\right\}\]</span></p>
<p><span class="math inline">\(\ell''\)</span> could be called the “Hessian” of the log-likelihood function.</p>
<p>Sometimes, we use <span class="math inline">\(I(\theta;x) \stackrel{\text{def}}{=}- \ell^{''}\)</span> (note the standard-font “I” here). <span class="math inline">\(I(\theta;x)\)</span> is the observed information, precision, or concentration matrix (Negative Hessian).</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key point
</div>
</div>
<div class="callout-body-container callout-body">
<p>The asymptotics of MLEs gives us <span class="math inline">\({\widehat{\theta}}_{ML} \sim N\left( \theta,\mathfrak{I}^{- 1}(\theta) \right)\)</span>, approximately, for large sample sizes.</p>
</div>
</div>
<p>We can estimate <span class="math inline">\(\mathcal{I}^{- 1}(\theta)\)</span> by working out <span class="math inline">\(\mathbb{E}\left[-\ell''\right]\)</span> or <span class="math inline">\(\mathbb{E}\left[\ell^{'}{\ell^{'}}^{\top}\right]\)</span> and plugging in <span class="math inline">\(\hat\theta_{\text{ML}}\)</span>, but sometimes we instead use <span class="math inline">\(I(\hat\theta_{\text{ML}},\tilde{x})\)</span> for convenience; there are some cases where it’s provably better according to some criteria (<span class="citation" data-cites="efron1978assessing">Efron and Hinkley (<a href="references.html#ref-efron1978assessing" role="doc-biblioref">1978</a>)</span>).</p>
</section><section id="gradient-ascent" class="level3" data-number="C.1.9"><h3 data-number="C.1.9" class="anchored" data-anchor-id="gradient-ascent">
<span class="header-section-number">C.1.9</span> Gradient ascent</h3>
<p>Note that later, when we are trying to find MLEs for likelihoods that we can’t easily differentiate, we will “hill-climb” using the Newton-Raphson algorithm:</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\theta}
&amp;\leftarrow \widehat{\theta} + \left\lbrack I\left( \widehat{\theta},y \right) \right\rbrack^{- 1}\ell^{'}\left( y,\widehat{\theta} \right)\\
&amp;= \widehat{\theta} - \left\lbrack \ell^{''}\left( y,\widehat{\theta} \right) \right\rbrack^{- 1}\ell^{'}\left( y,\widehat{\theta} \right)
\end{aligned}
\]</span></p>
<p>Here, for computational simplicity, we will sometimes use <span class="math inline">\(\mathfrak{I}^{- 1}(\theta)\)</span> in place of <span class="math inline">\(I\left( \widehat{\theta},y \right)\)</span>; doing so is called “Fisher scoring” or the “method of scoring”. Note that this is the opposite of the substitution that we are making for estimating the variance of the MLE; this time we should technically use the observed information but we use the expected information instead.</p>
<p>There’s also an “empirical information matrix” (see McLachlan and Krishnan 2007).</p>
<p><span class="math display">\[I_{e}(\theta,y) = \sum_{i = 1}^{n}{\ell_{i}^{'}\ {\ell_{i}^{'}}^{\top}} - \frac{1}{n}\ell^{'}{\ell^{'}}^{\top}\]</span></p>
<p>where <span class="math inline">\(\ell_{i}\)</span> is the log-likelihood of the ith observation. Note that <span class="math inline">\(\ell^{'} = \sum_{i = 1}^{n}\ell_{i}^{'}\)</span>.</p>
<p><span class="math inline">\(\frac{1}{n}I_{e}(\theta,y)\)</span> is the sample equivalent of</p>
<p><span class="math display">\[\mathfrak{I \stackrel{\text{def}}{=}I(}\theta) \stackrel{\text{def}}{=}{Cov}\left( \ell^{'}|\theta \right) = E\left\lbrack \ell^{'}{\ell^{'}}^{\top} \right\rbrack - E\left\lbrack \ell^{'} \right\rbrack\ E\left\lbrack \ell^{'} \right\rbrack^{\top}\]</span></p>
<p><span class="math display">\[\left\{ \mathfrak{I}_{jk} \stackrel{\text{def}}{=}{Cov}\left( {\ell^{'}}_{j},{\ell^{'}}_{k} \right) = E\left\lbrack \ell_{j}^{'}\ell_{k}^{'} \right\rbrack - E\left\lbrack {\ell^{'}}_{j} \right\rbrack E\left\lbrack {\ell^{'}}_{k} \right\rbrack \right\}\]</span></p>
<p><span class="math inline">\(I_{e}(\theta,y)\)</span> is sometimes computationally easier to compute for Newton-Raphson-type maximization algorithms.</p>
<section id="confidence-intervals-for-mles" class="level4"><h4 class="anchored" data-anchor-id="confidence-intervals-for-mles">Confidence intervals for MLEs</h4>
<p>An asymptotic approximation of a 95% confidence interval for <span class="math inline">\(\theta_k\)</span> is</p>
<p><span class="math display">\[
\hat\theta_{\text{ML}}\pm z_{0.975} \times \widehat{\text{SE}}\left(\hat\theta_k\right)
\]</span></p>
<p>where <span class="math inline">\(z_\beta\)</span> the <span class="math inline">\(\beta\)</span> quantile of the standard Gaussian distribution.</p>
</section><section id="p-values-and-hypothesis-tests-for-mles" class="level4"><h4 class="anchored" data-anchor-id="p-values-and-hypothesis-tests-for-mles">p-values and hypothesis tests for MLEs</h4>
<p>(to add)</p>
</section><section id="likelihood-ratio-tests-for-mles" class="level4"><h4 class="anchored" data-anchor-id="likelihood-ratio-tests-for-mles">Likelihood ratio tests for MLEs</h4>
<p>log(likelihood ratio) tests <span class="citation" data-cites="dobson4e">(c.f. <a href="references.html#ref-dobson4e" role="doc-biblioref">Dobson and Barnett 2018, sec. 5.7</a>)</span>:</p>
<p><span class="math display">\[-2\ell_{0} \sim \chi^{2}(p - q)\]</span></p>
<p>See also <a href="https://online.stat.psu.edu/stat504/book/export/html/657" class="uri">https://online.stat.psu.edu/stat504/book/export/html/657</a></p>
</section><section id="prediction-intervals-for-mles" class="level4"><h4 class="anchored" data-anchor-id="prediction-intervals-for-mles">Prediction intervals for MLEs</h4>
<p><span class="math display">\[\overline{X} \in \left\lbrack \widehat{\mu} \pm z_{1 - \alpha\text{/}2}\frac{\sigma}{m} \right\rbrack\]</span></p>
<p>Where <span class="math inline">\(m\)</span> is the sample size of the new data to be predicted (typically 1, except for binary outcomes, where it needs to be bigger for prediction intervals to make sense)</p>
</section></section></section><section id="example-maximum-likelihood-for-tropical-cyclones-in-australia" class="level2" data-number="C.2"><h2 data-number="C.2" class="anchored" data-anchor-id="example-maximum-likelihood-for-tropical-cyclones-in-australia">
<span class="header-section-number">C.2</span> Example: Maximum likelihood for Tropical Cyclones in Australia</h2>
<p>(Adapted from <span class="citation" data-cites="dobson4e">Dobson and Barnett (<a href="references.html#ref-dobson4e" role="doc-biblioref">2018</a>)</span> §1.6.5)</p>
<section id="data" class="level3" data-number="C.2.1"><h3 data-number="C.2.1" class="anchored" data-anchor-id="data">
<span class="header-section-number">C.2.1</span> Data</h3>
<p>The <code>cyclones</code> dataset in the <code>dobson</code> package (<a href="#tbl-cyclones-data" class="quarto-xref">Table&nbsp;<span>C.1</span></a>) records the number of tropical cyclones in Northeastern Australia during 13 November-to-April cyclone seasons (more details in <span class="citation" data-cites="dobson4e">Dobson and Barnett (<a href="references.html#ref-dobson4e" role="doc-biblioref">2018</a>)</span> §1.6.5 and <code><a href="https://rdrr.io/pkg/dobson/man/cyclones.html">help(cyclones, package = "dobson")</a></code>). <a href="#fig-dobson-cyclone-time-series" class="quarto-xref">Figure&nbsp;<span>C.1</span></a> graphs the number of cyclones (y-axis) by season (x-axis). Let’s use <span class="math inline">\(Y_i\)</span> to represent these counts, where <span class="math inline">\(i\)</span> is an indexing variable for the seasons and <span class="math inline">\(Y_i\)</span> is the number of cyclones in season <span class="math inline">\(i\)</span>.</p>
</section><section id="exploratory-analysis" class="level3" data-number="C.2.2"><h3 data-number="C.2.2" class="anchored" data-anchor-id="exploratory-analysis">
<span class="header-section-number">C.2.2</span> Exploratory analysis</h3>
<p>Suppose we want to learn about how many cyclones to expect per season.</p>
<div class="cell">
<details class="code-fold"><summary>Show R code</summary><div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">dobson</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">cyclones</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://rapporter.github.io/pander/">pander</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/pander/man/pander.html">pander</a></span><span class="op">(</span><span class="va">cyclones</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/relocate.html">relocate</a></span><span class="op">(</span><span class="va">season</span>, .before <span class="op">=</span> <span class="fu"><a href="https://tidyselect.r-lib.org/reference/everything.html">everything</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div id="tbl-cyclones-data" class="cell quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-cyclones-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;C.1: Number of tropical cyclones during a season from November to April in Northeastern Australia
</figcaption><div aria-describedby="tbl-cyclones-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell table table-sm table-striped small">
<colgroup>
<col style="width: 12%">
<col style="width: 13%">
<col style="width: 13%">
</colgroup>
<thead><tr class="header">
<th style="text-align: center;">season</th>
<th style="text-align: center;">years</th>
<th style="text-align: center;">number</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1956/7</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">1957/8</td>
<td style="text-align: center;">5</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">1958/9</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">1959/60</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;">1960/1</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="even">
<td style="text-align: center;">6</td>
<td style="text-align: center;">1961/2</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="odd">
<td style="text-align: center;">7</td>
<td style="text-align: center;">1962/3</td>
<td style="text-align: center;">12</td>
</tr>
<tr class="even">
<td style="text-align: center;">8</td>
<td style="text-align: center;">1963/4</td>
<td style="text-align: center;">7</td>
</tr>
<tr class="odd">
<td style="text-align: center;">9</td>
<td style="text-align: center;">1964/5</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="even">
<td style="text-align: center;">10</td>
<td style="text-align: center;">1965/6</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td style="text-align: center;">11</td>
<td style="text-align: center;">1966/7</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="even">
<td style="text-align: center;">12</td>
<td style="text-align: center;">1967/8</td>
<td style="text-align: center;">7</td>
</tr>
<tr class="odd">
<td style="text-align: center;">13</td>
<td style="text-align: center;">1968/9</td>
<td style="text-align: center;">4</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<div class="cell">
<details class="code-fold"><summary>Show R code</summary><div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span><span class="va">cyclones</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>years <span class="op">=</span> <span class="va">years</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span>level <span class="op">=</span> <span class="va">years</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">years</span>, </span>
<span>        y <span class="op">=</span> <span class="va">number</span>,</span>
<span>        group <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"Season"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"Number of cyclones"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/expand_limits.html">expand_limits</a></span><span class="op">(</span>y <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>axis.text.x <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html">element_text</a></span><span class="op">(</span>vjust <span class="op">=</span> <span class="fl">.5</span>, angle <span class="op">=</span> <span class="fl">45</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<div id="fig-dobson-cyclone-time-series" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-dobson-cyclone-time-series-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;C.1: Number of tropical cyclones per season in northeastern Australia, 1956-1969
</figcaption><div aria-describedby="fig-dobson-cyclone-time-series-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="intro-MLEs_files/figure-html/fig-dobson-cyclone-time-series-1.png" class="img-fluid figure-img" width="672">
</div>
</figure>
</div>
</div>
</div>
<p>There’s no obvious correlation between adjacent seasons, so let’s assume that each season is independent of the others.</p>
<p>Let’s also assume that they are identically distributed; let’s denote this distribution as <span class="math inline">\(P(Y=y)\)</span> (note that there’s no index <span class="math inline">\(i\)</span> in this expression, since we are assuming the <span class="math inline">\(Y_i\)</span>s are identically distributed). We can visualize the distribution using a bar plot (<a href="#fig-cyclones-bar-plot" class="quarto-xref">Figure&nbsp;<span>C.2</span></a>). <a href="#tbl-dobson-cyclones-sumstat" class="quarto-xref">Table&nbsp;<span>C.2</span></a> provides summary statistics.</p>
<div class="cell">
<details class="code-fold"><summary>Show R code</summary><div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cyclones</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">number</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/expand_limits.html">expand_limits</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"Number of cyclones"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"Count (number of seasons)"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<div id="fig-cyclones-bar-plot" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-cyclones-bar-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;C.2: Bar plot of cyclones per season
</figcaption><div aria-describedby="fig-cyclones-bar-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="intro-MLEs_files/figure-html/fig-cyclones-bar-plot-1.png" class="img-fluid figure-img" width="672">
</div>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold"><summary>Show R code</summary><div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cyclones</span> <span class="op">|&gt;</span> <span class="fu">table1</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/table1/man/table1.html">table1</a></span><span class="op">(</span>x <span class="op">=</span> <span class="op">~</span> <span class="va">number</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div id="tbl-dobson-cyclones-sumstat" class="cell quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-dobson-cyclones-sumstat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;C.2: Summary statistics for <code>cyclones</code> data
</figcaption><div aria-describedby="tbl-dobson-cyclones-sumstat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div>
<div class="Rtable1">
<table class="Rtable1 do-not-create-environment cell table table-sm table-striped small" data-quarto-postprocess="true">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead><tr class="header">
<th class="rowlabel firstrow lastrow" data-quarto-table-cell-role="th"></th>
<th class="firstrow lastrow" data-quarto-table-cell-role="th"><span class="stratlabel">Overall<br><span class="stratn">(N=13)</span></span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td class="rowlabel firstrow">number</td>
<td class="firstrow"></td>
</tr>
<tr class="even">
<td class="rowlabel">Mean (SD)</td>
<td>5.54 (2.47)</td>
</tr>
<tr class="odd">
<td class="rowlabel lastrow">Median [Min, Max]</td>
<td class="lastrow">6.00 [2.00, 12.0]</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</figure>
</div>
</div>
</section><section id="model" class="level3" data-number="C.2.3"><h3 data-number="C.2.3" class="anchored" data-anchor-id="model">
<span class="header-section-number">C.2.3</span> Model</h3>
<p>We want to estimate <span class="math inline">\(P(Y=y)\)</span>; that is, <span class="math inline">\(P(Y=y)\)</span> is our <a href="./estimation.html#def-estimand">estimand</a>.</p>
<p>We could estimate <span class="math inline">\(P(Y=y)\)</span> for each value of <span class="math inline">\(y\)</span> in <span class="math inline">\(0:\infty\)</span> separately (“nonparametrically”) using the fraction of our data with <span class="math inline">\(Y_i=y\)</span>, but then we would be estimating an infinitely large set of parameters, and we would have low precision. We will probably do better with a parametric model.</p>
<div id="exr-cyclone-choose-dist" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.1</strong></span> What parametric probability distribution family might we use to model this empirical distribution?</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span>Let’s use the Poisson. The Poisson distribution is appropriate for this data , because the data are counts that could theoretically take any integer value (discrete) in the range <span class="math inline">\(0:\infty\)</span>. Visually, the plot of our data closely resembles a Poisson or binomial distribution. Since cyclones do not have an “upper limit” on the number of events we could potentially observe in one season, the Poisson distribution is more appropriate than the binomial.</p>
</div>
</div>
<div id="exr-def-poisson" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.2</strong></span> Write down the Poisson distribution’s probability mass function.</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span><span class="math display">\[P(Y=y) = \frac{\lambda^{y} e^{-\lambda}}{y!}\]</span></p>
</div>
</div>
</section><section id="estimating-the-model-parameters-using-maximum-likelihood" class="level3" data-number="C.2.4"><h3 data-number="C.2.4" class="anchored" data-anchor-id="estimating-the-model-parameters-using-maximum-likelihood">
<span class="header-section-number">C.2.4</span> Estimating the model parameters using maximum likelihood</h3>
<p>Now, we can estimate the parameter <span class="math inline">\(\lambda\)</span> for this distribution using maximum likelihood estimation.</p>
<p>What is the likelihood?</p>
<div id="exr-poisson-likelihood" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.3</strong></span> Write down the likelihood (probability mass function or probability density function) of a single observation <span class="math inline">\(x\)</span>, according to your model.</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span><span class="math display">\[
\begin{aligned}
\mathcal{L}(\lambda; x)
&amp;= p(X=x|\Lambda = \lambda)\\
&amp;= \frac{\lambda^x e^{-\lambda}}{x!}\\
\end{aligned}
\]</span></p>
</div>
</div>
<div id="exr-poisson-parameters" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.4</strong></span> Write down the vector of parameters in your model.</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span>There is only one parameter, <span class="math inline">\(\lambda\)</span>:</p>
<p><span class="math display">\[\theta = (\lambda)\]</span></p>
</div>
</div>
<div id="exr-poisson-mean-and-variance" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.5</strong></span> Write down the population mean and variance of a single observation from your chosen probability model, as a function of the parameters (extra credit - derive them).</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span></p>
<ul>
<li>Population mean: <span class="math inline">\(\text{E}[X]=\lambda\)</span>
</li>
<li>Population variance: <span class="math inline">\(\text{Var}(X)=\lambda\)</span>
</li>
</ul>
</div>
</div>
<div id="exr-sample-likelihood" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.6</strong></span> Write down the likelihood of the full dataset.</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span><span class="math display">\[
\begin{aligned}
\mathcal{L}(\lambda; \tilde x)
&amp;= P(\tilde X = \tilde x) \\
&amp;= P(X_1 = x_1, X_2 = x_2, ..., X_{13} = x_{13}) \\
&amp;= \prod_{i=1}^{13} P(X_i = x_i) \\
&amp;= \prod_{i=1}^{13} \frac{\lambda^{x_i} e^{-\lambda}}{x_i!}
\end{aligned}
\]</span></p>
</div>
</div>
<div id="exr-graph-likelihood" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.7</strong></span> Graph the likelihood as a function of <span class="math inline">\(\lambda\)</span>.</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span></p>
<div class="cell">
<details class="code-fold"><summary>Show R code</summary><div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lik</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">lambda</span>, <span class="va">y</span> <span class="op">=</span> <span class="va">cyclones</span><span class="op">$</span><span class="va">number</span>, <span class="va">n</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span> </span>
<span><span class="op">{</span></span>
<span>  <span class="va">lambda</span><span class="op">^</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="va">n</span><span class="op">*</span><span class="va">lambda</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/prod.html">prod</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Special.html">factorial</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">geom_function</a></span><span class="op">(</span>fun <span class="op">=</span> <span class="va">lik</span>, n <span class="op">=</span> <span class="fl">1001</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html">xlim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">cyclones</span><span class="op">$</span><span class="va">number</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">cyclones</span><span class="op">$</span><span class="va">number</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"likelihood"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">'lambda'</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<div id="fig-cyclone-lik" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-cyclone-lik-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;C.3: Likelihood of Dobson cyclone data
</figcaption><div aria-describedby="fig-cyclone-lik-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="intro-MLEs_files/figure-html/fig-cyclone-lik-1.png" class="img-fluid figure-img" width="672">
</div>
</figure>
</div>
</div>
</div>
</div>
</div>
<div id="exr-sample-llik" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.8</strong></span> Write down the log-likelihood of the full dataset.</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span><span class="math display">\[
\begin{aligned}
\ell(\lambda; \tilde{x}) &amp;= \text{log}\left\{\mathcal{L}(\lambda;\tilde{x})\right\}\\
&amp;= \text{log}\left\{\prod_{i = 1}^n\frac{\lambda^{x_i}\text{e}^{-\lambda}}{x_i!}\right\}\\
&amp;= \sum_{i = 1}^n\text{log}\left\{\frac{\lambda^{x_i}\text{e}^{-\lambda}}{x_i!}\right\}\\
&amp;= \sum_{i = 1}^n{\text{log}\left\{\lambda^{x_i}\right\} +\text{log}\left\{\text{e}^{-\lambda}\right\} - \text{log}\left\{x_i!\right\}}\\
&amp;= \sum_{i = 1}^n{x_i\text{log}\left\{\lambda\right\} -\lambda - \text{log}\left\{x_i!\right\}}\\
&amp;= \sum_{i = 1}^nx_i\text{log}\left\{\lambda\right\} - \sum_{i = 1}^n\lambda - \sum_{i = 1}^n\text{log}\left\{x_i!\right\}\\
&amp;= \sum_{i = 1}^nx_i\text{log}\left\{\lambda\right\} - n\lambda - \sum_{i = 1}^n\text{log}\left\{x_i!\right\}\\
\end{aligned}
\]</span></p>
</div>
</div>
<div id="exr-graph-loglikelihood" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.9</strong></span> Graph the log-likelihood as a function of <span class="math inline">\(\lambda\)</span>.</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span></p>
<div class="cell">
<details class="code-fold"><summary>Show R code</summary><div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span></span>
<span><span class="va">loglik</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">lambda</span>, <span class="va">y</span> <span class="op">=</span> <span class="va">cyclones</span><span class="op">$</span><span class="va">number</span>, <span class="va">n</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span> <span class="op">-</span> <span class="va">n</span><span class="op">*</span><span class="va">lambda</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Special.html">factorial</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">ll_plot</span> <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">geom_function</a></span><span class="op">(</span>fun <span class="op">=</span> <span class="va">loglik</span>, n <span class="op">=</span> <span class="fl">1001</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html">xlim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">cyclones</span><span class="op">$</span><span class="va">number</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">cyclones</span><span class="op">$</span><span class="va">number</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"log-likelihood"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">'lambda'</span><span class="op">)</span></span>
<span><span class="va">ll_plot</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<div id="fig-cyclone-llik" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-cyclone-llik-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;C.4: log-likelihood of Dobson cyclone data
</figcaption><div aria-describedby="fig-cyclone-llik-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="intro-MLEs_files/figure-html/fig-cyclone-llik-1.png" class="img-fluid figure-img" width="672">
</div>
</figure>
</div>
</div>
</div>
</div>
</div>
<div id="exr-cyclone-score-fn" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.10</strong></span> Derive the score function for the dataset.</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span>The score function is the first derivative(s) of the log-likelihood:</p>
<p><span class="math display">\[
\begin{aligned}
\ell'( \lambda; \tilde{x} ) &amp;=
\frac{\partial}{\partial\lambda}{\sum_{i = 1}^nx_i\text{log}\left\{\lambda\right\} - n\lambda - \sum_{i = 1}^n\text{log}\left\{x_i!\right\}}\\
&amp;= \frac{\partial}{\partial\lambda}\sum_{i = 1}^nx_i\text{log}\left\{\lambda\right\} - \frac{\partial}{\partial\lambda}n\lambda - \frac{\partial}{\partial\lambda}\sum_{i = 1}^n\text{log}\left\{x_i!\right\}\\
&amp;= \sum_{i = 1}^nx_i\frac{\partial}{\partial\lambda}\text{log}\left\{\lambda\right\} - n\frac{\partial}{\partial\lambda}\lambda - \sum_{i = 1}^n\frac{\partial}{\partial\lambda}\text{log}\left\{x_i!\right\}\\
&amp;= \sum_{i = 1}^nx_i\frac{1}{\lambda} - n - 0\\
&amp;= \frac{1}{\lambda} \sum_{i = 1}^nx_i- n
\end{aligned}
\]</span></p>
</div>
</div>
<div id="exr-graph-score-function" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.11</strong></span> Graph the score function.</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span></p>
<div class="cell">
<details class="code-fold"><summary>Show R code</summary><div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span></span>
<span><span class="va">score</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">lambda</span>, <span class="va">y</span> <span class="op">=</span> <span class="va">cyclones</span><span class="op">$</span><span class="va">number</span>, <span class="va">n</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">/</span><span class="va">lambda</span> <span class="op">-</span> <span class="va">n</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">geom_function</a></span><span class="op">(</span>fun <span class="op">=</span> <span class="va">score</span>, n <span class="op">=</span> <span class="fl">1001</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html">xlim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">cyclones</span><span class="op">$</span><span class="va">number</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">cyclones</span><span class="op">$</span><span class="va">number</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"l'(lambda)"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">'lambda'</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span>, col <span class="op">=</span> <span class="st">'red'</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<div id="fig-cyclone-score" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-cyclone-score-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;C.5: score function of Dobson cyclone data
</figcaption><div aria-describedby="fig-cyclone-score-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="intro-MLEs_files/figure-html/fig-cyclone-score-1.png" class="img-fluid figure-img" width="672">
</div>
</figure>
</div>
</div>
</div>
</div>
</div>
<div id="exr-hessian" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.12</strong></span> Derive the Hessian matrix.</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span>The Hessian function for an iid sample is the 2nd derivative(s) of the log-likelihood:</p>
<p><span class="math display">\[
\begin{aligned}
\ell''( \lambda; \tilde{x} ) &amp;= \frac{\partial}{\partial\lambda}\left(\frac{1}{\lambda} \sum_{i = 1}^nx_i- n\right)\\
&amp;= \frac{\partial}{\partial\lambda}\frac{1}{\lambda} \sum_{i = 1}^nx_i- \frac{\partial}{\partial\lambda}n\\
&amp;= -\frac{1}{\lambda^2} \sum_{i = 1}^nx_i\\
&amp;= -\frac{1}{\lambda^2} n \bar x
\end{aligned}
\]</span></p>
</div>
</div>
<div id="exr-graph-hession" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.13</strong></span> Graph the Hessian.</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span></p>
<div class="cell">
<details class="code-fold"><summary>Show R code</summary><div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span></span>
<span><span class="va">hessian</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">lambda</span>, <span class="va">y</span> <span class="op">=</span> <span class="va">cyclones</span><span class="op">$</span><span class="va">number</span>, <span class="va">n</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">{</span></span>
<span>  <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">/</span><span class="va">lambda</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">geom_function</a></span><span class="op">(</span>fun <span class="op">=</span> <span class="va">hessian</span>, n <span class="op">=</span> <span class="fl">1001</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html">xlim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">cyclones</span><span class="op">$</span><span class="va">number</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">cyclones</span><span class="op">$</span><span class="va">number</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"l''(lambda)"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">'lambda'</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span>, col <span class="op">=</span> <span class="st">'red'</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<div id="fig-cyclone-hessian" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-cyclone-hessian-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;C.6: Hessian function of Dobson cyclone data
</figcaption><div aria-describedby="fig-cyclone-hessian-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="intro-MLEs_files/figure-html/fig-cyclone-hessian-1.png" class="img-fluid figure-img" width="672">
</div>
</figure>
</div>
</div>
</div>
</div>
</div>
<div id="exr-score-equation" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.14</strong></span> Write the score equation (estimating equation).</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span><span class="math display">\[\ell'( \lambda; \tilde{x} ) = 0\]</span></p>
</div>
</div>
<div id="exr-solve-score-equation" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.15</strong></span> Solve the estimating equation for <span class="math inline">\(\lambda\)</span>:</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span><span class="math display">\[
\begin{aligned}
0 &amp;= \frac{1}{\lambda}\sum_{i = 1}^nx_i - n\\
n &amp;= \frac{1}{\lambda}\sum_{i = 1}^nx_i\\
n\lambda &amp;= \sum_{i = 1}^nx_i\\
\lambda &amp;=
\frac{1}{n}\sum_{i = 1}^nx_i\\
&amp;=\bar x
\end{aligned}
\]</span></p>
</div>
</div>
<p>Let’s call this solution of the estimating equation <span class="math inline">\(\tilde \lambda\)</span> for now:</p>
<p><span class="math display">\[\tilde \lambda \stackrel{\text{def}}{=}\bar x\]</span></p>
<div id="exr-check-hessian" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.16</strong></span> Confirm that the Hessian <span class="math inline">\(\ell''(\lambda; \tilde{x})\)</span> is negative when evaluated at <span class="math inline">\(\tilde \lambda\)</span>.</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span><span class="math display">\[
\begin{aligned}
\ell''( \tilde\lambda; \tilde{x} ) &amp;=
-\frac{1}{\tilde\lambda^2} n \bar x\\
&amp;= -\frac{1}{\bar x^2} n\bar x\\
&amp;= -\frac{n}{\bar x}\\
&amp;&lt;0\\
\end{aligned}
\]</span></p>
</div>
</div>
<div id="exr-find-mle" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.17</strong></span> Find the MLE of <span class="math inline">\(\lambda\)</span>.</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span>Since <span class="math inline">\(\ell''(\tilde \lambda; \tilde{x})&lt;0\)</span>, <span class="math inline">\(\tilde \lambda\)</span> is at least a local maximizer of the likelihood function <span class="math inline">\(\mathcal L(\lambda)\)</span>. Since there is only one solution to the estimating equation and the Hessian is negative definite everywhere, <span class="math inline">\(\tilde \lambda\)</span> must also be the global maximizer of <span class="math inline">\(\mathcal L(\lambda; \tilde{x})\)</span>:</p>
<div class="cell">
<details class="code-fold"><summary>Show R code</summary><div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">mle</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">cyclones</span><span class="op">$</span><span class="va">number</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><span class="math display">\[\hat{\lambda}_{\text{ML}} = \bar x = 5.53846154\]</span></p>
</div>
</div>
<div id="exr-graph-mle" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise C.18</strong></span> Graph the log-likelihood with the MLE superimposed.</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span></p>
<div class="cell">
<details class="code-fold"><summary>Show R code</summary><div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">mle_data</span> <span class="op">=</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">mle</span>, y <span class="op">=</span> <span class="fu">loglik</span><span class="op">(</span><span class="va">mle</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">ll_plot</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">mle_data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>, col <span class="op">=</span> <span class="st">'red'</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<div id="fig-cyclone-llik-mle" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-cyclone-llik-mle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;C.7: log-likelihood of Dobson cyclone data with MLE
</figcaption><div aria-describedby="fig-cyclone-llik-mle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="intro-MLEs_files/figure-html/fig-cyclone-llik-mle-1.png" class="img-fluid figure-img" width="672">
</div>
</figure>
</div>
</div>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold"><summary>Show R code</summary><div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">obs_inf</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">lambda</span>, <span class="va">y</span> <span class="op">=</span> <span class="va">cyclones</span><span class="op">$</span><span class="va">number</span>, <span class="va">n</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">/</span><span class="va">lambda</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section></section><section id="maximum-likelihood-inference-for-univariate-gaussian-models" class="level2" data-number="C.3"><h2 data-number="C.3" class="anchored" data-anchor-id="maximum-likelihood-inference-for-univariate-gaussian-models">
<span class="header-section-number">C.3</span> Maximum likelihood inference for univariate Gaussian models</h2>
<p>Suppose <span class="math inline">\(X_{1}, ..., X_{n} \ \sim_{\text{iid}}\ N(\mu, \sigma^{2})\)</span>. Let <span class="math inline">\(X = (X_{1},\ldots,X_{n})^{\top}\)</span> be these random variables in vector format. Let <span class="math inline">\(x_{i}\)</span> and <span class="math inline">\(x\)</span> denote the corresponding observed data. Then <span class="math inline">\(\theta = (\mu,\sigma^{2})\)</span> is the vector of true parameters, and <span class="math inline">\(\Theta = (\text{M}, \Sigma^2)\)</span> is the vector of parameters as a random vector.</p>
<p>Then the log-likelihood is:</p>
<p><span class="math display">\[
\begin{aligned}
\ell
&amp;\propto - \frac{n}{2}\text{log}\left\{\sigma^{2}\right\} - \frac{1}{2}\sum_{i = 1}^{n}\frac{( x_{i} - \mu)^{2}}{\sigma^{2}}\\
&amp;= - \frac{n}{2}\text{log}\left\{\sigma^{2}\right\} - \frac{1}{2\sigma^{2}}\sum_{i = 1}^{n}{x_{i}^{2} - 2x_{i}\mu + \mu^{2}}
\end{aligned}
\]</span></p>
<section id="the-score-function-1" class="level3" data-number="C.3.1"><h3 data-number="C.3.1" class="anchored" data-anchor-id="the-score-function-1">
<span class="header-section-number">C.3.1</span> The score function</h3>
<p><span class="math display">\[\ell^{'}(x,\theta) \stackrel{\text{def}}{=}\frac{d}{d\theta}\ell(x,\theta) = \left( \begin{array}{r}
\frac{d}{d\mu}\ell(\theta;x) \\
\frac{d}{d\sigma^{2}}\ell(\theta;x)
\end{array} \right) = \left( \begin{array}{r}
\ell_{\mu}^{'}(\theta;x) \\
\ell_{\sigma^{2}}^{'}(\theta;x)
\end{array} \right)\]</span>.</p>
<p><span class="math inline">\(\ell^{'}(x,\theta)\)</span> is the function we set equal to 0 and solve to find the MLE:</p>
<p><span class="math display">\[{\widehat{\theta}}_{ML} = \left\{ \theta:\ell^{'}(x,\theta) = 0 \right\}\]</span></p>
</section><section id="mle-of-mu" class="level3" data-number="C.3.2"><h3 data-number="C.3.2" class="anchored" data-anchor-id="mle-of-mu">
<span class="header-section-number">C.3.2</span> MLE of <span class="math inline">\(\mu\)</span>
</h3>
<p><span class="math display">\[
\begin{aligned}
\frac{d\ell}{d\mu}
&amp;= - \frac{1}{2}\sum_{i = 1}^{n}
\frac{- 2(x_{i} - \mu)}{\sigma^{2}}
\\ &amp;= \frac{1}{\sigma^{2}}
\left[
    \left(
        \sum_{i = 1}^{n}x_{i}
    \right)
    - n\mu
\right]
\end{aligned}
\]</span></p>
<p>If <span class="math inline">\(\frac{d\ell}{d\mu} = 0\)</span>, then <span class="math inline">\(\mu = \overline{x} \stackrel{\text{def}}{=}\frac{1}{n}\sum_{i = 1}^{n}x_{i}\)</span>.</p>
<p><span class="math display">\[\frac{d^{2}\ell}{(d\mu)^{2}} = \frac{- n}{\sigma^{2}} &lt; 0\]</span></p>
<p>So <span class="math inline">\({\widehat{\mu}}_{ML} = \overline{x}\)</span>.</p>
</section><section id="mle-of-sigma2" class="level3" data-number="C.3.3"><h3 data-number="C.3.3" class="anchored" data-anchor-id="mle-of-sigma2">
<span class="header-section-number">C.3.3</span> MLE of <span class="math inline">\(\sigma^{2}\)</span>
</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reparametrizing the Gaussian distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>When solving for <span class="math inline">\({\widehat{\sigma}}_{ML}\)</span>, you can treat <span class="math inline">\(\sigma^{2}\)</span> as an atomic variable (don’t differentiate with respect to <span class="math inline">\(\sigma\)</span> or things get messy). In fact, you can replace <span class="math inline">\(\sigma^{2}\)</span> with <span class="math inline">\(1/\tau\)</span> and differentiate with respect to <span class="math inline">\(\tau\)</span> instead, and the process might be even easier.</p>
</div>
</div>
<p><span class="math display">\[\frac{d\ell}{d\sigma^{2}} = \frac{d}{d\sigma^{2}}\left( - \frac{n}{2}\text{log}\left\{\sigma^{2}\right\} - \frac{1}{2}\sum_{i = 1}^{n}\frac{\left( x_{i} - \mu \right)^{2}}{\sigma^{2}} \right)\ \]</span></p>
<p><span class="math display">\[= - \frac{n}{2}\left( \sigma^{2} \right)^{- 1} + \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2}\]</span></p>
<p>If <span class="math inline">\(\frac{d\ell}{d\sigma^{2}} = 0\)</span>, then:</p>
<p><span class="math display">\[\frac{n}{2}\left( \sigma^{2} \right)^{- 1} = \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2}\]</span></p>
<p><span class="math display">\[\sigma^{2} = \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2}\]</span></p>
<p>We plug in <span class="math inline">\({\widehat{\mu}}_{ML} = \overline{x}\)</span> to maximize globally (a technique called profiling):</p>
<p><span class="math display">\[\sigma_{ML}^{2} = \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}\]</span></p>
<p>Now:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{d^{2}\ell}{\left( d\sigma^{2} \right)^{2}}
&amp;= \frac{d}{d\sigma^{2}}\left\{ - \frac{n}{2}\left( \sigma^{2} \right)^{- 1} + \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right\}\\
&amp;= \left\{ - \frac{n}{2}\frac{d}{d\sigma^{2}}\left( \sigma^{2} \right)^{- 1} + \frac{1}{2}\frac{d}{d\sigma^{2}}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right\}\\
&amp;= \left\{ \frac{n}{2}\left( \sigma^{2} \right)^{- 2} - \left( \sigma^{2} \right)^{- 3}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right\}\\
&amp;= \left( \sigma^{2} \right)^{- 2}\left\{ \frac{n}{2} - \left( \sigma^{2} \right)^{- 1}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right\}
\end{aligned}
\]</span></p>
<p>Evaluated at <span class="math inline">\(\mu = \overline{x},\sigma^{2} = \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}\)</span>, we have:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{d^{2}\ell}{\left( d\sigma^{2} \right)^{2}}
&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}\left\{ \frac{n}{2} - \left( {\widehat{\sigma}}^{2} \right)^{- 1}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2} \right\}\\
&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}\left\{ \frac{n}{2} - \left( {\widehat{\sigma}}^{2} \right)^{- 1}n{\widehat{\sigma}}^{2} \right\}\\
&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}\left\{ \frac{n}{2} - n \right\}\\
&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}n\left\{ \frac{1}{2} - 1 \right\}\\
&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}n\left( - \frac{1}{2} \right) &lt; 0
\end{aligned}
\]</span></p>
<p>Finally, we have:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{d^{2}\ell}{d\mu\ d\sigma^{2}}
&amp;= \frac{d}{d\mu}\left\{ - \frac{n}{2}\left( \sigma^{2} \right)^{- 1} + \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right\}\\
&amp;= \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\frac{d}{d\mu}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2}\\
&amp;= \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}{- 2(x_{i} - \mu)}\\
&amp;= - \left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}{(x_{i} - \mu)}
\end{aligned}
\]</span></p>
<p>Evaluated at <span class="math inline">\(\mu = \widehat{\mu} = \overline{x},\sigma^{2} = {\widehat{\sigma}}^{2} = \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}\)</span>, we have:</p>
<p><span class="math display">\[\frac{d^{2}\ell}{d\mu\ d\sigma^{2}} = - \left( {\widehat{\sigma}}^{2} \right)^{- 2}\left( n\overline{x} - n\overline{x} \right) = 0\]</span></p>
</section><section id="covariance-matrix" class="level3" data-number="C.3.4"><h3 data-number="C.3.4" class="anchored" data-anchor-id="covariance-matrix">
<span class="header-section-number">C.3.4</span> Covariance matrix</h3>
<p><span class="math display">\[I = \begin{bmatrix}
\frac{n}{\sigma^{2}} &amp; 0 \\
0 &amp; \left( {\widehat{\sigma}}^{2} \right)^{- 2}n\left( - \frac{1}{2} \right)
\end{bmatrix} = \begin{bmatrix}
a &amp; 0 \\
0 &amp; d
\end{bmatrix}\]</span></p>
<p>So:</p>
<p><span class="math display">\[I^{- 1} = \frac{1}{ad}\begin{bmatrix}
d &amp; 0 \\
0 &amp; a
\end{bmatrix} = \begin{bmatrix}
\frac{1}{a} &amp; 0 \\
0 &amp; \frac{1}{d}
\end{bmatrix}\]</span></p>
<p><span class="math display">\[I^{- 1} = \begin{bmatrix}
\frac{{\widehat{\sigma}}^{2}}{n} &amp; 0 \\
0 &amp; \frac{{2\left( {\widehat{\sigma}}^{2} \right)}^{2}}{n}
\end{bmatrix}\]</span></p>
<p>See <span class="citation" data-cites="CaseBerg01">Casella and Berger (<a href="references.html#ref-CaseBerg01" role="doc-biblioref">2002</a>)</span> p322, example 7.2.12.</p>
<p>To prove it’s a maximum, need:</p>
<ul>
<li><p><span class="math inline">\(\ell^{'} = 0\)</span></p></li>
<li><p>At least one diagonal element of <span class="math inline">\(\ell''\)</span> is negative.</p></li>
<li><p>Determinant of <span class="math inline">\(\ell''\)</span> is positive.</p></li>
</ul>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-CaseBerg01" class="csl-entry" role="listitem">
Casella, George, and Roger Berger. 2002. <em>Statistical Inference</em>. 2nd ed. Cengage Learning. <a href="https://www.cengage.com/c/statistical-inference-2e-casella-berger/9780534243128/">https://www.cengage.com/c/statistical-inference-2e-casella-berger/9780534243128/</a>.
</div>
<div id="ref-dobson4e" class="csl-entry" role="listitem">
Dobson, Annette J, and Adrian G Barnett. 2018. <em>An Introduction to Generalized Linear Models</em>. 4th ed. CRC press. <a href="https://doi.org/10.1201/9781315182780">https://doi.org/10.1201/9781315182780</a>.
</div>
<div id="ref-efron1978assessing" class="csl-entry" role="listitem">
Efron, Bradley, and David V Hinkley. 1978. <span>“Assessing the Accuracy of the Maximum Likelihood Estimator: Observed Versus Expected Fisher Information.”</span> <em>Biometrika</em> 65 (3): 457–83.
</div>
<div id="ref-mclachlan2007algorithm" class="csl-entry" role="listitem">
McLachlan, Geoffrey J, and Thriyambakam Krishnan. 2007. <em>The EM Algorithm and Extensions</em>. 2nd ed. John Wiley &amp; Sons. <a href="https://doi.org/10.1002/9780470191613">https://doi.org/10.1002/9780470191613</a>.
</div>
</div>
</section></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p> <a href="https://en.wikipedia.org/wiki/Does_exactly_what_it_says_on_the_tin" class="uri">https://en.wikipedia.org/wiki/Does_exactly_what_it_says_on_the_tin</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>I might sometimes switch the order of <span class="math inline">\(x,\)</span> <span class="math inline">\(\theta\)</span>; this is unintentional and not meaningful.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./estimation.html" class="pagination-link" aria-label="Estimation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Estimation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./common-mistakes.html" class="pagination-link" aria-label="Common Mistakes">
        <span class="nav-page-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Common Mistakes</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb12" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction to Maximum Likelihood Inference {#sec-intro-MLEs}</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>{{&lt; include shared-config.qmd &gt;}}</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>These notes are derived primarily from @dobson4e (mostly chapters 1-5).</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>Some material was also taken from @mclachlan2007algorithm and @CaseBerg01.</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="fu">## Overview of maximum likelihood estimation</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="fu">### The likelihood function</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>:::{#def-lik}</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Likelihood</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>Let $\vec x$ be a dataset with corresponding random variable $\vec X$.</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>Let $\p_{\Th}(\vec X)$ be a probability model for the distribution of $\vX$ with parameter vector $\Th$.</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>Then the **likelihood** of parameter value $\th$, for model $\p_{\Th}(X)$ and data $\vX = \vx$, is the *joint probability* of $\vx$ given $\Th = \th$:</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>\ba</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>\Lik(\theta) &amp;\eqdef p_{\theta}(\vX = \vx)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>&amp;=p_{\theta}(X_1=x_1, ..., X_n = x_n)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>\ea</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>::: callout-note</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Notation for the likelihood function</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>The likelihood function can be written as:</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\Lik(\theta)$</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\Lik(\vec x;\theta)$</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\Lik(\theta; \vec x)$</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\Lik_{\vec x}(\theta)$</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\Lik_{\theta}(\vec x)$</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\Lik(\vec x | \theta)$</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>All of these notations mean the same thing.</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>::: notes</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>The likelihood is a function that takes $\theta$ (and implicitly, $\vec X$) as inputs and outputs a single number, the joint probability of $\vec x$ for model $p_\Theta(\vX=\vx)$ with $\Theta = \theta$.</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>:::{#thm-lik-iid}</span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Likelihood of an independent sample</span></span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>For <span class="co">[</span><span class="ot">mutually independent</span><span class="co">](probability.qmd#def-indpt)</span> data $X_1, ..., X_n$:</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>$$\Lik(\vec x|\theta) = \prod_{i=1}^n \p(X_i=x_i|\theta)$$ {#eq-Lik}</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>\ba</span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>\Lik(\vec x|\theta) </span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>&amp;\eqdef \p(X_1 = x_1, …,X_n =x_n|\theta) </span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>&amp;= \prod_{i=1}^n \p(X_i=x_i|\theta)</span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>\ea</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>The second equality is by the definition of statistical independence.</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a><span class="fu">### The maximum likelihood estimate</span></span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>:::{#def-mle}</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Maximum likelihood estimate</span></span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a>The **maximum likelihood estimate** of a parameter vector $\Theta$, denoted $\hthml$, is the value of $\Theta$ that maximizes the likelihood:</span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a>\hthml \eqdef \arg \max_\Th \Lik(\Th)</span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mle}</span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a><span class="fu">### Finding the maximum of a function</span></span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a>Recall from calculus: the maxima of a continuous function $f(x)$ over a range of input values $\range{x}$ can be found either:</span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>at the edges of the range of input values, *OR*:</span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>where the function is flat (i.e. where the gradient function $f'(x) = 0$) *AND* the second derivative is negative definite ($f''(x) &lt; 0$).</span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a><span class="fu">### Directly maximizing the likelihood function for *iid* data</span></span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a>To find the maximizer(s) of the likelihood function, we need to solve $\Lik'(\th) = 0$ for $\theta$. However, even for mutually independent data, we quickly run into a problem:</span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a>\ba</span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a>\Lik'(\th) </span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a>&amp;= \deriv{\th} \Lik(\th)</span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \deriv{\th} \prod_{i=1}^n p(X_i=x_i|\theta)</span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a>\ea</span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a>$$ {#eq-deriv-Lik}</span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a>The derivative of the likelihood of independent data is the derivative of a product. </span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a>We will have to perform a massive application of the product rule to evaluate this derivative.</span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a><span class="fu">### The log-likelihood function</span></span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a>It is typically easier to work with the log of the likelihood function:</span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-113"><a href="#cb12-113" aria-hidden="true" tabindex="-1"></a>:::{#def-loglik}</span>
<span id="cb12-114"><a href="#cb12-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Log-likelihood</span></span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a>The **log-likelihood** of parameter value $\theta$, for model $\p_{\Theta}(\vX)$ and data $\vX = \vx$, is the natural logarithm of the likelihood^<span class="co">[</span><span class="ot"> &lt;https://en.wikipedia.org/wiki/Does_exactly_what_it_says_on_the_tin&gt;</span><span class="co">]</span>:</span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a>$$\lik(\th) \eqdef \logf{\Lik(\th)}$$</span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a>:::{#thm-mle-use-log}</span>
<span id="cb12-123"><a href="#cb12-123" aria-hidden="true" tabindex="-1"></a>####</span>
<span id="cb12-124"><a href="#cb12-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-125"><a href="#cb12-125" aria-hidden="true" tabindex="-1"></a>The likelihood and log-likelihood have the same maximizer:</span>
<span id="cb12-126"><a href="#cb12-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-127"><a href="#cb12-127" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-128"><a href="#cb12-128" aria-hidden="true" tabindex="-1"></a>\am_\th \Lik(\th) = \am_\th \lik(\th)</span>
<span id="cb12-129"><a href="#cb12-129" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-130"><a href="#cb12-130" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb12-131"><a href="#cb12-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-132"><a href="#cb12-132" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb12-133"><a href="#cb12-133" aria-hidden="true" tabindex="-1"></a>Left to the reader.</span>
<span id="cb12-134"><a href="#cb12-134" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-135"><a href="#cb12-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-136"><a href="#cb12-136" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-137"><a href="#cb12-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-138"><a href="#cb12-138" aria-hidden="true" tabindex="-1"></a>:::{#thm-llik-iid}</span>
<span id="cb12-139"><a href="#cb12-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-140"><a href="#cb12-140" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Log-likelihood of an iid sample</span></span>
<span id="cb12-141"><a href="#cb12-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-142"><a href="#cb12-142" aria-hidden="true" tabindex="-1"></a>For iid data $X_1, ..., X_n$ with shared distribution $\p(X=x)$:</span>
<span id="cb12-143"><a href="#cb12-143" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-144"><a href="#cb12-144" aria-hidden="true" tabindex="-1"></a>\ba</span>
<span id="cb12-145"><a href="#cb12-145" aria-hidden="true" tabindex="-1"></a>\ell(x|\theta) </span>
<span id="cb12-146"><a href="#cb12-146" aria-hidden="true" tabindex="-1"></a>&amp;\eqdef \log{\mathcal L(x|\theta)}</span>
<span id="cb12-147"><a href="#cb12-147" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>&amp;= \sum_{i=1}^n \log{p(X=x_i|\theta)}</span>
<span id="cb12-148"><a href="#cb12-148" aria-hidden="true" tabindex="-1"></a>\ea</span>
<span id="cb12-149"><a href="#cb12-149" aria-hidden="true" tabindex="-1"></a>$$ {#eq-loglik}</span>
<span id="cb12-150"><a href="#cb12-150" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-151"><a href="#cb12-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-152"><a href="#cb12-152" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Derivative of the log-likelihood function for *iid* data</span></span>
<span id="cb12-153"><a href="#cb12-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-154"><a href="#cb12-154" aria-hidden="true" tabindex="-1"></a>For iid data, we will have a much easier time taking the derivative of the log-likelihood:</span>
<span id="cb12-155"><a href="#cb12-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-156"><a href="#cb12-156" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-157"><a href="#cb12-157" aria-hidden="true" tabindex="-1"></a>\ba</span>
<span id="cb12-158"><a href="#cb12-158" aria-hidden="true" tabindex="-1"></a>\lik'(\th) </span>
<span id="cb12-159"><a href="#cb12-159" aria-hidden="true" tabindex="-1"></a>&amp;= \deriv{\th} \lik(\th)</span>
<span id="cb12-160"><a href="#cb12-160" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \deriv{\th} \sum_{i=1}^n \log{\p(X=x_i|\theta)}</span>
<span id="cb12-161"><a href="#cb12-161" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \sum_{i=1}^n \deriv{\th} \log{\p(X=x_i|\theta)}</span>
<span id="cb12-162"><a href="#cb12-162" aria-hidden="true" tabindex="-1"></a>\ea</span>
<span id="cb12-163"><a href="#cb12-163" aria-hidden="true" tabindex="-1"></a>$$ {#eq-deriv-llik}</span>
<span id="cb12-164"><a href="#cb12-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-165"><a href="#cb12-165" aria-hidden="true" tabindex="-1"></a><span class="fu">### The score function</span></span>
<span id="cb12-166"><a href="#cb12-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-167"><a href="#cb12-167" aria-hidden="true" tabindex="-1"></a>The first derivative of the log-likelihood, $\lik'(\th)$, is important enough to have its own name and symbol: the *score function*, $U(\theta)$.</span>
<span id="cb12-168"><a href="#cb12-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-169"><a href="#cb12-169" aria-hidden="true" tabindex="-1"></a>:::{#def-score}</span>
<span id="cb12-170"><a href="#cb12-170" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Score function</span></span>
<span id="cb12-171"><a href="#cb12-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-172"><a href="#cb12-172" aria-hidden="true" tabindex="-1"></a>The **score function** of a statistical model $\pr(\vec X=\vec x)$ is the gradient (i.e., first derivative) of the log-likelihood of that model:</span>
<span id="cb12-173"><a href="#cb12-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-174"><a href="#cb12-174" aria-hidden="true" tabindex="-1"></a>$$\lik'(\th) \eqdef \deriv{\th} \lik(\th)$$</span>
<span id="cb12-175"><a href="#cb12-175" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-176"><a href="#cb12-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-177"><a href="#cb12-177" aria-hidden="true" tabindex="-1"></a>::: notes</span>
<span id="cb12-178"><a href="#cb12-178" aria-hidden="true" tabindex="-1"></a>We often</span>
<span id="cb12-179"><a href="#cb12-179" aria-hidden="true" tabindex="-1"></a>skip writing the arguments $x$ and/or $\theta)$, so</span>
<span id="cb12-180"><a href="#cb12-180" aria-hidden="true" tabindex="-1"></a>$\ell^{'} \eqdef \ell^{'}(\vec x;\theta) \ell^{'}(\theta)$.<span class="ot">[^1]</span> Some statisticians</span>
<span id="cb12-181"><a href="#cb12-181" aria-hidden="true" tabindex="-1"></a>use $U$ or $S$ instead of $\ell^{'}$. I prefer $\ell^{'}$.</span>
<span id="cb12-182"><a href="#cb12-182" aria-hidden="true" tabindex="-1"></a>Why use up extra letters?</span>
<span id="cb12-183"><a href="#cb12-183" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-184"><a href="#cb12-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-185"><a href="#cb12-185" aria-hidden="true" tabindex="-1"></a><span class="fu">### Asymptotic distribution of the maximum likelihood estimate</span></span>
<span id="cb12-186"><a href="#cb12-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-187"><a href="#cb12-187" aria-hidden="true" tabindex="-1"></a>::: notes</span>
<span id="cb12-188"><a href="#cb12-188" aria-hidden="true" tabindex="-1"></a>We learned how to quantify our uncertainty about these maximum likelihood estimates; with sufficient sample size, $\hthml$ has the approximate distribution:</span>
<span id="cb12-189"><a href="#cb12-189" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-190"><a href="#cb12-190" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-191"><a href="#cb12-191" aria-hidden="true" tabindex="-1"></a>\hat\theta_{ML} \dot \sim N(\theta,\mathcal I(\theta)^{-1})</span>
<span id="cb12-192"><a href="#cb12-192" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-193"><a href="#cb12-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-194"><a href="#cb12-194" aria-hidden="true" tabindex="-1"></a>Recall:</span>
<span id="cb12-195"><a href="#cb12-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-196"><a href="#cb12-196" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\einf(\theta) \eqdef \E{\oinf(\vX;\theta)}$</span>
<span id="cb12-197"><a href="#cb12-197" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\oinf(\vX,\theta) \eqdef -\ell''(\vX;\theta)$</span>
<span id="cb12-198"><a href="#cb12-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-199"><a href="#cb12-199" aria-hidden="true" tabindex="-1"></a>We can estimate $\einf(\th)$ using either $\einf(\hthml)$ or $\oinf(\vec x; \hthml)$.</span>
<span id="cb12-200"><a href="#cb12-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-201"><a href="#cb12-201" aria-hidden="true" tabindex="-1"></a>So we can estimate the standard error of $\hth_k$ as:</span>
<span id="cb12-202"><a href="#cb12-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-203"><a href="#cb12-203" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-204"><a href="#cb12-204" aria-hidden="true" tabindex="-1"></a>\HSE{\hth_k} = \sqrt{\sb{\inv{\heinff{\hthml}}}_{kk}}</span>
<span id="cb12-205"><a href="#cb12-205" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-206"><a href="#cb12-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-207"><a href="#cb12-207" aria-hidden="true" tabindex="-1"></a><span class="fu">### The (Fisher) (expected) information matrix</span></span>
<span id="cb12-208"><a href="#cb12-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-209"><a href="#cb12-209" aria-hidden="true" tabindex="-1"></a>The variance of $\ell^{'}(x,\theta)$,</span>
<span id="cb12-210"><a href="#cb12-210" aria-hidden="true" tabindex="-1"></a>${Cov}\left<span class="sc">\{</span> \ell^{'}(x,\theta) \right<span class="sc">\}</span>$, is also very</span>
<span id="cb12-211"><a href="#cb12-211" aria-hidden="true" tabindex="-1"></a>important; we call it the "expected information matrix", "Fisher</span>
<span id="cb12-212"><a href="#cb12-212" aria-hidden="true" tabindex="-1"></a>information matrix", or just "information matrix", and we represent it</span>
<span id="cb12-213"><a href="#cb12-213" aria-hidden="true" tabindex="-1"></a>using the symbol $\mathfrak{I}$ (<span class="in">`\frakturI`</span> in Unicode, <span class="in">`\mathfrak{I}`</span> in LaTeX).</span>
<span id="cb12-214"><a href="#cb12-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-215"><a href="#cb12-215" aria-hidden="true" tabindex="-1"></a>$$\mathfrak{I \eqdef I(}\theta) \eqdef {Cov}\left( \ell^{'}|\theta \right) = E\left\lbrack \ell^{'}{\ell^{'}}^{\top} \right\rbrack - E\left\lbrack \ell^{'} \right\rbrack\ E\left\lbrack \ell^{'} \right\rbrack^{\top}</span>
<span id="cb12-216"><a href="#cb12-216" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-217"><a href="#cb12-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-218"><a href="#cb12-218" aria-hidden="true" tabindex="-1"></a>The elements of $\mathfrak{I}$ are:</span>
<span id="cb12-219"><a href="#cb12-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-220"><a href="#cb12-220" aria-hidden="true" tabindex="-1"></a>$$\left<span class="sc">\{</span> \mathfrak{I}_{ij} \eqdef {Cov}\left( {\ell^{'}}_{i},{\ell^{'}}_{j} \right) = E\left\lbrack \ell_{i}^{'}\ell_{j}^{'} \right\rbrack - E\left\lbrack {\ell^{'}}_{i} \right\rbrack E\left\lbrack {\ell^{'}}_{j} \right\rbrack \right<span class="sc">\}</span>$$</span>
<span id="cb12-221"><a href="#cb12-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-222"><a href="#cb12-222" aria-hidden="true" tabindex="-1"></a>Here, </span>
<span id="cb12-223"><a href="#cb12-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-224"><a href="#cb12-224" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-225"><a href="#cb12-225" aria-hidden="true" tabindex="-1"></a>\ba</span>
<span id="cb12-226"><a href="#cb12-226" aria-hidden="true" tabindex="-1"></a>\E{\ell'}</span>
<span id="cb12-227"><a href="#cb12-227" aria-hidden="true" tabindex="-1"></a>&amp;\eqdef \int_{x \in \range{x}}</span>
<span id="cb12-228"><a href="#cb12-228" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb12-229"><a href="#cb12-229" aria-hidden="true" tabindex="-1"></a>\ell'(x,\th) \p(X = x | \th) dx</span>
<span id="cb12-230"><a href="#cb12-230" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-231"><a href="#cb12-231" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \int_{x \in \range{X}}</span>
<span id="cb12-232"><a href="#cb12-232" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb12-233"><a href="#cb12-233" aria-hidden="true" tabindex="-1"></a>\paren</span>
<span id="cb12-234"><a href="#cb12-234" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb12-235"><a href="#cb12-235" aria-hidden="true" tabindex="-1"></a>\frac{d}{d\th}</span>
<span id="cb12-236"><a href="#cb12-236" aria-hidden="true" tabindex="-1"></a>\log{\p(X = x | \th)}</span>
<span id="cb12-237"><a href="#cb12-237" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-238"><a href="#cb12-238" aria-hidden="true" tabindex="-1"></a>\p(X = x | \theta) dx</span>
<span id="cb12-239"><a href="#cb12-239" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-240"><a href="#cb12-240" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= </span>
<span id="cb12-241"><a href="#cb12-241" aria-hidden="true" tabindex="-1"></a>\int_{x \in \range{X}}</span>
<span id="cb12-242"><a href="#cb12-242" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb12-243"><a href="#cb12-243" aria-hidden="true" tabindex="-1"></a>\frac</span>
<span id="cb12-244"><a href="#cb12-244" aria-hidden="true" tabindex="-1"></a>{\frac{d}{d\theta} \p(X = x | \th)}</span>
<span id="cb12-245"><a href="#cb12-245" aria-hidden="true" tabindex="-1"></a>{\p(X = x | \theta)}</span>
<span id="cb12-246"><a href="#cb12-246" aria-hidden="true" tabindex="-1"></a>\p(X = x | \theta) dx</span>
<span id="cb12-247"><a href="#cb12-247" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-248"><a href="#cb12-248" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= </span>
<span id="cb12-249"><a href="#cb12-249" aria-hidden="true" tabindex="-1"></a>\int_{x \in \range{X}}</span>
<span id="cb12-250"><a href="#cb12-250" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb12-251"><a href="#cb12-251" aria-hidden="true" tabindex="-1"></a>\frac{d}{d\theta} \p(X = x | \th) dx</span>
<span id="cb12-252"><a href="#cb12-252" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-253"><a href="#cb12-253" aria-hidden="true" tabindex="-1"></a>\ea</span>
<span id="cb12-254"><a href="#cb12-254" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-255"><a href="#cb12-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-256"><a href="#cb12-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-257"><a href="#cb12-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-258"><a href="#cb12-258" aria-hidden="true" tabindex="-1"></a>And similarly</span>
<span id="cb12-259"><a href="#cb12-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-260"><a href="#cb12-260" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-261"><a href="#cb12-261" aria-hidden="true" tabindex="-1"></a>\Exp{\ell' \ell'^{\top}} </span>
<span id="cb12-262"><a href="#cb12-262" aria-hidden="true" tabindex="-1"></a>\eqdef </span>
<span id="cb12-263"><a href="#cb12-263" aria-hidden="true" tabindex="-1"></a>\int_{x \in R(x)}</span>
<span id="cb12-264"><a href="#cb12-264" aria-hidden="true" tabindex="-1"></a>{\ell'(x,\theta)\ell'(x,\theta)^{\top}\ </span>
<span id="cb12-265"><a href="#cb12-265" aria-hidden="true" tabindex="-1"></a>\pf{X = x | \th}\ dx}</span>
<span id="cb12-266"><a href="#cb12-266" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-267"><a href="#cb12-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-268"><a href="#cb12-268" aria-hidden="true" tabindex="-1"></a>Note that $\Exp{\ell'}$ and</span>
<span id="cb12-269"><a href="#cb12-269" aria-hidden="true" tabindex="-1"></a>$\Exp{\ell'{\ell'}^{\top}}$</span>
<span id="cb12-270"><a href="#cb12-270" aria-hidden="true" tabindex="-1"></a>are functions of $\theta$ but not of $x$; </span>
<span id="cb12-271"><a href="#cb12-271" aria-hidden="true" tabindex="-1"></a>the expectation operator removed $x$.</span>
<span id="cb12-272"><a href="#cb12-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-273"><a href="#cb12-273" aria-hidden="true" tabindex="-1"></a>Also note that for most of the distributions you are familiar with</span>
<span id="cb12-274"><a href="#cb12-274" aria-hidden="true" tabindex="-1"></a>(including Gaussian, binomial, Poisson, exponential):</span>
<span id="cb12-275"><a href="#cb12-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-276"><a href="#cb12-276" aria-hidden="true" tabindex="-1"></a>$$\Exp{\ell^{'}} = 0$$</span>
<span id="cb12-277"><a href="#cb12-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-278"><a href="#cb12-278" aria-hidden="true" tabindex="-1"></a>So</span>
<span id="cb12-279"><a href="#cb12-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-280"><a href="#cb12-280" aria-hidden="true" tabindex="-1"></a>$$\einff{\theta} = \Exp{\ell^{'}{\ell^{'}}^{\top} }$$</span>
<span id="cb12-281"><a href="#cb12-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-282"><a href="#cb12-282" aria-hidden="true" tabindex="-1"></a>Moreover, for those distributions (called the "exponential family"), we</span>
<span id="cb12-283"><a href="#cb12-283" aria-hidden="true" tabindex="-1"></a>have:</span>
<span id="cb12-284"><a href="#cb12-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-285"><a href="#cb12-285" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-286"><a href="#cb12-286" aria-hidden="true" tabindex="-1"></a>\mathfrak{I} = -\Exp{\ell''}</span>
<span id="cb12-287"><a href="#cb12-287" aria-hidden="true" tabindex="-1"></a>= \Exp{- \ell''}</span>
<span id="cb12-288"><a href="#cb12-288" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-289"><a href="#cb12-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-290"><a href="#cb12-290" aria-hidden="true" tabindex="-1"></a>(see @dobson4e, §3.17), where</span>
<span id="cb12-291"><a href="#cb12-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-292"><a href="#cb12-292" aria-hidden="true" tabindex="-1"></a>$$\ell^{''} \eqdef \frac{d}{d\theta}\ell^{'(x,\theta)^{\top}} = \frac{d}{d\theta}\frac{d}{d\theta^{\top}}\ell(x,\theta)$$</span>
<span id="cb12-293"><a href="#cb12-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-294"><a href="#cb12-294" aria-hidden="true" tabindex="-1"></a>is the $p \times p$ matrix whose elements are:</span>
<span id="cb12-295"><a href="#cb12-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-296"><a href="#cb12-296" aria-hidden="true" tabindex="-1"></a>$$\ell_{ij}'' \eqdef \frac{d}{d\theta_{i}}\frac{d}{d\theta_{j}}\log{ p\left( X = x \mid \theta \right)}$$</span>
<span id="cb12-297"><a href="#cb12-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-298"><a href="#cb12-298" aria-hidden="true" tabindex="-1"></a>$\ell''$ could be called the "Hessian" of the log-likelihood</span>
<span id="cb12-299"><a href="#cb12-299" aria-hidden="true" tabindex="-1"></a>function.</span>
<span id="cb12-300"><a href="#cb12-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-301"><a href="#cb12-301" aria-hidden="true" tabindex="-1"></a>Sometimes, we use $I(\theta;x) \eqdef - \ell^{''}$ (note the</span>
<span id="cb12-302"><a href="#cb12-302" aria-hidden="true" tabindex="-1"></a>standard-font "I" here). $I(\theta;x)$ is the observed information, precision, or concentration</span>
<span id="cb12-303"><a href="#cb12-303" aria-hidden="true" tabindex="-1"></a>matrix (Negative Hessian).</span>
<span id="cb12-304"><a href="#cb12-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-305"><a href="#cb12-305" aria-hidden="true" tabindex="-1"></a>:::{.callout-important}</span>
<span id="cb12-306"><a href="#cb12-306" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Key point</span></span>
<span id="cb12-307"><a href="#cb12-307" aria-hidden="true" tabindex="-1"></a> The asymptotics of MLEs gives us</span>
<span id="cb12-308"><a href="#cb12-308" aria-hidden="true" tabindex="-1"></a>${\widehat{\theta}}_{ML} \sim N\left( \theta,\mathfrak{I}^{- 1}(\theta) \right)$,</span>
<span id="cb12-309"><a href="#cb12-309" aria-hidden="true" tabindex="-1"></a>approximately, for large sample sizes.</span>
<span id="cb12-310"><a href="#cb12-310" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-311"><a href="#cb12-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-312"><a href="#cb12-312" aria-hidden="true" tabindex="-1"></a>We can estimate $\einf^{- 1}(\theta)$ by working out</span>
<span id="cb12-313"><a href="#cb12-313" aria-hidden="true" tabindex="-1"></a>$\Ef{-\ell''}$ or</span>
<span id="cb12-314"><a href="#cb12-314" aria-hidden="true" tabindex="-1"></a>$\Ef{\ell^{'}{\ell^{'}}^{\top}}$</span>
<span id="cb12-315"><a href="#cb12-315" aria-hidden="true" tabindex="-1"></a>and plugging in $\hthml$, but sometimes we instead use</span>
<span id="cb12-316"><a href="#cb12-316" aria-hidden="true" tabindex="-1"></a>$\oinf(\hthml,\vx)$ for convenience; there are</span>
<span id="cb12-317"><a href="#cb12-317" aria-hidden="true" tabindex="-1"></a>some cases where it’s provably better according to some criteria (@efron1978assessing).</span>
<span id="cb12-318"><a href="#cb12-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-319"><a href="#cb12-319" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gradient ascent</span></span>
<span id="cb12-320"><a href="#cb12-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-321"><a href="#cb12-321" aria-hidden="true" tabindex="-1"></a>Note that later, when we are trying to find MLEs for likelihoods that we</span>
<span id="cb12-322"><a href="#cb12-322" aria-hidden="true" tabindex="-1"></a>can’t easily differentiate, we will "hill-climb" using the</span>
<span id="cb12-323"><a href="#cb12-323" aria-hidden="true" tabindex="-1"></a>Newton-Raphson algorithm:</span>
<span id="cb12-324"><a href="#cb12-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-325"><a href="#cb12-325" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-326"><a href="#cb12-326" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb12-327"><a href="#cb12-327" aria-hidden="true" tabindex="-1"></a>\widehat{\theta} </span>
<span id="cb12-328"><a href="#cb12-328" aria-hidden="true" tabindex="-1"></a>&amp;\leftarrow \widehat{\theta} + \left\lbrack I\left( \widehat{\theta},y \right) \right\rbrack^{- 1}\ell^{'}\left( y,\widehat{\theta} \right)<span class="sc">\\</span></span>
<span id="cb12-329"><a href="#cb12-329" aria-hidden="true" tabindex="-1"></a>&amp;= \widehat{\theta} - \left\lbrack \ell^{''}\left( y,\widehat{\theta} \right) \right\rbrack^{- 1}\ell^{'}\left( y,\widehat{\theta} \right)</span>
<span id="cb12-330"><a href="#cb12-330" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb12-331"><a href="#cb12-331" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-332"><a href="#cb12-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-333"><a href="#cb12-333" aria-hidden="true" tabindex="-1"></a>Here, for computational simplicity, we will sometimes use</span>
<span id="cb12-334"><a href="#cb12-334" aria-hidden="true" tabindex="-1"></a>$\mathfrak{I}^{- 1}(\theta)$ in place of</span>
<span id="cb12-335"><a href="#cb12-335" aria-hidden="true" tabindex="-1"></a>$I\left( \widehat{\theta},y \right)$; doing so is called "Fisher</span>
<span id="cb12-336"><a href="#cb12-336" aria-hidden="true" tabindex="-1"></a>scoring" or the "method of scoring". Note that this is the opposite of</span>
<span id="cb12-337"><a href="#cb12-337" aria-hidden="true" tabindex="-1"></a>the substitution that we are making for estimating the variance of the</span>
<span id="cb12-338"><a href="#cb12-338" aria-hidden="true" tabindex="-1"></a>MLE; this time we should technically use the observed information but we</span>
<span id="cb12-339"><a href="#cb12-339" aria-hidden="true" tabindex="-1"></a>use the expected information instead.</span>
<span id="cb12-340"><a href="#cb12-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-341"><a href="#cb12-341" aria-hidden="true" tabindex="-1"></a>There’s also an "empirical information matrix" (see McLachlan and</span>
<span id="cb12-342"><a href="#cb12-342" aria-hidden="true" tabindex="-1"></a>Krishnan 2007).</span>
<span id="cb12-343"><a href="#cb12-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-344"><a href="#cb12-344" aria-hidden="true" tabindex="-1"></a>$$I_{e}(\theta,y) = \sum_{i = 1}^{n}{\ell_{i}^{'}\ {\ell_{i}^{'}}^{\top}} - \frac{1}{n}\ell^{'}{\ell^{'}}^{\top}$$</span>
<span id="cb12-345"><a href="#cb12-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-346"><a href="#cb12-346" aria-hidden="true" tabindex="-1"></a>where $\ell_{i}$ is the log-likelihood of the ith observation.</span>
<span id="cb12-347"><a href="#cb12-347" aria-hidden="true" tabindex="-1"></a>Note that $\ell^{'} = \sum_{i = 1}^{n}\ell_{i}^{'}$.</span>
<span id="cb12-348"><a href="#cb12-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-349"><a href="#cb12-349" aria-hidden="true" tabindex="-1"></a>$\frac{1}{n}I_{e}(\theta,y)$ is the sample equivalent of</span>
<span id="cb12-350"><a href="#cb12-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-351"><a href="#cb12-351" aria-hidden="true" tabindex="-1"></a>$$\mathfrak{I \eqdef I(}\theta) \eqdef {Cov}\left( \ell^{'}|\theta \right) = E\left\lbrack \ell^{'}{\ell^{'}}^{\top} \right\rbrack - E\left\lbrack \ell^{'} \right\rbrack\ E\left\lbrack \ell^{'} \right\rbrack^{\top}$$</span>
<span id="cb12-352"><a href="#cb12-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-353"><a href="#cb12-353" aria-hidden="true" tabindex="-1"></a>$$\left<span class="sc">\{</span> \mathfrak{I}_{jk} \eqdef {Cov}\left( {\ell^{'}}_{j},{\ell^{'}}_{k} \right) = E\left\lbrack \ell_{j}^{'}\ell_{k}^{'} \right\rbrack - E\left\lbrack {\ell^{'}}_{j} \right\rbrack E\left\lbrack {\ell^{'}}_{k} \right\rbrack \right<span class="sc">\}</span>$$</span>
<span id="cb12-354"><a href="#cb12-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-355"><a href="#cb12-355" aria-hidden="true" tabindex="-1"></a>$I_{e}(\theta,y)$ is sometimes computationally easier to compute for</span>
<span id="cb12-356"><a href="#cb12-356" aria-hidden="true" tabindex="-1"></a>Newton-Raphson-type maximization algorithms.</span>
<span id="cb12-357"><a href="#cb12-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-358"><a href="#cb12-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-359"><a href="#cb12-359" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Confidence intervals for MLEs</span></span>
<span id="cb12-360"><a href="#cb12-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-361"><a href="#cb12-361" aria-hidden="true" tabindex="-1"></a>An asymptotic approximation of a 95% confidence interval for $\theta_k$ is</span>
<span id="cb12-362"><a href="#cb12-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-363"><a href="#cb12-363" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-364"><a href="#cb12-364" aria-hidden="true" tabindex="-1"></a>\hthml \pm z_{0.975} \times \HSE{\hth_k}</span>
<span id="cb12-365"><a href="#cb12-365" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-366"><a href="#cb12-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-367"><a href="#cb12-367" aria-hidden="true" tabindex="-1"></a>where $z_\beta$ the $\beta$ quantile of the standard Gaussian distribution.</span>
<span id="cb12-368"><a href="#cb12-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-369"><a href="#cb12-369" aria-hidden="true" tabindex="-1"></a><span class="fu">#### p-values and hypothesis tests for MLEs</span></span>
<span id="cb12-370"><a href="#cb12-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-371"><a href="#cb12-371" aria-hidden="true" tabindex="-1"></a>(to add)</span>
<span id="cb12-372"><a href="#cb12-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-373"><a href="#cb12-373" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Likelihood ratio tests for MLEs</span></span>
<span id="cb12-374"><a href="#cb12-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-375"><a href="#cb12-375" aria-hidden="true" tabindex="-1"></a>log(likelihood ratio) tests <span class="co">[</span><span class="ot">c.f. @dobson4e §5.7</span><span class="co">]</span>:</span>
<span id="cb12-376"><a href="#cb12-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-377"><a href="#cb12-377" aria-hidden="true" tabindex="-1"></a>$$-2\ell_{0} \sim \chi^{2}(p - q)$$</span>
<span id="cb12-378"><a href="#cb12-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-379"><a href="#cb12-379" aria-hidden="true" tabindex="-1"></a>See also <span class="ot">&lt;https://online.stat.psu.edu/stat504/book/export/html/657&gt;</span></span>
<span id="cb12-380"><a href="#cb12-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-381"><a href="#cb12-381" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Prediction intervals for MLEs</span></span>
<span id="cb12-382"><a href="#cb12-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-383"><a href="#cb12-383" aria-hidden="true" tabindex="-1"></a>$$\overline{X} \in \left\lbrack \widehat{\mu} \pm z_{1 - \alpha\text{/}2}\frac{\sigma}{m} \right\rbrack$$</span>
<span id="cb12-384"><a href="#cb12-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-385"><a href="#cb12-385" aria-hidden="true" tabindex="-1"></a>Where $m$ is the sample size of the new data to be predicted (typically</span>
<span id="cb12-386"><a href="#cb12-386" aria-hidden="true" tabindex="-1"></a>1, except for binary outcomes, where it needs to be bigger for</span>
<span id="cb12-387"><a href="#cb12-387" aria-hidden="true" tabindex="-1"></a>prediction intervals to make sense)</span>
<span id="cb12-388"><a href="#cb12-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-389"><a href="#cb12-389" aria-hidden="true" tabindex="-1"></a><span class="ot">[^1]: </span>I might sometimes switch the order of $x,$ $\theta$; this is</span>
<span id="cb12-390"><a href="#cb12-390" aria-hidden="true" tabindex="-1"></a>    unintentional and not meaningful.</span>
<span id="cb12-391"><a href="#cb12-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-392"><a href="#cb12-392" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Maximum likelihood for Tropical Cyclones in Australia</span></span>
<span id="cb12-393"><a href="#cb12-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-394"><a href="#cb12-394" aria-hidden="true" tabindex="-1"></a>{{&lt; include dobson-cyclone-example.qmd &gt;}}</span>
<span id="cb12-395"><a href="#cb12-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-396"><a href="#cb12-396" aria-hidden="true" tabindex="-1"></a><span class="fu">## Maximum likelihood inference for univariate Gaussian models</span></span>
<span id="cb12-397"><a href="#cb12-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-398"><a href="#cb12-398" aria-hidden="true" tabindex="-1"></a>Suppose $X_{1}, ..., X_{n} \siid N(\mu, \sigma^{2})$.</span>
<span id="cb12-399"><a href="#cb12-399" aria-hidden="true" tabindex="-1"></a>Let $X = (X_{1},\ldots,X_{n})^{\top}$ be these random</span>
<span id="cb12-400"><a href="#cb12-400" aria-hidden="true" tabindex="-1"></a>variables in vector format. Let $x_{i}$ and $x$ denote the corresponding</span>
<span id="cb12-401"><a href="#cb12-401" aria-hidden="true" tabindex="-1"></a>observed data. Then $\theta = (\mu,\sigma^{2})$ is</span>
<span id="cb12-402"><a href="#cb12-402" aria-hidden="true" tabindex="-1"></a>the vector of true parameters, and $\Theta = (\Mu, \Sigma^2)$ is the vector of parameters as a random</span>
<span id="cb12-403"><a href="#cb12-403" aria-hidden="true" tabindex="-1"></a>vector.</span>
<span id="cb12-404"><a href="#cb12-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-405"><a href="#cb12-405" aria-hidden="true" tabindex="-1"></a>Then the log-likelihood</span>
<span id="cb12-406"><a href="#cb12-406" aria-hidden="true" tabindex="-1"></a>is:</span>
<span id="cb12-407"><a href="#cb12-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-408"><a href="#cb12-408" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-409"><a href="#cb12-409" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb12-410"><a href="#cb12-410" aria-hidden="true" tabindex="-1"></a>\ell </span>
<span id="cb12-411"><a href="#cb12-411" aria-hidden="true" tabindex="-1"></a>&amp;\propto - \frac{n}{2}\log{\sigma^{2}} - \frac{1}{2}\sum_{i = 1}^{n}\frac{( x_{i} - \mu)^{2}}{\sigma^{2}}<span class="sc">\\</span></span>
<span id="cb12-412"><a href="#cb12-412" aria-hidden="true" tabindex="-1"></a>&amp;= - \frac{n}{2}\log{\sigma^{2}} - \frac{1}{2\sigma^{2}}\sum_{i = 1}^{n}{x_{i}^{2} - 2x_{i}\mu + \mu^{2}}</span>
<span id="cb12-413"><a href="#cb12-413" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb12-414"><a href="#cb12-414" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-415"><a href="#cb12-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-416"><a href="#cb12-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-417"><a href="#cb12-417" aria-hidden="true" tabindex="-1"></a><span class="fu">### The score function</span></span>
<span id="cb12-418"><a href="#cb12-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-419"><a href="#cb12-419" aria-hidden="true" tabindex="-1"></a>$$\ell^{'}(x,\theta) \eqdef \frac{d}{d\theta}\ell(x,\theta) = \left( \begin{array}{r}</span>
<span id="cb12-420"><a href="#cb12-420" aria-hidden="true" tabindex="-1"></a>\frac{d}{d\mu}\ell(\theta;x) <span class="sc">\\</span></span>
<span id="cb12-421"><a href="#cb12-421" aria-hidden="true" tabindex="-1"></a>\frac{d}{d\sigma^{2}}\ell(\theta;x)</span>
<span id="cb12-422"><a href="#cb12-422" aria-hidden="true" tabindex="-1"></a>\end{array} \right) = \left( \begin{array}{r}</span>
<span id="cb12-423"><a href="#cb12-423" aria-hidden="true" tabindex="-1"></a>\ell_{\mu}^{'}(\theta;x) <span class="sc">\\</span></span>
<span id="cb12-424"><a href="#cb12-424" aria-hidden="true" tabindex="-1"></a>\ell_{\sigma^{2}}^{'}(\theta;x)</span>
<span id="cb12-425"><a href="#cb12-425" aria-hidden="true" tabindex="-1"></a>\end{array} \right)$$.</span>
<span id="cb12-426"><a href="#cb12-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-427"><a href="#cb12-427" aria-hidden="true" tabindex="-1"></a>$\ell^{'}(x,\theta)$ is the function we set equal to 0 and solve</span>
<span id="cb12-428"><a href="#cb12-428" aria-hidden="true" tabindex="-1"></a>to find the MLE:</span>
<span id="cb12-429"><a href="#cb12-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-430"><a href="#cb12-430" aria-hidden="true" tabindex="-1"></a>$${\widehat{\theta}}_{ML} = \left<span class="sc">\{</span> \theta:\ell^{'}(x,\theta) = 0 \right<span class="sc">\}</span>$$</span>
<span id="cb12-431"><a href="#cb12-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-432"><a href="#cb12-432" aria-hidden="true" tabindex="-1"></a><span class="fu">### MLE of $\mu$</span></span>
<span id="cb12-433"><a href="#cb12-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-434"><a href="#cb12-434" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-435"><a href="#cb12-435" aria-hidden="true" tabindex="-1"></a>\ba</span>
<span id="cb12-436"><a href="#cb12-436" aria-hidden="true" tabindex="-1"></a>\frac{d\ell}{d\mu} </span>
<span id="cb12-437"><a href="#cb12-437" aria-hidden="true" tabindex="-1"></a>&amp;= - \frac{1}{2}\sum_{i = 1}^{n}</span>
<span id="cb12-438"><a href="#cb12-438" aria-hidden="true" tabindex="-1"></a>\frac{- 2(x_{i} - \mu)}{\sigma^{2}}</span>
<span id="cb12-439"><a href="#cb12-439" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \frac{1}{\sigma^{2}}</span>
<span id="cb12-440"><a href="#cb12-440" aria-hidden="true" tabindex="-1"></a>\sb{</span>
<span id="cb12-441"><a href="#cb12-441" aria-hidden="true" tabindex="-1"></a>    \paren{</span>
<span id="cb12-442"><a href="#cb12-442" aria-hidden="true" tabindex="-1"></a>        \sum_{i = 1}^{n}x_{i}</span>
<span id="cb12-443"><a href="#cb12-443" aria-hidden="true" tabindex="-1"></a>    } </span>
<span id="cb12-444"><a href="#cb12-444" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>n\mu</span>
<span id="cb12-445"><a href="#cb12-445" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-446"><a href="#cb12-446" aria-hidden="true" tabindex="-1"></a>\ea</span>
<span id="cb12-447"><a href="#cb12-447" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-448"><a href="#cb12-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-449"><a href="#cb12-449" aria-hidden="true" tabindex="-1"></a>If $\frac{d\ell}{d\mu} = 0$, then</span>
<span id="cb12-450"><a href="#cb12-450" aria-hidden="true" tabindex="-1"></a>$\mu = \overline{x} \eqdef \frac{1}{n}\sum_{i = 1}^{n}x_{i}$.</span>
<span id="cb12-451"><a href="#cb12-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-452"><a href="#cb12-452" aria-hidden="true" tabindex="-1"></a>$$\frac{d^{2}\ell}{(d\mu)^{2}} = \frac{- n}{\sigma^{2}} &lt; 0$$</span>
<span id="cb12-453"><a href="#cb12-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-454"><a href="#cb12-454" aria-hidden="true" tabindex="-1"></a>So ${\widehat{\mu}}_{ML} = \overline{x}$.</span>
<span id="cb12-455"><a href="#cb12-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-456"><a href="#cb12-456" aria-hidden="true" tabindex="-1"></a><span class="fu">### MLE of $\sigma^{2}$</span></span>
<span id="cb12-457"><a href="#cb12-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-458"><a href="#cb12-458" aria-hidden="true" tabindex="-1"></a>:::{.callout-tip}</span>
<span id="cb12-459"><a href="#cb12-459" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Reparametrizing the Gaussian distribution</span></span>
<span id="cb12-460"><a href="#cb12-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-461"><a href="#cb12-461" aria-hidden="true" tabindex="-1"></a>When solving for ${\widehat{\sigma}}_{ML}$, you can treat</span>
<span id="cb12-462"><a href="#cb12-462" aria-hidden="true" tabindex="-1"></a>$\sigma^{2}$ as an atomic variable (don’t differentiate with respect to</span>
<span id="cb12-463"><a href="#cb12-463" aria-hidden="true" tabindex="-1"></a>$\sigma$ or things get messy). In fact, you can replace $\sigma^{2}$</span>
<span id="cb12-464"><a href="#cb12-464" aria-hidden="true" tabindex="-1"></a>with $1/\tau$ and differentiate with respect to $\tau$ instead, and the</span>
<span id="cb12-465"><a href="#cb12-465" aria-hidden="true" tabindex="-1"></a>process might be even easier.</span>
<span id="cb12-466"><a href="#cb12-466" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-467"><a href="#cb12-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-468"><a href="#cb12-468" aria-hidden="true" tabindex="-1"></a>$$\frac{d\ell}{d\sigma^{2}} = \frac{d}{d\sigma^{2}}\left( - \frac{n}{2}\log{\sigma^{2}} - \frac{1}{2}\sum_{i = 1}^{n}\frac{\left( x_{i} - \mu \right)^{2}}{\sigma^{2}} \right)\ $$</span>
<span id="cb12-469"><a href="#cb12-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-470"><a href="#cb12-470" aria-hidden="true" tabindex="-1"></a>$$= - \frac{n}{2}\left( \sigma^{2} \right)^{- 1} + \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2}$$</span>
<span id="cb12-471"><a href="#cb12-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-472"><a href="#cb12-472" aria-hidden="true" tabindex="-1"></a>If $\frac{d\ell}{d\sigma^{2}} = 0$, then:</span>
<span id="cb12-473"><a href="#cb12-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-474"><a href="#cb12-474" aria-hidden="true" tabindex="-1"></a>$$\frac{n}{2}\left( \sigma^{2} \right)^{- 1} = \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2}$$</span>
<span id="cb12-475"><a href="#cb12-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-476"><a href="#cb12-476" aria-hidden="true" tabindex="-1"></a>$$\sigma^{2} = \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2}$$</span>
<span id="cb12-477"><a href="#cb12-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-478"><a href="#cb12-478" aria-hidden="true" tabindex="-1"></a>We plug in ${\widehat{\mu}}_{ML} = \overline{x}$ to maximize globally (a</span>
<span id="cb12-479"><a href="#cb12-479" aria-hidden="true" tabindex="-1"></a>technique called profiling):</span>
<span id="cb12-480"><a href="#cb12-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-481"><a href="#cb12-481" aria-hidden="true" tabindex="-1"></a>$$\sigma_{ML}^{2} = \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}$$</span>
<span id="cb12-482"><a href="#cb12-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-483"><a href="#cb12-483" aria-hidden="true" tabindex="-1"></a>Now:</span>
<span id="cb12-484"><a href="#cb12-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-485"><a href="#cb12-485" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-486"><a href="#cb12-486" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb12-487"><a href="#cb12-487" aria-hidden="true" tabindex="-1"></a>\frac{d^{2}\ell}{\left( d\sigma^{2} \right)^{2}} </span>
<span id="cb12-488"><a href="#cb12-488" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{d}{d\sigma^{2}}\left<span class="sc">\{</span> - \frac{n}{2}\left( \sigma^{2} \right)^{- 1} + \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right<span class="sc">\}\\</span></span>
<span id="cb12-489"><a href="#cb12-489" aria-hidden="true" tabindex="-1"></a>&amp;= \left<span class="sc">\{</span> - \frac{n}{2}\frac{d}{d\sigma^{2}}\left( \sigma^{2} \right)^{- 1} + \frac{1}{2}\frac{d}{d\sigma^{2}}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right<span class="sc">\}\\</span></span>
<span id="cb12-490"><a href="#cb12-490" aria-hidden="true" tabindex="-1"></a>&amp;= \left<span class="sc">\{</span> \frac{n}{2}\left( \sigma^{2} \right)^{- 2} - \left( \sigma^{2} \right)^{- 3}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right<span class="sc">\}\\</span></span>
<span id="cb12-491"><a href="#cb12-491" aria-hidden="true" tabindex="-1"></a>&amp;= \left( \sigma^{2} \right)^{- 2}\left<span class="sc">\{</span> \frac{n}{2} - \left( \sigma^{2} \right)^{- 1}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right<span class="sc">\}</span></span>
<span id="cb12-492"><a href="#cb12-492" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb12-493"><a href="#cb12-493" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-494"><a href="#cb12-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-495"><a href="#cb12-495" aria-hidden="true" tabindex="-1"></a>Evaluated at</span>
<span id="cb12-496"><a href="#cb12-496" aria-hidden="true" tabindex="-1"></a>$\mu = \overline{x},\sigma^{2} = \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}$,</span>
<span id="cb12-497"><a href="#cb12-497" aria-hidden="true" tabindex="-1"></a>we have:</span>
<span id="cb12-498"><a href="#cb12-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-499"><a href="#cb12-499" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-500"><a href="#cb12-500" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb12-501"><a href="#cb12-501" aria-hidden="true" tabindex="-1"></a>\frac{d^{2}\ell}{\left( d\sigma^{2} \right)^{2}} </span>
<span id="cb12-502"><a href="#cb12-502" aria-hidden="true" tabindex="-1"></a>&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}\left<span class="sc">\{</span> \frac{n}{2} - \left( {\widehat{\sigma}}^{2} \right)^{- 1}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2} \right<span class="sc">\}\\</span></span>
<span id="cb12-503"><a href="#cb12-503" aria-hidden="true" tabindex="-1"></a>&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}\left<span class="sc">\{</span> \frac{n}{2} - \left( {\widehat{\sigma}}^{2} \right)^{- 1}n{\widehat{\sigma}}^{2} \right<span class="sc">\}\\</span></span>
<span id="cb12-504"><a href="#cb12-504" aria-hidden="true" tabindex="-1"></a>&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}\left<span class="sc">\{</span> \frac{n}{2} - n \right<span class="sc">\}\\</span></span>
<span id="cb12-505"><a href="#cb12-505" aria-hidden="true" tabindex="-1"></a>&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}n\left<span class="sc">\{</span> \frac{1}{2} - 1 \right<span class="sc">\}\\</span></span>
<span id="cb12-506"><a href="#cb12-506" aria-hidden="true" tabindex="-1"></a>&amp;= \left( {\widehat{\sigma}}^{2} \right)^{- 2}n\left( - \frac{1}{2} \right) &lt; 0</span>
<span id="cb12-507"><a href="#cb12-507" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb12-508"><a href="#cb12-508" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-509"><a href="#cb12-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-510"><a href="#cb12-510" aria-hidden="true" tabindex="-1"></a>Finally, we have:</span>
<span id="cb12-511"><a href="#cb12-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-512"><a href="#cb12-512" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-513"><a href="#cb12-513" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb12-514"><a href="#cb12-514" aria-hidden="true" tabindex="-1"></a>\frac{d^{2}\ell}{d\mu\ d\sigma^{2}} </span>
<span id="cb12-515"><a href="#cb12-515" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{d}{d\mu}\left<span class="sc">\{</span> - \frac{n}{2}\left( \sigma^{2} \right)^{- 1} + \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right<span class="sc">\}\\</span></span>
<span id="cb12-516"><a href="#cb12-516" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\frac{d}{d\mu}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2}<span class="sc">\\</span></span>
<span id="cb12-517"><a href="#cb12-517" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{2}\left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}{- 2(x_{i} - \mu)}<span class="sc">\\</span></span>
<span id="cb12-518"><a href="#cb12-518" aria-hidden="true" tabindex="-1"></a>&amp;= - \left( \sigma^{2} \right)^{- 2}\sum_{i = 1}^{n}{(x_{i} - \mu)}</span>
<span id="cb12-519"><a href="#cb12-519" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb12-520"><a href="#cb12-520" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-521"><a href="#cb12-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-522"><a href="#cb12-522" aria-hidden="true" tabindex="-1"></a>Evaluated at</span>
<span id="cb12-523"><a href="#cb12-523" aria-hidden="true" tabindex="-1"></a>$\mu = \widehat{\mu} = \overline{x},\sigma^{2} = {\widehat{\sigma}}^{2} = \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}$,</span>
<span id="cb12-524"><a href="#cb12-524" aria-hidden="true" tabindex="-1"></a>we have:</span>
<span id="cb12-525"><a href="#cb12-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-526"><a href="#cb12-526" aria-hidden="true" tabindex="-1"></a>$$\frac{d^{2}\ell}{d\mu\ d\sigma^{2}} = - \left( {\widehat{\sigma}}^{2} \right)^{- 2}\left( n\overline{x} - n\overline{x} \right) = 0$$</span>
<span id="cb12-527"><a href="#cb12-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-528"><a href="#cb12-528" aria-hidden="true" tabindex="-1"></a><span class="fu">### Covariance matrix</span></span>
<span id="cb12-529"><a href="#cb12-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-530"><a href="#cb12-530" aria-hidden="true" tabindex="-1"></a>$$I = \begin{bmatrix}</span>
<span id="cb12-531"><a href="#cb12-531" aria-hidden="true" tabindex="-1"></a>\frac{n}{\sigma^{2}} &amp; 0 <span class="sc">\\</span></span>
<span id="cb12-532"><a href="#cb12-532" aria-hidden="true" tabindex="-1"></a>0 &amp; \left( {\widehat{\sigma}}^{2} \right)^{- 2}n\left( - \frac{1}{2} \right)</span>
<span id="cb12-533"><a href="#cb12-533" aria-hidden="true" tabindex="-1"></a>\end{bmatrix} = \begin{bmatrix}</span>
<span id="cb12-534"><a href="#cb12-534" aria-hidden="true" tabindex="-1"></a>a &amp; 0 <span class="sc">\\</span></span>
<span id="cb12-535"><a href="#cb12-535" aria-hidden="true" tabindex="-1"></a>0 &amp; d</span>
<span id="cb12-536"><a href="#cb12-536" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}$$</span>
<span id="cb12-537"><a href="#cb12-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-538"><a href="#cb12-538" aria-hidden="true" tabindex="-1"></a>So:</span>
<span id="cb12-539"><a href="#cb12-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-540"><a href="#cb12-540" aria-hidden="true" tabindex="-1"></a>$$I^{- 1} = \frac{1}{ad}\begin{bmatrix}</span>
<span id="cb12-541"><a href="#cb12-541" aria-hidden="true" tabindex="-1"></a>d &amp; 0 <span class="sc">\\</span></span>
<span id="cb12-542"><a href="#cb12-542" aria-hidden="true" tabindex="-1"></a>0 &amp; a</span>
<span id="cb12-543"><a href="#cb12-543" aria-hidden="true" tabindex="-1"></a>\end{bmatrix} = \begin{bmatrix}</span>
<span id="cb12-544"><a href="#cb12-544" aria-hidden="true" tabindex="-1"></a>\frac{1}{a} &amp; 0 <span class="sc">\\</span></span>
<span id="cb12-545"><a href="#cb12-545" aria-hidden="true" tabindex="-1"></a>0 &amp; \frac{1}{d}</span>
<span id="cb12-546"><a href="#cb12-546" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}$$</span>
<span id="cb12-547"><a href="#cb12-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-548"><a href="#cb12-548" aria-hidden="true" tabindex="-1"></a>$$I^{- 1} = \begin{bmatrix}</span>
<span id="cb12-549"><a href="#cb12-549" aria-hidden="true" tabindex="-1"></a>\frac{{\widehat{\sigma}}^{2}}{n} &amp; 0 <span class="sc">\\</span></span>
<span id="cb12-550"><a href="#cb12-550" aria-hidden="true" tabindex="-1"></a>0 &amp; \frac{{2\left( {\widehat{\sigma}}^{2} \right)}^{2}}{n}</span>
<span id="cb12-551"><a href="#cb12-551" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}$$</span>
<span id="cb12-552"><a href="#cb12-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-553"><a href="#cb12-553" aria-hidden="true" tabindex="-1"></a>See @CaseBerg01 p322, example 7.2.12.</span>
<span id="cb12-554"><a href="#cb12-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-555"><a href="#cb12-555" aria-hidden="true" tabindex="-1"></a>To prove it’s a maximum, need:</span>
<span id="cb12-556"><a href="#cb12-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-557"><a href="#cb12-557" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\ell^{'} = 0$</span>
<span id="cb12-558"><a href="#cb12-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-559"><a href="#cb12-559" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>At least one diagonal element of $\ell''$ is negative.</span>
<span id="cb12-560"><a href="#cb12-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-561"><a href="#cb12-561" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Determinant of $\ell''$ is positive.</span>
<span id="cb12-562"><a href="#cb12-562" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
<li class="nav-item">
 Copyright 2024, Douglas Ezra Morrison
  </li>  
</ul>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/d-morrison/rme/edit/main/intro-MLEs.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/d-morrison/rme/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>


</body></html>