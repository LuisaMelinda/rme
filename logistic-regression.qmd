---
title: "Logistic Regression"
date: last-modified
date-format: "[Last modified:] YYYY-MM-DD: H:mm:ss (A)"
format: 
  revealjs:
    theme: dark
    smaller: false
    number-sections: true
    slide-number: true
    scrollable: true
    echo: true
    code-fold: true
    code-link: true
    code-summary: "[R code]"
    code-overflow: scroll
    # chalkboard: true
    # fig-width: 7
    # fig-height: 5
    # out-width: "100%"
  html:
    toc: true
    number-sections: true
    code-fold: true
    code-tools: true
    code-summary: "Show the code"
    code-link: true
  pdf:
    toc: true
    number-sections: true
    code-link: true
    code-overflow: wrap
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

::: hidden
$$\def\eqdef{\stackrel{\text{def}}{=}}$$
:::

## Acknowledgements {.unnumbered}

This content is adapted from:

-   Dobson & Barnett 2018, "An Introduction to Generalized Linear
    Models", 4th edition, Chapter 7
-   Vittinghoff et al 2012, "Regression Methods in Biostatistics", 2nd
    edition, Chapter 5
- <https://dmrocke.ucdavis.edu/Class/EPI204-Spring-2021/EPI204-Spring-2021.html>    

## Configuring R {.unnumbered}

Functions from these packages will be used throughout this document:

```{r packages, message = FALSE}
library(pander) # format tables for markdown
library(ggplot2) # graphics
library(ggeasy) # help with graphics
library(dplyr) # manipulate data
library(haven) # import Stata files
library(knitr) # format R output for markdown
library(tidyr) # Tools to help to create tidy data
library(plotly) # interactive graphics
library(dobson) # datasets from Dobson and Barnett 2018
library(parameters) # format model output tables for markdown
library(conflicted) # check for conflicting function definitions
```

Here are some R settings I use in this document:

```{r options, message=FALSE}
rm(list = ls()) # delete any data that's already loaded into R
knitr::opts_chunk$set(message = FALSE)
pander::panderOptions("table.emphasize.rownames", FALSE)
options('digits' = 4)
```

# Introduction to logistic regression

## Binary outcomes models - one group, no covariates

$$
\begin{aligned}
p(Y=1) &= \pi\\
p(Y=0) &= 1-\pi\\
p(Y=y) &= \pi^y (1-\pi)^{1-y}\\
\mathbf y  &= (y_1, ..., y_n)\\
\mathcal L(\pi;\mathbf y) &= \pi^{\sum y_i} (1-\pi)^{n - \sum y_i}\\
\ell(\pi, \mathbf y) &= \left({\sum y_i}\right) \log \pi + \left(n - \sum y_i\right) \log (1-\pi)\\
&= \left({\sum y_i}\right) \left(\log \pi - \log (1-\pi)\right) + n \cdot \log (1-\pi)\\
&= \left({\sum y_i}\right) \log \left( \frac{\pi}{ 1-\pi}\right) + n \cdot \log (1-\pi)
\end{aligned}
$$

## Binary outcomes - general

$$
\begin{aligned}
p(Y_i=1) &= \pi_i\\
p(Y_i=0) &= 1-\pi_i\\
p(Y_i=y) &= (\pi_i)^y (1-\pi_i)^{1-y}\\
\mathbf y  &= (y_1, ..., y_n)\\
\mathcal L(\pi;\mathbf y) &= \prod_{i=1}^n 
(\pi_i)^{y_i} (1-\pi_i)^{1 - y_i}\\
\ell(\pi, \mathbf y) &= 
\sum_{i=1}^n 
y_i \log \pi_i + (1 - y_i) \log (1-\pi_i)
\end{aligned}
$$

## Modeling $\pi_i$ as a function of $X_i$

If there are only a few distinct $X_i$ values, we can model each one
separately.

Otherwise, we need regression.

$$
\begin{aligned}
\pi(x) &\equiv \text{E}(Y=1|X=x)\\
&= f(x^\top\beta)
\end{aligned}
$$

### expit function

Typically, we use

$$
\begin{aligned}
f(\eta) &= \text{expit}(\eta)\\
&\eqdef \frac{\exp(\eta)}{1+\exp(\eta)}\\
&= (1 + \exp\{-\eta\})^{-1}
\end{aligned}
$$

This function is called the "expit", "inverse-logit", or "logistic"
function.

```{r}
expit = function(eta) exp(eta)/(1+exp(eta))
library(ggplot2)
expit_plot = 
  ggplot() + 
  geom_function(fun = expit) + 
  xlim(-5, 5) + 
  ylim(0,1) +
  ylab(expression(expit(eta))) +
  xlab(expression(eta)) +
  theme_bw()
```

::: {.content-visible when-format="html"}
```{r}
print(expit_plot)
```
:::

::: {.content-visible when-format="pdf"}
```{r}
print(expit_plot)
```
:::

The inverse of the `expit` function is the `logit` function:

$$g(p) = f^{-1}(p) = \text{logit}(p) = \log \left\{\frac{p}{1-p}\right\}$$

### odds function

```{r}
odds = function(p) p/(1-p)
odds_plot = 
  ggplot() + 
  geom_function(fun = odds) + 
  xlim(0, .99) + 
  ylab("odds(p)") +
  xlab("p") +
  theme_bw()
```

::: {.content-visible when-format="html"}
```{r}
print(odds_plot)
```
:::

::: {.content-visible when-format="pdf"}
```{r}
print(odds_plot)
```
:::

### logit function

```{r}
logit = function(p) log(odds(p))

logit_plot = 
  ggplot() + 
  geom_function(fun = logit) + 
  xlim(.01, .99) + 
  ylab("logit(p)") +
  xlab("p") +
  theme_bw()
```

::: {.content-visible when-format="html"}
```{r}
print(logit_plot)
```
:::

::: {.content-visible when-format="pdf"}
```{r}
print(logit_plot)
```
:::

## Diagram of expit and logit {#sec-expit-logit-diagram}

::: {.content-visible when-format="pdf"}
$$
\left[ \pi  \eqdef\Pr(Y=1)\right]
\underbrace{
\overbrace{
\underset{
\xleftarrow[ \frac{\omega}{1+\omega}]{}
}
{
\xrightarrow{\frac{\pi}{1-\pi}}
}
\left[\omega \eqdef \text{odds}(Y=1)\right]
\underset{
\xleftarrow[\exp\{\eta\}]{}
}
{
\xrightarrow{\log\{\omega\}}
}
}^{\text{logit}(\pi)}
}_{\text{expit}(\eta)}
\left[\eta \eqdef \text{log-odds}(Y=1)\right]
$$
:::

::: {.content-hidden when-format="pdf"}
$$
\underbrace{\pi}_{\atop{\Pr(Y=1)} }
\overbrace{
\underbrace{
\underset{
\xleftarrow[\frac{\omega}{1+\omega}]{}
}
{
\xrightarrow{\frac{\pi}{1-\pi}}
}
\underbrace{\omega}_{\text{odds}(Y=1)}
\underset{
\xleftarrow[\exp\{\eta\}]{}
}
{
\xrightarrow{\log\{\omega\}}
}
}_{\text{expit}(\eta)}
}^{\text{logit}(\pi)}
\underbrace{\eta}_{\atop{\text{log-odds}(Y=1)}}
$$
:::

## Meet the beetles

```{r}
library(glmx)

data(BeetleMortality, package = "glmx")
beetles = BeetleMortality |>
  mutate(
    pct = died/n,
    survived = n - died
  )

plot1 = 
  beetles |> 
  ggplot(aes(x = dose, y = pct)) +
  geom_point(aes(size = n)) +
  xlab("Dose (log mg/L)") +
  ylab("Mortality rate (%)") +
  scale_y_continuous(labels = scales::percent) +
  scale_size(range = c(1,2)) +
  theme_bw(base_size = 18)

print(plot1)
```

Mortality rates of adult flour beetles after five hours' exposure to
gaseous carbon disulphide (Bliss 1935)

## Why don't we use linear regression?

```{r}


beetles_glm_grouped = beetles |> 
  glm(formula = cbind(died, survived) ~ dose, family = "binomial")

lm1 = 
  beetles  |> 
  reframe(
    .by = everything(),
    outcome = c(
      rep(1, times = died), 
      rep(0, times = survived))
  ) |> 
  lm(
    formula = outcome ~ dose, 
    data = _)

lm2 = 
  beetles  |> 
  reframe(
    .by = everything(),
    outcome = c(
      rep(1, times = died), 
      rep(0, times = survived))
  ) |> 
  lm(
    formula = outcome ~ log(dose), 
    data = _)

range1 = range(beetles$dose) + c(-.2, .2)
f = function(x) predict(beetles_glm_grouped, newdata = data.frame(dose = x), type = "response")
f.linear = function(x) predict(lm1, newdata = data.frame(dose = x))
f.linearlog = function(x) predict(lm2, newdata = data.frame(dose = x))

plot2 = 
  plot1 + 
  geom_function(
    fun = f.linear, 
    aes(col = "Straight line")) +
  labs(colour="Model", size = "")

plot2 |> print()

```

## Zoom out

```{r}
(plot2 + expand_limits(x = c(1.6, 2))) |> print()
```

## log transformation of dose?

```{r}

plot3 = plot2 + 
  expand_limits(x = c(1.6, 2)) +
  geom_function(fun = f.linearlog, aes(col = "Log-transform dose"))
(plot3 + expand_limits(x = c(1.6, 2))) |> print()

```

## Logistic regression

```{r}

plot4 = plot3 + geom_function(fun = f, aes(col = "Logistic regression"))
plot4 |> print()
```

## Three parts to regression models

-   What distribution does the outcome have for a specific subpopulation
    defined by covariates? (outcome model)

-   How does the combination of covariates relate to the mean? (link
    function)

-   How do the covariates combine? (linear predictor, interactions)

## Logistic regression in R

```{r}

beetles_glm_grouped = 
  beetles |> 
  glm(
    formula = cbind(died, survived) ~ dose, 
    family = "binomial")

beetles_glm_grouped |> 
  parameters() |> 
  print_md()

```

Fitted values:

```{r}
fitted.values(beetles_glm_grouped)
predict(beetles_glm_grouped, type = "response")
predict(beetles_glm_grouped, type = "link")

fit_y = beetles$n * fitted.values(beetles_glm_grouped)
```

## Individual observations

```{r}

beetles_long = 
  beetles  |> 
  reframe(
    .by = everything(),
    outcome = c(
      rep(1, times = died), 
      rep(0, times = survived))
  )
beetles_long |> tibble() |> print()
```

Here's the model with individual data

```{r}

beetles_glm_ungrouped = 
  beetles_long |> 
  glm(
    formula = outcome ~ dose, 
    family = "binomial")

beetles_glm_ungrouped |> parameters() |> print_md()
```

Here's the previous version again:

```{r}

beetles_glm_grouped |> parameters() |> print_md()
```

They seem the same! But not quite:

```{r}

logLik(beetles_glm_grouped)
logLik(beetles_glm_ungrouped)

```

The difference is due to the binomial coefficient
$\left(n\atop x \right)$ which isn't included in the
individual-observations (Bernoulli) version of the model.

# Multiple logistic regression

## Coronary heart disease (WCGS) study data

Let's use the data from the following study to explore multiple logistic
regression:

### Summary of study

*From Vittinghoff et al 2012:*

"The **Western Collaborative Group Study (WCGS)** was a large
epidemiological study designed to investigate the association between
the"type A" behavior pattern and coronary heart disease (CHD) (Rosenman
et al. 1964)."

*From Wikipedia, "Type A and Type B personality theory":*

"The hypothesis describes Type A individuals as outgoing, ambitious,
rigidly organized, highly status-conscious, impatient, anxious,
proactive, and concerned with time management....

The hypothesis describes Type B individuals as a contrast to those of
Type A. Type B personalities, by definition, are noted to live at lower
stress levels. They typically work steadily and may enjoy achievement,
although they have a greater tendency to disregard physical or mental
stress when they do not achieve."

### Study design

*from `?faraway::wcgs`:*

3154 healthy young men aged 39-59 from the San Francisco area were
assessed for their personality type. All were free from coronary heart
disease at the start of the research. Eight and a half years later
change in CHD status was recorded.

*Details (from `faraway::wcgs`)*

The WCGS began in 1960 with 3,524 male volunteers who were employed by
11 California companies. Subjects were 39 to 59 years old and free of
heart disease as determined by electrocardiogram. After the initial
screening, the study population dropped to 3,154 and the number of
companies to 10 because of various exclusions. The cohort comprised both
blue- and white-collar employees.

At baseline the following information was collected:

-   socio-demographic including:
-   age
-   education
-   marital status
-   income
-   occupation
-   physical and physiological including:
-   height
-   weight
-   blood pressure
-   electrocardiogram
-   corneal arcus;
-   biochemical including:
-   cholesterol and lipoprotein fractions;
-   medical and family history and use of medications;
-   behavioral data including
-   Type A interview,
-   smoking,
-   exercise
-   alcohol use.

Later surveys added data on: 

* anthropometry 
* triglycerides 
* Jenkins Activity Survey 
* caffeine use

Average follow-up continued for 8.5 years with repeat examinations.

Reference: Coronary Heart Disease in the Western Collaborative Group
Study Final Follow-up Experience of 8 1/2 Years Ray H. Rosenman, MD;
Richard J. Brand, PhD; C. David Jenkins, PhD; Meyer Friedman, MD; Reuben
Straus, MD; Moses Wurm, MD JAMA. 1975;233(8):872-877.
doi:10.1001/jama.1975.03260080034016.

## Load the data

Here, I load the data:

```{r}
#| code-fold: show

## load the data directly from a UCSF website:
# library(haven)
# url = paste0( 
#     # I'm breaking up the url into two chunks for readability
#     "https://regression.ucsf.edu/sites/g/files/",
#     "tkssra6706/f/wysiwyg/home/data/wcgs.dta")
# wcgs = haven::read_dta(url)


# I presaved the data in my project's `data` folder
library(here) # provides the `here()` function
library(fs) # provides the `path()` function
here::here() |> 
  fs::path('data/wcgs.rda') |> 
  load()

```

## Now let's do some data cleaning

```{r}
#| code-fold: show
library(arsenal) # provides `set_labels()`

wcgs = wcgs |> 
  mutate(
    age = age |> 
      arsenal::set_labels("Age (years)"),
    
    arcus = 
      arcus |> 
      as.logical() |> 
      arsenal::set_labels("Arcus Senilis"),
    
    time169 = 
      time169 |> 
      as.numeric() |> 
      arsenal::set_labels("Observation (follow up) time (days)"),
    
    dibpat =
      dibpat |> 
      as_factor() |> 
      relevel(ref = "Type A") |> 
      arsenal::set_labels("Behavioral Pattern"),
    
    typchd69 = typchd69 |> 
      labelled(
        label = "Type of CHD Event",
        labels = 
          c(
            "None" = 0, 
            "infdeath" = 1,
            "silent" = 2,
            "angina" = 3)),
    
    # turn stata-style labelled variables in to R-style factors:
    across(
      where(is.labelled), 
      haven::as_factor)
  )

```

## What's in the data {.smaller}

Here's a table of the data:

```{r, results = "asis"}
wcgs |>
  select(-c(id, uni, t1)) |>
  tableby(chd69 ~ ., data = _) |>
  summary(
    pfootnote = TRUE,
    title =
      "Baseline characteristics by CHD status at end of follow-up")

```

## Data by age and personality type

For now, we will look at the interaction between age and personality
type (`dibpat`). To make it easier to visualize the data, we summarize
the event rates for each combination of age:

```{r}

chd_grouped_data = 
  wcgs |> 
  summarize(
    .by = c(age, dibpat),
    n = n(),
    `p(chd)` = mean(chd69 == "Yes") |> 
      labelled(label = "CHD Event by 1969"),
    `odds(chd)` = `p(chd)`/(1-`p(chd)`),
    `logit(chd)` = log(`odds(chd)`)
  )

chd_grouped_data
```

## Graphical exploration

### Probability scale

```{r "graphical exploration"}

library(ggplot2)
library(ggeasy)
library(scales)
chd_plot_probs = 
  chd_grouped_data |> 
  ggplot(
    aes(
      x = age, 
      y = `p(chd)`, 
      col = dibpat)
  ) +
  geom_point(aes(size = n), alpha = .7) + 
  scale_size(range = c(1,4)) +
  geom_line() +
  theme_bw() +
  ylab("P(CHD Event by 1969)") +
  scale_y_continuous(labels = scales::label_percent()) +
  ggeasy::easy_labs()

```

::: {.content-visible when-format="html"}
```{r}
ggplotly(chd_plot_probs)
```
:::

::: {.content-visible when-format="pdf"}
```{r}
print(chd_plot_probs)
```
:::

### Odds scale

```{r}
chd_plot_odds = 
  chd_grouped_data |> 
  ggplot(
    aes(
      x = age, 
      y = `odds(chd)`, 
      col = dibpat)
  ) +
  geom_point(aes(size = n), alpha = .7) + 
  scale_size(range = c(1,4)) +
  geom_line() +
  theme_bw() +
  ylab("odds(CHD Event by 1969)") +
  ggeasy::easy_labs()
```

::: {.content-visible when-format="html"}
```{r}
ggplotly(chd_plot_odds)
```
:::

::: {.content-visible when-format="pdf"}
```{r}
print(chd_plot_odds)
```
:::

### Log-odds (logit) scale

```{r}
chd_plot_logit = 
  chd_grouped_data |> 
  ggplot(
    aes(
      x = age, 
      y = `logit(chd)`, 
      col = dibpat)
  ) +
  geom_point(aes(size = n), alpha = .7) + 
  scale_size(range = c(1,4)) +
  geom_line() +
  theme_bw() +
  ylab("log{odds(CHD Event by 1969)}") +
  ggeasy::easy_labs()
```

::: {.content-visible when-format="html"}
```{r}
ggplotly(chd_plot_logit)
```
:::

::: {.content-visible when-format="pdf"}
```{r}
print(chd_plot_logit)
```
:::

## Logistic regression models for CHD data {.smaller}

Here, we fit stratified models for CHD by personality type.

```{r}

chd_glm_strat = glm(
  "formula" = chd69 == "Yes" ~ dibpat + dibpat:age - 1, 
  "data" = wcgs,
  "family" = binomial(link = "logit")
)

chd_glm_strat |> parameters() |> print_md()

```

We can get the corresponding odds ratios ($e^{\beta}$s) by passing
`exponentiate = TRUE` to `parameters()`:

```{r}
chd_glm_strat |> 
  parameters(exponentiate = TRUE) |> 
  print_md()
```

## Models superimposed on data

We can graph our fitted models on each scale (probability, odds,
log-odds).

### probability scale

```{r}

curve_type_A = function(x) 
{
  chd_glm_strat |> predict(
    type = "response",
    newdata = tibble(age = x, dibpat = "Type A"))
}

curve_type_B = function(x) 
{
  chd_glm_strat |> predict(
    type = "response",
    newdata = tibble(age = x, dibpat = "Type B"))
}

chd_plot_probs_2 =
  chd_plot_probs +
  geom_function(
    fun = curve_type_A,
    aes(col = "Type A")
  ) +
  geom_function(
    fun = curve_type_B,
    aes(col = "Type B")
  )

```

::: {.content-visible when-format="html"}
```{r}
ggplotly(chd_plot_probs_2)
```
:::

::: {.content-visible when-format="pdf"}
```{r}
print(chd_plot_probs_2)
```
:::

### odds scale

```{r}

curve_type_A = function(x) 
{
  chd_glm_strat |> predict(
    type = "link",
    newdata = tibble(age = x, dibpat = "Type A")) |> exp()
}
curve_type_B = function(x) 
{
  chd_glm_strat |> predict(
    type = "link",
    newdata = tibble(age = x, dibpat = "Type B")) |> exp()
}

chd_plot_odds_2 =
  chd_plot_odds +
  geom_function(
    fun = curve_type_A,
    aes(col = "Type A")
  ) +
  geom_function(
    fun = curve_type_B,
    aes(col = "Type B")
  )


```

::: {.content-visible when-format="html"}
```{r}
ggplotly(chd_plot_odds_2)
```
:::

::: {.content-visible when-format="pdf"}
```{r}
print(chd_plot_odds_2)
```
:::

### log-odds (logit) scale

```{r}

curve_type_A = function(x) 
{
  chd_glm_strat |> predict(
    type = "link",
    newdata = tibble(age = x, dibpat = "Type A"))
}
curve_type_B = function(x) 
{
  chd_glm_strat |> predict(
    type = "link",
    newdata = tibble(age = x, dibpat = "Type B"))
}

chd_plot_logit_2 =
  chd_plot_logit +
  geom_function(
    fun = curve_type_A,
    aes(col = "Type A")
  ) +
  geom_function(
    fun = curve_type_B,
    aes(col = "Type B")
  )


```

::: {.content-visible when-format="html"}
```{r}
ggplotly(chd_plot_logit_2)
```
:::

::: {.content-visible when-format="pdf"}
```{r}
print(chd_plot_logit_2)
```
:::

## reference-group and contrast parametrization

We can also use the corner-point parametrization (with reference groups
and contrasts):

```{r}

chd_glm_contrasts = 
  wcgs |> 
  glm(
    "data" = _,
    "formula" = chd69 == "Yes" ~ dibpat*age, 
    "family" = binomial(link = "logit")
  )

chd_glm_contrasts |> 
  parameters() |> 
  print_md()
```

Compare with what we had before:

```{r}
chd_glm_strat |> 
  parameters() |> 
  print_md()
```

If I give you model 1, how would you get the coefficients of model 2?

# Fitting logistic regression models

## 

In general, the estimating equation $\ell'(\beta; \mathbf x) = 0$ cannot
be solved analytically.

Instead, we have to use a variant of the Newton-Raphson method, which
was discussed briefly in Epi 203. We won't go over it in this class; if
you need to learn it, see Chapter 4 of Dobson and Barnett.

For now, all you need to know is that we make an iterative series of
guesses, and each guess helps us make the next guess better (higher
log-likelihood).

You can see some information about this process like so:

```{r}
options(digits = 8)
temp = 
  wcgs |> 
  glm(
    control = glm.control(trace = TRUE),
    "data" = _,
    "formula" = chd69 == "Yes" ~ dibpat*age, 
    "family" = binomial(link = "logit")
  )

```

After each iteration of the fitting procedure, the deviance
($2(\ell_{\text{full}} - \ell(\hat\beta))$ ) is printed. You can see
that the algorithm took six iterations to converge to a solution where
the likelihood wasn't changing much anymore.

# Model comparisons for logistic models {#sec-gof}

## Deviance test

We can compare the maximized log-likelihood of our model,
$\ell(\hat\beta; \mathbf x)$, versus the log-likelihood of the full
model (aka saturated model aka maximal model), $\ell_{\text{full}}$,
which has one parameter per covariate pattern. With enough data,
$2(\ell_{\text{full}} - \ell(\hat\beta; \mathbf x)) \dot \sim \chi^2(N - p)$,
where $N$ is the number of distinct covariate patterns and $p$ is the
number of $\beta$ parameters in our model. A significant p-value for
this **deviance** statistic indicates that there's some detectable
pattern in the data that our model isn't flexible enough to catch.

::: callout-caution
The deviance statistic needs to have a large amount of data **for each
covariate pattern** for the $\chi^2$ approximation to hold. A guideline
from Dobson is that if there are $q$ distinct covariate patterns
$x_1...,x_q$, with $n_1,...,n_q$ observations per pattern, then the
expected frequencies $n_k \cdot \pi(x_k)$ should be at least 1 for every
pattern $k\in 1:q$.
:::

If you have covariates measured on a continuous scale, you may not be
able to use the deviance tests to assess goodness of fit.

## Hosmer-Lemeshow test

If our covariate patterns produce groups that are too small, a
reasonable solution is to make bigger groups by merging some of the
covariate-pattern groups together.

Hosmer and Lemeshow (1980) proposed that we group the patterns by their
predicted probabilities according to the model of interest. For example,
you could group all of the observations with predicted probabilities of
10% or less together, then group the observations with 11%-20%
probability together, and so on; $g=10$ categories in all.

Then we can construct a statistic
$$X^2 = \sum_{c=1}^g \frac{(o_c - e_c)^2}{e_c}$$ where $o_c$ is the
number of events *observed* in group $c$, and $e_c$ is the number of
events expected in group $c$ (based on the sum of the fitted values
$\hat\pi_i$ for observations in group $c$).

If each group has enough observations in it, you can compare $X^2$ to a
$\chi^2$ distribution; by simulation, the degrees of freedom has been
found to be approximately $g-2$.

For our CHD model, this procedure would be:

```{r}
wcgs = 
  wcgs |> 
  mutate(
    pred_probs_glm1 = chd_glm_strat |> fitted(),
    pred_prob_cats1 = 
      pred_probs_glm1 |> 
      cut(breaks = seq(0, 1, by = .1), 
          include.lowest = TRUE))

HL_table = 
  wcgs |> 
  summarize(
    .by = pred_prob_cats1,
    n = n(),
    o = sum(chd69 == "Yes"),
    e = sum(pred_probs_glm1)
  )

HL_table |> pander()

X2 = HL_table |> 
  summarize(
    `X^2` = sum((o-e)^2/e)
  ) |> 
  pull(`X^2`)
print(X2)

pval1 = pchisq(X2, lower = FALSE, df = nrow(HL_table) - 2)
```

Our statistic is $X^2 = `r X2`$; $p(\chi^2(1) > `r X2`) = `r pval1`$,
which is our p-value for detecting a lack of goodness of fit.

Unfortunately that grouping plan left us with just three categories with
any observations, so instead of grouping by 10% increments of predicted
probability, typically analysts use deciles of the predicted
probabilities:

```{r}
wcgs = 
  wcgs |> 
  mutate(
    pred_probs_glm1 = chd_glm_strat |> fitted(),
    pred_prob_cats1 = 
      pred_probs_glm1 |> 
      cut(breaks = quantile(pred_probs_glm1, seq(0, 1, by = .1)), 
          include.lowest = TRUE))

HL_table = 
  wcgs |> 
  summarize(
    .by = pred_prob_cats1,
    n = n(),
    o = sum(chd69 == "Yes"),
    e = sum(pred_probs_glm1)
  )

HL_table |> pander()

X2 = HL_table |> 
  summarize(
    `X^2` = sum((o-e)^2/e)
  ) |> 
  pull(`X^2`)

print(X2)

pval1 = pchisq(X2, lower = FALSE, df = nrow(HL_table) - 2)
```

Now we have more evenly split categories. The p-value is $`r pval1`$,
still not significant.

Graphically, we have compared:

```{r}

HL_plot = 
  HL_table |> 
  ggplot(aes(x = pred_prob_cats1)) + 
  geom_line(aes(y = e, x = pred_prob_cats1, group = "Expected", col = "Expected")) +
  geom_point(aes(y = e, size = n, col = "Expected")) +
  geom_point(aes(y = o, size = n, col = "Observed")) +
  geom_line(aes(y = o, col = "Observed", group = "Observed")) +
  scale_size(range = c(1,4)) +
  theme_bw() +
  ylab("number of CHD events") +
  theme(axis.text.x = element_text(angle = 45))
```

::: {.content-visible when-format="html"}
```{r}
ggplotly(HL_plot)
```
:::

::: {.content-visible when-format="pdf"}
```{r}
print(HL_plot)
```
:::

## Comparing models

-   AIC = $-2 * \ell(\hat\theta) + 2 * p$ \[lower is better\]
-   BIC = $-2 * \ell(\hat\theta) + p * \text{log}(n)$ \[lower is
    better\]
-   likelihood ratio \[higher is better\]

# Residual-based diagnostics

## Logistic regression residuals only work for grouped data

Residuals only work if there is more than one observation for most
covariate patterns.

Here we will create the grouped-data version of our CHD model from the
WCGS study:

```{r}

wcgs_grouped = 
  wcgs |> 
  summarize(
    .by = c(dibpat, age),
    n = n(),
    chd = sum(chd69 == "Yes"),
    `!chd` = sum(chd69 == "No")
  )

chd_glm_strat_grouped = glm(
  "formula" = cbind(chd, `!chd`) ~ dibpat + dibpat:age - 1, 
  "data" = wcgs_grouped,
  "family" = binomial(link = "logit")
)

chd_glm_strat_grouped |> parameters() |> print_md()

```

## (Response) residuals

$$e_k \eqdef \bar y_k - \hat{\pi}(x_k)$$

($k$ indexes the covariate patterns)

We can graph these residuals $e_k$ against the fitted values
$\hat\pi(x_k)$:

```{r}
#| code-fold: show
wcgs_grouped = 
  wcgs_grouped |> 
  mutate(
    fitted = chd_glm_strat_grouped |> fitted(),
    fitted_logit = fitted |> logit(),
    response_resids = 
      chd_glm_strat_grouped |> resid(type = "response")
  )

wcgs_response_resid_plot = 
  wcgs_grouped |> 
  ggplot(
    mapping = aes(
      x = fitted,
      y = response_resids
    )
  ) + 
  geom_point(
    aes(col = dibpat)
  ) +
  geom_hline(yintercept = 0) + 
  geom_smooth( #<1>
    se = TRUE,  #<1>
    method.args = list( #<1>
      span=2/3, #<1>
      degree=1, #<1>
      family="symmetric", #<1>
      iterations=3), #<1>
    method = stats::loess) #<1>

```

1.  Don't worry about these options for now; I chose them to match
    `autoplot()` as closely as I can. `plot.glm` and `autoplot` use
    `stats::lowess` instead of `stats::loess`; `stats::lowess` is older,
    hard to use with `geom_smooth`, and hard to match exactly with
    `stats::loess`; see https://support.bioconductor.org/p/2323/.\]

::: {.content-visible when-format="pdf"}
```{r}
#| fig-height: 6
wcgs_response_resid_plot |> print()
```
:::

::: {.content-visible when-format="html"}
```{r}
wcgs_response_resid_plot |> ggplotly()
```
:::

We can see a slight fan-shape here: observations on the right have
larger variance (as expected since $var(\bar y) = \pi(1-\pi)/n$ is
maximized when $\pi = 0.5$).

## Pearson residuals

The fan-shape in the response residuals plot isn't necessarily a concern
here, since we haven't made an assumption of constant residual variance,
as we did for linear regression.

However, we might want to divide by the standard error in order to make
the graph easier to interpret. Here's one way to do that:

The Pearson (chi-squared) residual for covariate pattern $k$ is: $$
\begin{aligned}
X_k &= \frac{\bar y_k - \hat\pi_k}{\sqrt{\hat \pi_k (1-\hat\pi_k)/n_k}}
\end{aligned}
$$

where $$
\begin{aligned}
\hat\pi_k 
&\eqdef \hat\pi(x_k)\\
&\eqdef \hat P(Y=1|X=x_k)\\ 
&\eqdef \text{expit}(x_i'\hat \beta)\\
&\eqdef \text{expit}(\hat \beta_0 + \sum_{j=1}^p \hat \beta_j x_{ij})
\end{aligned}
$$

Let's take a look at the Pearson residuals for our CHD model from the
WCGS data (graphed against the fitted values on the logit scale):

```{r}
library(ggfortify)
```

::: {.content-visible when-format="pdf"}
```{r}
#| fig-height: 6
autoplot(chd_glm_strat_grouped, which = 1, ncol = 1) |> print()
```
:::

::: {.content-visible when-format="html"}
```{r}
autoplot(chd_glm_strat_grouped, which = 1, ncol = 1) |> print()
```
:::

The fan-shape is gone, and these residuals don't show any obvious signs
of model fit issues.

### Pearson residuals plot for `beetles` data

If we create the same plot for the `beetles` model, we see some strong
evidence of a lack of fit:

::: {.content-visible when-format="html"}
```{r}
autoplot(beetles_glm_grouped, which = 1, ncol = 1) |> print()
```
:::

::: {.content-visible when-format="pdf"}
```{r}
#| fig-height: 6
autoplot(beetles_glm_grouped, which = 1, ncol = 1) |> print()
```
:::

### Pearson residuals with individual (ungrouped) data

What happens if we try to compute residuals without grouping the data by
covariate pattern?

```{r}
library(ggfortify)
```

::: {.content-visible when-format="html"}
```{r}
autoplot(chd_glm_strat, which = 1, ncol = 1) |> print()
```
:::

::: {.content-visible when-format="pdf"}
```{r}
#| fig-height: 6
autoplot(chd_glm_strat, which = 1, ncol = 1) |> print()
```
:::

Meaningless.

### Residuals plot by hand (*optional section*)

If you want to check your understanding of what these residual plots
are, try building them yourself:

```{r}

wcgs_grouped = 
  wcgs_grouped |> 
  mutate(
    fitted = chd_glm_strat_grouped |> fitted(),
    fitted_logit = fitted |> logit(),
    resids = chd_glm_strat_grouped |> resid(type = "pearson")
  )

wcgs_resid_plot1 = 
  wcgs_grouped |> 
  ggplot(
    mapping = aes(
      x = fitted_logit,
      y = resids
      
    ) 
    
  ) + 
  geom_point(
    aes(col = dibpat)
  ) +
  geom_hline(yintercept = 0) + 
  geom_smooth(se = FALSE, 
              method.args = list(
                span=2/3,
                degree=1,
                family="symmetric",
                iterations=3,
                surface="direct"
                # span = 2/3, 
                # iterations = 3
              ),
              method = stats::loess)
# plot.glm and autoplot use stats::lowess, which is hard to use with 
# geom_smooth and hard to match exactly; 
# see https://support.bioconductor.org/p/2323/

```

::: {.content-visible when-format="pdf"}
```{r}
#| fig-height: 6
wcgs_resid_plot1 |> print()
```
:::

::: {.content-visible when-format="html"}
```{r}
wcgs_resid_plot1 |> ggplotly()
```
:::

## Pearson chi-squared goodness of fit test

The Pearson chi-squared goodness of fit statistic is: $$
X^2 = \sum_{k=1}^m X_k^2 
$$ Under the null hypothesis that the model in question is correct
(i.e., sufficiently complex), $X^2\ \dot \sim\ \chi^2(N-p)$.

```{r}

X = chd_glm_strat_grouped |> 
  resid(type = "pearson")

chisq_stat = sum(X^2)

pval = pchisq(
  chisq_stat, 
  lower = FALSE, 
  df = length(X) - length(coef(chd_glm_strat_grouped)))


```

For our CHD model, the p-value for this test is `r pval`; no significant
evidence of a lack of fit at the 0.05 level.

### Standardized Pearson residuals

Especially for small data sets, we might want to adjust our residuals
for leverage (since outliers in $X$ add extra variance to the
residuals):

$$r_{P_k} = \frac{X_k}{\sqrt{1-h_k}}$$

where $h_k$ is the leverage of $X_k$. The functions `autoplot()` and
`plot.lm()` use these for some of their graphs.

## Deviance residuals

For large sample sizes, the Pearson and deviance residuals will be
approximately the same. For small sample sizes, the deviance residuals
from covariate patterns with small sample sizes can be unreliable (high
variance).

$$d_k = \text{sign}(y_k - n_k \hat \pi_k)\left\{\sqrt{2[\ell_{\text{full}}(x_k) - \ell(\hat\beta; x_k)]}\right\}$$

### Standardized deviance residuals

$$r_{D_k} = \frac{d_k}{\sqrt{1-h_k}}$$

## Diagnostic plots

Let's take a look at the full set of `autoplot()` diagnostics now for
our `CHD` model:

::: {.content-visible when-format="html"}
```{r}
chd_glm_strat_grouped |> autoplot(which = 1:6) |> print()
```
:::

::: {.content-visible when-format="pdf"}
```{r}
#| fig-height: 6
chd_glm_strat_grouped |> autoplot(which = 1:6) |> print()
```
:::

Things look pretty good here. The QQ plot is still usable; with large
samples; the residuals should be approximately Gaussian.

### Beetles

Let's look at the beetles model diagnostic plots for comparison:

::: {.content-visible when-format="html"}
```{r}
beetles_glm_grouped |> autoplot(which = 1:6) |> print()
```
:::

::: {.content-visible when-format="pdf"}
```{r}
#| fig-height: 6
beetles_glm_grouped |> autoplot(which = 1:6) |> print()
```
:::

Hard to tell much from so little data, but there might be some issues
here.

# Odds Ratios vs Probability (Risk) Ratios {#sec-OR-RR}

::: {#def-rr}
## Relative risk

The **relative risk** comparing two probabilities $\pi_1$ and $\pi_2$,
also known as the **risk ratio**, **relative risk ratio**, **probability
ratio**, and **rate ratio**, is:

$$\rho(1,2) \eqdef \frac{\pi_1}{\pi_2}$$
:::

## 

### Case 1: rare events

For rare events, odds ratios and probability (a.k.a. risk, a.k.a.
prevalence) ratios will be close:

$\pi_1 = .01$ $\pi_2 = .02$

```{r "rare events or-rr"}
pi1 = .01
pi2 = .02
pi2/pi1
odds(pi2)/odds(pi1)

```

### Case 2: frequent events

$\pi_1 = .4$ $\pi_2 = .5$

For more frequently-occurring outcomes, this won't be the case:

```{r}
pi1 = .4
pi2 = .5
pi2/pi1
odds(pi2)/odds(pi1)

```

If you want risk ratios, you can sometimes get them by changing the link
function:

```{r}

data(anthers, package = "dobson")
anthers.sum<-aggregate(
  anthers[c("n","y")], 
  by=anthers[c("storage")],FUN=sum) 

anthers_glm_log = glm(
  formula = cbind(y,n-y)~storage,
  data=anthers.sum, 
  family=binomial(link="log"))

anthers_glm_log |> parameters() |> print_md()

```

Now $\exp(\beta)$ gives us risk ratios instead of odds ratios:

```{r}
anthers_glm_log |> parameters(exponentiate = TRUE) |> print_md()

```

Let's compare this model with a logistic model:

```{r}

anthers_glm_logit = glm(
  cbind(y,n-y)~storage,
  data=anthers.sum, 
  family=binomial(link="logit"))

anthers_glm_logit |> parameters(exponentiate = TRUE) |> print_md()

```

\[to add: fitted plots on each outcome scale\]

When I try to use `link ="log"` in practice, I often get errors about
not finding good starting values for the estimation procedure. This is
likely because the model is producing fitted probabilities greater than
1.

When this happens, you can try to fit Poisson regression models instead
(we will see those soon!). But then the outcome distribution isn't quite
right, and you won't get warnings about fitted probabilities greater
than 1. In my opinion, the Poisson model for binary outcomes is
confusing and not very appealing.
