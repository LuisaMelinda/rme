[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression Models for Epidemiology",
    "section": "",
    "text": "Preface\nThis web-book is derived from my lecture slides for the Spring 2023 session of Epidemiology 204: “Quantitative Epidemiology III: Statistical Models”, at UC Davis.\nI have drawn these materials from many sources, including but not limited to:\n\nhttps://www.taylorfrancis.com/books/mono/10.1201/9781315182780/introduction-generalized-linear-models-adrian-barnett-annette-dobson\nhttps://dmrocke.ucdavis.edu/Class/EPI204-Spring-2021/EPI204-Spring-2021.html\nhttps://link.springer.com/book/10.1007/978-1-4614-1353-0"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Linear-models-overview.html",
    "href": "Linear-models-overview.html",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "",
    "text": "4 Understanding Gaussian Linear Regression Models\nResearch question: is there really an interaction between sex and age?\n\\(H_0: \\beta_3 = 0\\)\n\\(H_A: \\beta_3 \\neq 0\\)\n\\(P(|\\hat\\beta_3| &gt; |-18.4172| \\mid H_0)\\) = ?\nIf we have a lot of covariates in our dataset, we might want to choose a small subset to use in our model.\nThere are a few possible metrics to consider for choosing a “best” model."
  },
  {
    "objectID": "Linear-models-overview.html#motivating-example-birthweights-and-gestational-age",
    "href": "Linear-models-overview.html#motivating-example-birthweights-and-gestational-age",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n4.1 Motivating example: birthweights and gestational age",
    "text": "4.1 Motivating example: birthweights and gestational age\nSuppose we want to learn about the distributions of birthweights for (human) babies born at different gestational ages and with different chromosomal sexes (Dobson and Barnett, Example 2.2.2):\n\nCodedata(\"birthweight\", package = \"dobson\")\n\nbw = \n  birthweight |&gt; \n  pivot_longer(\n    cols = everything(),\n    names_to = c(\"sex\", \".value\"),\n    names_sep = \"s \"\n  ) |&gt; \n  mutate(\n    sex = ifelse(sex == \"boy\", \"male\", \"female\"),\n    male = (sex == \"male\") |&gt; as.integer())  |&gt; \n  rename(age = `gestational age`)\n\n\n\nCodeplot1 = bw |&gt; \n  ggplot(aes(\n    x = age, \n    y = weight,\n    linetype = sex,\n    shape = sex,\n    col = sex))  +\n  theme_bw() +\n  xlab(\"Gestational age (weeks)\") +\n  ylab(\"Birthweight (grams)\") +\n  # expand_limits(y = 0, x = 0) +\n  geom_point(alpha = .7)\n\n\n\nCodeggplotly(plot1 + facet_wrap(~ sex))"
  },
  {
    "objectID": "Linear-models-overview.html#parallel-lines-regression",
    "href": "Linear-models-overview.html#parallel-lines-regression",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n4.2 Parallel lines regression",
    "text": "4.2 Parallel lines regression\nWe don’t have enough data to model the distribution of birth weight separately for each combination of gestational age and sex, so let’s instead consider a (relatively) simple model for how that distribution varies with gestational age and sex.\n\n4.2.1 Notation\nLet:\n\n\n\\(Y\\) represent birthweight (measured in grams)\n\n\\(X_1\\) represent chromosomal sex:\n\n\n\\(X_1 = 0\\) if female (XX)\n\n\\(X_1 = 1\\) if male (XY)\n\n\n\n\\(X_2\\) represent gestational age at birth (measured in weeks).\n\n\n\n\n\n\n\nNote\n\n\n\nFemale is the reference level for the categorical variable \\(X_1\\) (chromosomal sex). The choice of a reference level is arbitrary and does not limit what we can do with the resulting model; it only makes it more computationally convenient to make inferences about comparisons involving that reference group.\n\n\nNow, consider the following model:\n\\[Y \\sim N(\\mu(X_1,X_2), \\sigma^2)\\]\n\\[\\mu(X_1,X_2)\\eqdef E[Y|X_1, X_2] = \\beta_0 + \\beta_1 X_1+ \\beta_2 X_2\\]\n\n4.2.2 Implementing the Model in R\nHere’s how we can implement this model in R:\n\nCodebw_lm1 = lm(\n  formula = weight ~ sex + age, \n  data = bw)\n\nbw_lm1 |&gt; \n  parameters(show_sigma = TRUE) |&gt; \n  print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(21)\np\n\n\n\n(Intercept)\n-1773.32\n794.59\n(-3425.75, -120.89)\n-2.23\n0.037\n\n\nsex (male)\n163.04\n72.81\n(11.63, 314.45)\n2.24\n0.036\n\n\nage\n120.89\n20.46\n(78.34, 163.45)\n5.91\n&lt; .001\n\n\n\n\n\nHere’s how this model looks, superimposed on the data:\n\nCodebw = \n  bw |&gt; \n  mutate(`E[Y|X=x]` = fitted(bw_lm1)) |&gt; \n  arrange(sex, age)\n\nplot2 = \n  plot1 %+% bw +\n  geom_line(aes(y = `E[Y|X=x]`))\n\n\n\nCodeggplotly(plot2)\n\n\nParallel-slopes model of birthweight\n\n\n\n4.2.3 Model assumptions and predictions\nTo learn what this model is assuming, let’s plug in a few values.\nAccording to this model, what’s the mean birthweight for a female born at 36 weeks?\n\nCodepred_female = coef(bw_lm1)[\"(Intercept)\"] + coef(bw_lm1)[\"age\"]*36\n\n# print(pred_female)\n## built-in prediction: \n# predict(bw_lm1, newdata = tibble(sex = \"female\", age = 36))\n\n\n\\[E[Y|X_1 = 0, X_2 = 36] = \\beta_0 + \\beta_1 \\cdot 0+ \\beta_2 \\cdot 36 =  2578.8739\\]\nWhat’s the mean birthweight for a male born at 36 weeks?\n\nCodepred_male = \n  coef(bw_lm1)[\"(Intercept)\"] + \n  coef(bw_lm1)[\"sexmale\"] + \n  coef(bw_lm1)[\"age\"]*36\n\n\n\\[\nE[Y|X_1 = 1, X_2 = 36] = \\beta_0 + \\beta_1 \\cdot 1+ \\beta_2 \\cdot 36 =  2741.9132\n\\]\nWhat’s the difference in mean birthweights between males born at 36 weeks and females born at 36 weeks?\n\\[\n\\begin{aligned}\n& E[Y|X_1 = 1, X_2 = 36] - E[Y|X_1 = 0, X_2 = 36]\\\\\n&=\n2741.9132 - 2578.8739\\\\\n&=\n163.0393\n\\end{aligned}\n\\]\nShortcut:\n\\[\n\\begin{aligned}\n& E[Y|X_1 = 1, X_2 = 36] -\nE[Y|X_1 = 0, X_2 = 36]\\\\\n&= (\\beta_0 + \\beta_1 \\cdot 1+ \\beta_2 \\cdot 36) - (\\beta_0 + \\beta_1 \\cdot 0+ \\beta_2 \\cdot 36) \\\\\n&= \\beta_1 \\\\\n&=  163.0393\n\\end{aligned}\n\\]\nNote that age doesn’t show up in this difference: in other words, according to this model, the difference between females and males with the same gestational age is the same for every age.\nThat’s an assumption of the model; it’s built-in to the parametric structure, even before we plug in the estimated values of those parameters.\nThat’s why the lines are parallel."
  },
  {
    "objectID": "Linear-models-overview.html#interactions",
    "href": "Linear-models-overview.html#interactions",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n4.3 Interactions",
    "text": "4.3 Interactions\nWhat if we don’t like that parallel lines assumption?\nThen we need to allow an “interaction” between age and sex:\n\\[\nE[Y|X_1, X_2] = \\beta_0 + \\beta_1 X_1+ \\beta_2 X_2 + \\beta_3 (X_1 \\cdot X_2)\n\\]\n\nCodebw_lm2 = lm(weight ~ sex + age + sex:age, data = bw)\n\n\nHere are the estimated parameters (\\(\\beta\\)s):\n\nCodebw_lm2 |&gt; \n  parameters() |&gt; \n  print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\n\n(Intercept)\n-2141.67\n1163.60\n(-4568.90, 285.56)\n-1.84\n0.081\n\n\nsex (male)\n872.99\n1611.33\n(-2488.18, 4234.17)\n0.54\n0.594\n\n\nage\n130.40\n30.00\n(67.82, 192.98)\n4.35\n&lt; .001\n\n\nsex (male) × age\n-18.42\n41.76\n(-105.52, 68.68)\n-0.44\n0.664\n\n\n\n\n\nHere’s another way we could rewrite this model (by collecting terms involving \\(X_2\\)):\n\\[\nE[Y|X_1, X_2] = \\beta_0 + \\beta_1 X_1+ (\\beta_2 + \\beta_3 X_1) X_2\n\\]\n\n\n\n\n\n\nNote\n\n\n\nIf you want to understand a coefficient in a model with interactions, collect terms for the corresponding variable, and you will see what other variables are interacting with the variable you are interested in.\n\n\nIn this case, the coefficient \\(X_2\\) is interacting with \\(X_1\\). So the slope of \\(Y\\) with respect to \\(X_2\\) depends on the value of \\(X_2\\).\nThere is no longer a slope; we can’t talk about “the slope of birthweight with respect to age”. We can only talk about “the slope of birthweight with respect to age among males” and “the slope of birthweight with respect to age among females”.\nThen: that coefficient is the difference in means per unit change in its corresponding coefficient, when the other collected variables are set to 0.\nHere’s how this model looks, superimposed on the data:\n\nCodebw = \n  bw |&gt; \n  mutate(\n    predlm2 = predict(bw_lm2)\n  ) |&gt; \n  arrange(sex, age)\n\nplot1_interact = \n  plot1 %+% bw +\n  geom_line(aes(y = predlm2))\n\n\n\nCodeggplotly(plot1_interact)\n\n\n\n\n\nNow we can see that the lines aren’t parallel.\nTo learn what this model is assuming, let’s plug in a few values.\nAccording to this model, what’s the mean birthweight for a female born at 36 weeks?\n\nCodepred_female = coef(bw_lm2)[\"(Intercept)\"] + coef(bw_lm2)[\"age\"]*36\n\n\n\\[E[Y|X_1 = 0, X_2 = 36] = \\beta_0 + \\beta_1 \\cdot 0+ \\beta_2 \\cdot 36 + \\beta_3 \\cdot (0 * 36) =  2552.7333\\] What’s the mean birthweight for a male born at 36 weeks?\n\nCodepred_male = \n  coef(bw_lm2)[\"(Intercept)\"] + \n  coef(bw_lm2)[\"sexmale\"] + \n  coef(bw_lm2)[\"age\"]*36 + \n  coef(bw_lm2)[\"sexmale:age\"] * 36\n\n\n\\[E[Y|X_1 = 0, X_2 = 36] = \\beta_0 + \\beta_1 \\cdot 1+ \\beta_2 \\cdot 36 + \\beta_3 \\cdot 1 \\cdot 36 =  2762.7069\\]\nWhat’s the difference in mean birthweights between males born at 36 weeks and females born at 36 weeks?\n\\[\n\\begin{aligned}\n& E[Y|X_1 = 1, X_2 = 36] - E[Y|X_1 = 0, X_2 = 36]\\\\\n&= (\\beta_0 + \\beta_1 \\cdot 1+ \\beta_2 \\cdot 36 + \\beta_3 \\cdot 1 \\cdot 36)\\\\\n&\\ \\ \\ \\ \\  -(\\beta_0 + \\beta_1 \\cdot 0+ \\beta_2 \\cdot 36 + \\beta_3 \\cdot 0 \\cdot 36) \\\\\n&= \\beta_2 + \\beta_3\\cdot 36\\\\\n&=  209.9736\n\\end{aligned}\n\\]\nNote that age now does show up in the difference: in other words, according to this model, the difference in mean birthweights between females and males with the same gestational age can vary by gestational age.\nThat’s how the lines in the graph ended up non-parallel."
  },
  {
    "objectID": "Linear-models-overview.html#stratified-regression",
    "href": "Linear-models-overview.html#stratified-regression",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n4.4 Stratified regression",
    "text": "4.4 Stratified regression\nWe could re-write the interaction model as a stratified model, with a slope and intercept for each sex:\n\nCodebw_lm_strat = \n  bw |&gt; \n  lm(\n    formula = weight ~ sex + sex:age - 1, \n    data = _)\n\nbw_lm_strat |&gt; \n  parameters() |&gt; \n  print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\n\nsex (female)\n-2141.67\n1163.60\n(-4568.90, 285.56)\n-1.84\n0.081\n\n\nsex (male)\n-1268.67\n1114.64\n(-3593.77, 1056.42)\n-1.14\n0.268\n\n\nsex (female) × age\n130.40\n30.00\n(67.82, 192.98)\n4.35\n&lt; .001\n\n\nsex (male) × age\n111.98\n29.05\n(51.39, 172.57)\n3.86\n&lt; .001"
  },
  {
    "objectID": "Linear-models-overview.html#curved-line-regression",
    "href": "Linear-models-overview.html#curved-line-regression",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n4.5 Curved-line regression",
    "text": "4.5 Curved-line regression\nIf we transform some of our covariates (\\(X\\)s) and plot the resulting model on the original covariate scale, we end up with curved regression lines:\n\nCodebw_lm3 = lm(weight ~ sex:log(age) - 1, data = bw)\nlibrary(palmerpenguins)\n\nggpenguins &lt;- \n  palmerpenguins::penguins |&gt; \n  dplyr::filter(species == \"Adelie\") |&gt; \n  ggplot(\n    aes(x = bill_length_mm , y = body_mass_g)) +\n  geom_point() + \n  xlab(\"Bill length (mm)\") + \n  ylab(\"Body mass (g)\")\n\nggpenguins2 = ggpenguins +\n  stat_smooth(\n    method = \"lm\",\n              formula = y ~ log(x),\n              geom = \"smooth\") +\n  xlab(\"Bill length (mm)\") + \n  ylab(\"Body mass (g)\")\n\n\n\nCodeggpenguins2 |&gt;  ggplotly()\n\nWarning: Removed 1 rows containing non-finite values (`stat_smooth()`)."
  },
  {
    "objectID": "Linear-models-overview.html#review-of-one-sample-inference",
    "href": "Linear-models-overview.html#review-of-one-sample-inference",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n5.1 Review of one-sample inference",
    "text": "5.1 Review of one-sample inference\nPreviously, we learned how to fit outcome-only models of the form \\(p(X=x|\\theta)\\) to iid data \\(\\mathbf x = (x_1,…,x_n)\\) using maximum likelihood estimation:\n\\[\\mathcal L(\\mathbf x|\\theta) = p(X_1 = x_1, …,X_n =x_n|\\theta) = \\prod_{i=1}^n p(X=x_i|\\theta)\\]\n\\[\\ell(x|\\theta) = \\log \\mathcal L(x|\\theta)\\]\n\\[\\hat \\theta_{ML} = \\arg \\max_\\theta \\ell(x|\\theta)\\]\nWe learned how to quantify our uncertainty about these maximum likelihood estimates; with sufficient sample size, \\(\\hat \\theta_{ML}\\) has the approximate distribution:\n\\[\n\\hat\\theta_{ML} \\dot \\sim N(\\theta,\\mathcal I(\\theta)^{-1})\n\\]\nFor models in the “exponential family” of distributions, which includes the Gaussian, Poisson, Bernoulli, Binomial, Exponential, and Gamma distributions, \\(\\mathcal I(\\theta) = \\text -E[\\mathcal{l}''(X|\\theta)]\\), so we estimated \\(\\mathcal I(\\theta)\\) using either \\(\\mathcal I(\\theta)|_{\\theta = \\hat \\theta_{ML}}\\) or \\(\\mathcal{l}''(\\mathbf x |\\theta)|_{\\theta = \\hat \\theta_{ML}}\\).\nThen an asymptotic approximation of a 95% confidence interval for \\(\\theta_k\\) is\n\\[\\hat \\theta_{ML} \\pm z_{0.975} \\times \\left[\\left(\\hat{\\mathcal I}(\\hat \\theta_{ML})\\right)^{-1}\\right]_{kk}\\]\nwhere \\(z_\\beta\\) the \\(\\beta\\) quantile of the standard Gaussian distribution."
  },
  {
    "objectID": "Linear-models-overview.html#mles-for-linear-regression",
    "href": "Linear-models-overview.html#mles-for-linear-regression",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n5.2 MLEs for Linear Regression",
    "text": "5.2 MLEs for Linear Regression\nLet’s use maximum likelihood again:\n\\[\n\\mathcal L(\\mathbf y|\\mathbf x,\\beta, \\sigma^2) = \\prod_{i=1}^n (2\\pi\\sigma^2)^{-1/2} \\exp\\left\\{-\\frac{1}{2\\sigma^2}(y_i - x_i'\\beta)^2\\right\\}\n\\]\n\\[\n\\ell(\\mathbf y|\\mathbf x,\\beta, \\sigma^2) \\propto -\\frac{n}{2}\\log{\\sigma^2} - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - x_i' \\beta)^2\n\\]\n\\[\n\\ell'(\\mathbf y|\\mathbf x,\\beta, \\sigma^2) \\propto -\\frac{n}{2}\\log{\\sigma^2} - \\frac{1}{2\\sigma^2}\\frac{d}{d\\beta}\\left(\\sum_{i=1}^n (y_i - x_i' \\beta)^2\\right)\n\\]\nA few tools from linear algebra will make this analysis go easier (see the recommended text by Fieller, Section 7.2 for details).\n\\[\nf_{\\beta}(\\mathbf x) = (f_{\\beta}(x_1), f_{\\beta}(x_2), ..., f_{\\beta}(x_n))^\\top\n\\]\nLet \\(\\mathbf x\\) and \\(\\beta\\) be vectors of length \\(p\\), or in other words, matrices of length \\(p\\times 1\\):\n\\[\nx = \\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\n\\vdots \\\\\nx_{p}\n\\end{bmatrix} \\\\\n\\beta = \\begin{bmatrix}\n\\beta_{1} \\\\\n\\beta_{2} \\\\\n\\vdots \\\\\n\\beta_{p}\n\\end{bmatrix}\n\\]\nThen\n\\[\nx' \\equiv x^\\top \\equiv [x_1, x_2, ..., x_p]\n\\]\nand\n\\[\nx'\\beta = [x_1, x_2, ..., x_p]\n\\begin{bmatrix}\n\\beta_{1} \\\\\n\\beta_{2} \\\\\n\\vdots \\\\\n\\beta_{p}\n\\end{bmatrix} =\nx_1\\beta_1+x_2\\beta_2 +...+x_p \\beta_p\n\\]\nIf \\(f(\\beta)\\) is a function that takes \\(\\beta\\) as input and outputs a scalar, such as \\(f(\\beta) = x'\\beta\\), then:\n\\[\n\\frac{d}{d \\beta} f(\\beta)=\n\\begin{bmatrix}\n\\frac{d}{d\\beta_1}f(\\beta) \\\\\n\\frac{d}{d\\beta_2}f(\\beta) \\\\\n\\vdots \\\\\n\\frac{d}{d\\beta_p}f(\\beta)\n\\end{bmatrix}\n\\]\nIn particular, if \\(f(\\beta) = x'\\beta\\), then:\n\\[\n\\frac{d}{d \\beta} x'\\beta=\n\\begin{bmatrix}\n\\frac{d}{d\\beta_1}(x_1\\beta_1+x_2\\beta_2 +...+x_p \\beta_p ) \\\\\n\\frac{d}{d\\beta_2}(x_1\\beta_1+x_2\\beta_2 +...+x_p \\beta_p ) \\\\\n\\vdots \\\\\n\\frac{d}{d\\beta_p}(x_1\\beta_1+x_2\\beta_2 +...+x_p \\beta_p )\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\n\\vdots \\\\\nx_{p}\n\\end{bmatrix}\n= \\mathbf x\n\\]\nIn general:\n\\[\n\\frac{d}{d\\beta} x'\\beta = x\n\\]\nThis looks a lot like non-vector calculus, except that you have to transpose the coefficient.\nSimilarly,\n\\[\n\\frac{d}{d\\beta} \\beta'\\beta = 2\\beta\n\\]\nThis is like taking the derivative of \\(x^2\\).\nAnd finally, if \\(S\\) is a \\(p\\times p\\) matrix, then:\n\\[\n\\frac{d}{d\\beta} \\beta'S\\beta = 2S\\beta\n\\]\nAgain, this is like taking the derivative of \\(cx^2\\) with respect to \\(c\\) in non-vector calculus.\nThus:\n\\[\n\\sum_{i=1}^n (y_i - f_\\beta(x_i))^2 = (\\mathbf y - X\\beta)'(\\mathbf y - X\\beta)\n\\]\n\\[\n(\\mathbf y - X\\beta)' = (\\mathbf y' - (X\\beta)') = (\\mathbf y' - \\beta'X')\n\\]\nSo\n\\[\n\\begin{aligned}\n(\\mathbf y - X\\beta)'(\\mathbf y - X\\beta) &= (\\mathbf y' - \\beta'X')(\\mathbf y - X\\beta)\\\\\n&= y'y - \\beta'X'y - y'X\\beta +\\beta'X'X\\beta\\\\\n&= y'y - 2y'X\\beta +\\beta'X'X\\beta\n\\end{aligned}\n\\]\nSo\n\\[\n\\begin{aligned}\n\\frac{d}{d\\beta}\\left(\\sum_{i=1}^n (y_i - x_i' \\beta)^2\\right) &=\n\\frac{d}{d\\beta}(\\mathbf y - X\\beta)'(\\mathbf y - X\\beta)\\\\\n&= \\frac{d}{d\\beta} (y'y - 2y'X\\beta +\\beta'X'X\\beta)\\\\\n&= (- 2X'y +2X'X\\beta)\n\\end{aligned}\n\\]\nSo if \\(\\ell(\\beta,\\sigma^2) =0\\), then\n\\[\n\\begin{aligned}\n0 &= (- 2X'y +2X'X\\beta)\\\\\n2X'y &= 2X'X\\beta\\\\\nX'y &= X'X\\beta\\\\\n(X'X)^{-1}X'y &= \\beta\n\\end{aligned}\n\\]\nThe second derivative matrix \\(\\ell_{\\beta, \\beta'} ''(\\beta, \\sigma^2;\\mathbf X,\\mathbf y)\\) is negative definite at \\(\\beta = (X'X)^{-1}X'y\\), so \\(\\hat \\beta_{ML} = (X'X)^{-1}X'y\\) is the MLE for \\(\\beta\\).\nSimilarly (not shown):\n\\[\n\\hat\\sigma^2_{ML} = \\frac{1}{n} (Y-X\\hat\\beta)'(Y-X\\hat\\beta)\n\\]\nAnd\n\\[\n\\begin{aligned}\n\\mathcal I_{\\beta} &= E[-\\ell_{\\beta, \\beta'} ''(Y|X,\\beta, \\sigma^2)]\\\\\n&= \\frac{1}{\\sigma^2}X'X\n\\end{aligned}\n\\]\nSo:\n\\[\nVar(\\hat \\beta) \\approx (\\mathcal I_{\\beta})^{-1} = \\sigma^2 (X'X)^{-1}\n\\]\nand\n\\[\n\\hat\\beta \\dot \\sim N(\\beta, \\mathcal I_{\\beta}^{-1})\n\\] These are all results you have hopefully seen before, and in the Gaussian linear regression case they are exact, not just approximate.\nIn our model 2 above, this matrix is:\n\nCodebw_lm2 |&gt; vcov()\n\n            (Intercept)  sexmale      age sexmale:age\n(Intercept)     1353968 -1353968 -34871.0     34871.0\nsexmale        -1353968  2596387  34871.0    -67211.0\nage              -34871    34871    899.9      -899.9\nsexmale:age       34871   -67211   -899.9      1743.5\n\n\nNote that if we take the square roots of the diagonals, we get the standard errors listed in the model output:\n\nCodebw_lm2 |&gt; vcov() |&gt; diag() |&gt; sqrt()\n\n(Intercept)     sexmale         age sexmale:age \n    1163.60     1611.33       30.00       41.76 \n\n\n\nCodebw_lm2 |&gt; summary() |&gt; coef() |&gt; pander()\n\n\n\n\n\n\n\n\n\n\n \nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n(Intercept)\n-2142\n1164\n-1.841\n0.08057\n\n\nsexmale\n873\n1611\n0.5418\n0.594\n\n\nage\n130.4\n30\n4.347\n0.0003127\n\n\nsexmale:age\n-18.42\n41.76\n-0.4411\n0.6639\n\n\n\n\n\nSo we can do confidence intervals, hypothesis tests, and p-values exactly as in the one-variable case we looked at previously."
  },
  {
    "objectID": "Linear-models-overview.html#wald-tests-and-cis",
    "href": "Linear-models-overview.html#wald-tests-and-cis",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n6.1 Wald tests and CIs",
    "text": "6.1 Wald tests and CIs\nR can give you Wald tests for single coefficients and corresponding CIs:\n\nCodebw_lm2 |&gt; \n  parameters(ci_method = \"wald\") |&gt; \n  print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\n\n(Intercept)\n-2141.67\n1163.60\n(-4568.90, 285.56)\n-1.84\n0.081\n\n\nsex (male)\n872.99\n1611.33\n(-2488.18, 4234.17)\n0.54\n0.594\n\n\nage\n130.40\n30.00\n(67.82, 192.98)\n4.35\n&lt; .001\n\n\nsex (male) × age\n-18.42\n41.76\n(-105.52, 68.68)\n-0.44\n0.664"
  },
  {
    "objectID": "Linear-models-overview.html#one-parameter-inference-by-hand",
    "href": "Linear-models-overview.html#one-parameter-inference-by-hand",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n6.2 One-parameter inference by hand",
    "text": "6.2 One-parameter inference by hand\nTo understand what’s happening, let’s replicate these results by hand for the interaction term.\n\n6.2.1 P-value\n\nCodebeta_hat = coef(summary(bw_lm2))[\"sexmale:age\", \"Estimate\"]\nse_hat = coef(summary(bw_lm2))[\"sexmale:age\", \"Std. Error\"]\ndfresid = bw_lm2$df.residual\nt_stat = abs(beta_hat)/se_hat\npval_t = pt(abs(t_stat), df = dfresid, lower = FALSE) * 2\n\n\n\\[\n\\begin{aligned}\n&P\\left(\n| \\hat \\beta_3  | &gt;\n| -18.4172| \\middle| H_0\n\\right)\n&= P\\left(\n\\left| \\frac{\\hat\\beta_3}{\\hat{SE}(\\hat\\beta_3)} \\right| &gt;\n\\left| \\frac{-18.4172}{41.7558} \\right| \\middle| H_0\n\\right)\\\\\n&= P\\left(\n| T_{20} | &gt;  0.4411 \\middle| H_0\n\\right)\\\\\n&= 0.6639\n\\end{aligned}\n\\] This matches the result in the table above.\n\n6.2.2 Confidence interval\n\nCodeconfint_radius_t = se_hat * qt(p = 0.975, df = dfresid, lower = TRUE)\nconfint_t = beta_hat + c(-1,1) * confint_radius_t\nprint(confint_t)\n\n[1] -105.52   68.68\n\n\nThis also matches."
  },
  {
    "objectID": "Linear-models-overview.html#gaussian-approximations",
    "href": "Linear-models-overview.html#gaussian-approximations",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n6.3 Gaussian approximations",
    "text": "6.3 Gaussian approximations\nHere are the asymptotic (Gaussian approximation) equivalents:\n\n6.3.1 P-value\n\nCodepval_z = pnorm(abs(t_stat), lower = FALSE) * 2\n\nprint(pval_z)\n\n[1] 0.6592\n\n\n\n6.3.2 Confidence interval\n\nCodeconfint_radius_z = se_hat * qnorm(0.975, lower = TRUE)\nconfint_z = \n  beta_hat + c(-1,1) * confint_radius_z\nprint(confint_z)\n\n[1] -100.26   63.42"
  },
  {
    "objectID": "Linear-models-overview.html#likelihood-ratio-statistics",
    "href": "Linear-models-overview.html#likelihood-ratio-statistics",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n6.4 Likelihood ratio statistics",
    "text": "6.4 Likelihood ratio statistics\n\nCodelogLik(bw_lm2)\n\n'log Lik.' -156.6 (df=5)\n\nCodelogLik(bw_lm1)\n\n'log Lik.' -156.7 (df=4)\n\nCodelLR = (logLik(bw_lm2) - logLik(bw_lm1)) |&gt; as.numeric()\ndelta_df = (bw_lm1$df.residual - df.residual(bw_lm2))\n\nd_lLR = function(x, df = delta_df) dchisq(x, df = df)\n\nx_max = 1\n\nchisq_plot = \n  ggplot() + \n  geom_function(fun = d_lLR) +\n  stat_function( fun = d_lLR, xlim = c(lLR, x_max), geom = \"area\", fill = \"gray\") +\n  geom_segment(aes(x = lLR, xend = lLR, y = 0, yend = d_lLR(lLR)), col = \"red\") + \n  xlim(0.0001,x_max) + \n  ylim(0,4) + \n  ylab(\"p(X=x)\") + \n  xlab(\"log(likelihood ratio) statistic [x]\") +\n  theme_classic()\n\n\n\nCodepchisq(\n  q = 2*lLR, \n  df = delta_df, \n  lower = FALSE)\n\n[1] 0.6298\n\n\n\nCodechisq_plot |&gt;  ggplotly()\n\n\n\n\n\nNow we can get the p-value:\n\nCodepchisq(2*lLR, df = delta_df, lower = FALSE)\n\n[1] 0.6298"
  },
  {
    "objectID": "Linear-models-overview.html#section",
    "href": "Linear-models-overview.html#section",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n6.5 ",
    "text": "6.5 \nIn practice you don’t have to do this by hand; there are functions to do it for you:\n\nCode# built in\nlibrary(lmtest)\nlrtest(bw_lm2, bw_lm1)\n\nLikelihood ratio test\n\nModel 1: weight ~ sex + age + sex:age\nModel 2: weight ~ sex + age\n  #Df LogLik Df Chisq Pr(&gt;Chisq)\n1   5   -157                    \n2   4   -157 -1  0.23       0.63"
  },
  {
    "objectID": "Linear-models-overview.html#aic-and-bic",
    "href": "Linear-models-overview.html#aic-and-bic",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n7.1 AIC and BIC",
    "text": "7.1 AIC and BIC\nWhen we use likelihood ratio tests, we are comparing how well different models fit the data.\nLikelihood ratio tests require “nested” models: one must be a special case of the other.\nIf we have non-nested models, we can instead use the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC):\n\nAIC = \\(-2 * \\ell(\\hat\\theta) + 2 * p\\)\nBIC = \\(-2 * \\ell(\\hat\\theta) + p * \\text{log}(n)\\)\n\nwhere \\(\\ell\\) is the log-likelihood of the data evaluated using the parameter estimates \\(\\hat\\theta\\), \\(p\\) is the number of estimated parameters in the model (including \\(\\hat\\sigma^2\\)), and \\(n\\) is the number of observations.\nYou can calculate these criteria using the logLik() function, or use the built-in R functions:\n\n7.1.1 AIC in R\n\nCode-2 * logLik(bw_lm2) |&gt; as.numeric() + \n  2*(length(coef(bw_lm2))+1) # sigma counts as a parameter here\n\n[1] 323.2\n\nCodeAIC(bw_lm2)\n\n[1] 323.2\n\n\n\n7.1.2 BIC in R\n\nCode-2 * logLik(bw_lm2) |&gt; as.numeric() + \n  (length(coef(bw_lm2))+1) * log(nobs(bw_lm2))\n\n[1] 329\n\nCodeBIC(bw_lm2)\n\n[1] 329\n\n\nLarge values of AIC and BIC are worse than small values. There are no hypothesis tests or p-values associated with these criteria."
  },
  {
    "objectID": "Linear-models-overview.html#residual-deviance",
    "href": "Linear-models-overview.html#residual-deviance",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n7.2 (Residual) Deviance",
    "text": "7.2 (Residual) Deviance\nLet \\(q\\) be the number of distinct covariate combinations in a data set.\n\nCodebw.X.unique = \n  bw |&gt; \n  count(sex, age)\n\nn_unique.bw  = nrow(bw.X.unique)\n\n\nFor example, in the birthweight data, there are \\(q = 12\\) unique patterns:\n\nCodebw.X.unique |&gt; \n  pander(\n    row.names = rownames(bw.X.unique))\n\n\n\n\n\n\n\n\n\n \nsex\nage\nn\n\n\n\n1\nfemale\n36\n2\n\n\n2\nfemale\n37\n1\n\n\n3\nfemale\n38\n2\n\n\n4\nfemale\n39\n2\n\n\n5\nfemale\n40\n4\n\n\n6\nfemale\n42\n1\n\n\n7\nmale\n35\n1\n\n\n8\nmale\n36\n1\n\n\n9\nmale\n37\n2\n\n\n10\nmale\n38\n3\n\n\n11\nmale\n40\n4\n\n\n12\nmale\n41\n1\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf a given covariate pattern has more than one observation in a dataset, those observations are called replicates.\n\n\nThen the most complicated model we could fit would have one parameter (a mean) for each of these combinations, plus a variance parameter:\n\nCodelm_max = \n  bw |&gt; \n  mutate(age = factor(age)) |&gt; \n  lm(\n    formula = weight ~ sex:age - 1, \n    data = _)\n\nlm_max |&gt; \n  parameters() |&gt; \n  print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(12)\np\n\n\n\nsex (male) × age35\n2925.00\n187.92\n(2515.55, 3334.45)\n15.56\n&lt; .001\n\n\nsex (female) × age36\n2570.50\n132.88\n(2280.98, 2860.02)\n19.34\n&lt; .001\n\n\nsex (male) × age36\n2625.00\n187.92\n(2215.55, 3034.45)\n13.97\n&lt; .001\n\n\nsex (female) × age37\n2539.00\n187.92\n(2129.55, 2948.45)\n13.51\n&lt; .001\n\n\nsex (male) × age37\n2737.50\n132.88\n(2447.98, 3027.02)\n20.60\n&lt; .001\n\n\nsex (female) × age38\n2872.50\n132.88\n(2582.98, 3162.02)\n21.62\n&lt; .001\n\n\nsex (male) × age38\n2982.00\n108.50\n(2745.60, 3218.40)\n27.48\n&lt; .001\n\n\nsex (female) × age39\n2846.00\n132.88\n(2556.48, 3135.52)\n21.42\n&lt; .001\n\n\nsex (female) × age40\n3152.25\n93.96\n(2947.52, 3356.98)\n33.55\n&lt; .001\n\n\nsex (male) × age40\n3256.25\n93.96\n(3051.52, 3460.98)\n34.66\n&lt; .001\n\n\nsex (male) × age41\n3292.00\n187.92\n(2882.55, 3701.45)\n17.52\n&lt; .001\n\n\nsex (female) × age42\n3210.00\n187.92\n(2800.55, 3619.45)\n17.08\n&lt; .001\n\n\n\n\n\nWe call this model the full, maximal, or saturated model for this dataset.\nWe can calculate the log-likelihood of this model as usual:\n\nCodelogLik(lm_max)\n\n'log Lik.' -151.4 (df=13)\n\n\nWe can compare this model to our other models using chi-square tests, as usual:\n\nCodelrtest(lm_max, bw_lm2)\n\nLikelihood ratio test\n\nModel 1: weight ~ sex:age - 1\nModel 2: weight ~ sex + age + sex:age\n  #Df LogLik Df Chisq Pr(&gt;Chisq)\n1  13   -151                    \n2   5   -157 -8  10.4       0.24\n\n\nThe likelihood ratio statistic for this test is \\[\\lambda = 2 * (\\ell_{\\text{full}} - \\ell) = 10.3554\\] where:\n\n\n\\(\\ell_{\\text{max}}\\) is the log-likelihood of the full model: -151.4016\n\n\\(\\ell\\) is the log-likelihood of our comparison model (two slopes, two intercepts): -156.5793\n\nThis statistic is called the deviance or residual deviance for our two-slopes and two-intercepts model; it tells us how much the likelihood of that model deviates from the likelihood of the maximal model.\nThe corresponding p-value tells us whether there we have enough evidence to detect that our two-slopes, two-intercepts model is a worse fit for the data than the maximal model; in other words, it tells us if there’s evidence that we missed any important patterns. (Remember, a nonsignificant p-value could mean that we didn’t miss anything and a more complicated model is unnecessary, or it could mean we just don’t have enough data to tell the difference between these models.)"
  },
  {
    "objectID": "Linear-models-overview.html#null-deviance",
    "href": "Linear-models-overview.html#null-deviance",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n7.3 Null Deviance",
    "text": "7.3 Null Deviance\nSimilarly, the least complicated model we could fit would have only one mean parameter, an intercept:\n\\[\\text E[Y|X=x] = \\beta_0\\] We can fit this model in R like so:\n\nCodelm0 = lm(weight ~ 1, data = bw)\n\nlm0 |&gt; parameters() |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(23)\np\n\n\n(Intercept)\n2967.67\n57.58\n(2848.56, 3086.77)\n51.54\n&lt; .001\n\n\n\n\nThis model also has a likelihood:\n\nCodelogLik(lm0)\n\n'log Lik.' -169 (df=2)\n\n\nAnd we can compare it to more complicated models using a likelihood ratio test:\n\nCodelrtest(bw_lm2, lm0)\n\nLikelihood ratio test\n\nModel 1: weight ~ sex + age + sex:age\nModel 2: weight ~ 1\n  #Df LogLik Df Chisq Pr(&gt;Chisq)    \n1   5   -157                        \n2   2   -169 -3  24.8    1.7e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe likelihood ratio statistic for the test comparing the null model to the maximal model is \\[\\lambda = 2 * (\\ell_{\\text{full}} - \\ell_{0}) = 35.1067\\] where:\n\n\n\\(\\ell_{\\text{0}}\\) is the log-likelihood of the null model: -168.955\n\n\\(\\ell_{\\text{full}}\\) is the log-likelihood of the maximal model: -151.4016\n\nIn R, this test is:\n\nCodelrtest(lm_max, lm0)\n\nLikelihood ratio test\n\nModel 1: weight ~ sex:age - 1\nModel 2: weight ~ 1\n  #Df LogLik  Df Chisq Pr(&gt;Chisq)    \n1  13   -151                         \n2   2   -169 -11  35.1    0.00024 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis log-likelihood ratio statistic is called the null deviance. It tells us whether we have enough data to detect a difference between the null and full models.\n\n\n\n\n\n\nCaution\n\n\n\nThis definition is a bit different from what I said in class (4/25); then, I said the null deviance was twice the difference in log-likelihood between the model of interest and the null model. The one I have written above is the standard one, and we’ll stick to the standard definition going forward."
  },
  {
    "objectID": "Linear-models-overview.html#rescale-age",
    "href": "Linear-models-overview.html#rescale-age",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n8.1 Rescale age",
    "text": "8.1 Rescale age\n\nCodebw = \n  bw |&gt;\n  mutate(\n    `age - mean` = age - mean(age),\n    `age - 36wks` = age - 36\n  )\n\nlm1c = lm(weight ~ sex + `age - 36wks`, data = bw)\n\nlm2c = lm(weight ~ sex + `age - 36wks` + sex:`age - 36wks`, data = bw)\n\nparameters(lm2c, ci_method = \"wald\") |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\n\n(Intercept)\n2552.73\n97.59\n(2349.16, 2756.30)\n26.16\n&lt; .001\n\n\nsex (male)\n209.97\n129.75\n(-60.68, 480.63)\n1.62\n0.121\n\n\nage - 36wks\n130.40\n30.00\n(67.82, 192.98)\n4.35\n&lt; .001\n\n\nsex (male) × age - 36wks\n-18.42\n41.76\n(-105.52, 68.68)\n-0.44\n0.664\n\n\n\n\n\nCompare with what we got without rescaling:\n\nCodeparameters(bw_lm2, ci_method = \"wald\") |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\n\n(Intercept)\n-2141.67\n1163.60\n(-4568.90, 285.56)\n-1.84\n0.081\n\n\nsex (male)\n872.99\n1611.33\n(-2488.18, 4234.17)\n0.54\n0.594\n\n\nage\n130.40\n30.00\n(67.82, 192.98)\n4.35\n&lt; .001\n\n\nsex (male) × age\n-18.42\n41.76\n(-105.52, 68.68)\n-0.44\n0.664"
  },
  {
    "objectID": "Linear-models-overview.html#prediction-for-linear-models",
    "href": "Linear-models-overview.html#prediction-for-linear-models",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n9.1 Prediction for linear models",
    "text": "9.1 Prediction for linear models\n\\[\n\\begin{aligned}\n\\hat Y &= \\hat E[Y|X=x] \\\\\n&= x'\\hat\\beta \\\\\n&= \\hat\\beta_0\\cdot 1 + \\hat\\beta_1 x_1 + ... + \\hat\\beta_p x_p\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "Linear-models-overview.html#prediction-in-r",
    "href": "Linear-models-overview.html#prediction-in-r",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n9.2 prediction in R",
    "text": "9.2 prediction in R\n\nCodeX = model.matrix(bw_lm1)\nX |&gt; as_tibble() |&gt; print()\n\n# A tibble: 24 × 3\n   `(Intercept)` sexmale   age\n           &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1             1       1    40\n 2             1       0    40\n 3             1       1    38\n 4             1       0    36\n 5             1       1    40\n 6             1       0    40\n 7             1       1    35\n 8             1       0    38\n 9             1       1    36\n10             1       0    42\n# ℹ 14 more rows\n\nCodeprint(X[1,])\n\n(Intercept)     sexmale         age \n          1           1          40 \n\nCodeprint(coef(bw_lm1))\n\n(Intercept)     sexmale         age \n    -1773.3       163.0       120.9 \n\nCodeprint(X[1,] * coef(bw_lm1))\n\n(Intercept)     sexmale         age \n      -1773         163        4836 \n\nCode{X[1,] * coef(bw_lm1)} |&gt; sum() |&gt; print()\n\n[1] 3225\n\nCodeX %*% coef(bw_lm1) |&gt; as.vector()\n\n [1] 3225 3062 2984 2579 3225 3062 2621 2821 2742 3304 2863 2942 3346 3062 3225\n[16] 2700 2863 2579 2984 2821 3225 2942 2984 3062\n\nCodepredict(bw_lm1)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n3225 3062 2984 2579 3225 3062 2621 2821 2742 3304 2863 2942 3346 3062 3225 2700 \n  17   18   19   20   21   22   23   24 \n2863 2579 2984 2821 3225 2942 2984 3062 \n\nCodepredict(bw_lm1, se.fit = TRUE)\n\n$fit\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n3225 3062 2984 2579 3225 3062 2621 2821 2742 3304 2863 2942 3346 3062 3225 2700 \n  17   18   19   20   21   22   23   24 \n2863 2579 2984 2821 3225 2942 2984 3062 \n\n$se.fit\n [1] 61.46 57.17 51.58 76.03 61.46 57.17 85.25 53.38 69.96 83.89 57.95 51.38\n[13] 74.78 57.17 61.46 62.42 57.95 76.03 51.58 53.38 61.46 51.38 51.58 57.17\n\n$df\n[1] 21\n\n$residual.scale\n[1] 177.1\n\nCodepredict(bw_lm1, se.fit = TRUE)$fit\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n3225 3062 2984 2579 3225 3062 2621 2821 2742 3304 2863 2942 3346 3062 3225 2700 \n  17   18   19   20   21   22   23   24 \n2863 2579 2984 2821 3225 2942 2984 3062 \n\nCodepredict(bw_lm1, se.fit = TRUE)$se.fit\n\n [1] 61.46 57.17 51.58 76.03 61.46 57.17 85.25 53.38 69.96 83.89 57.95 51.38\n[13] 74.78 57.17 61.46 62.42 57.95 76.03 51.58 53.38 61.46 51.38 51.58 57.17\n\nCodepredict(bw_lm1, se.fit = TRUE, interval = \"confidence\")$fit |&gt; as_tibble()\n\n# A tibble: 24 × 3\n     fit   lwr   upr\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 3225. 3098. 3353.\n 2 3062. 2944. 3181.\n 3 2984. 2876. 3091.\n 4 2579. 2421. 2737.\n 5 3225. 3098. 3353.\n 6 3062. 2944. 3181.\n 7 2621. 2444. 2798.\n 8 2821. 2710. 2932.\n 9 2742. 2596. 2887.\n10 3304. 3130. 3479.\n# ℹ 14 more rows\n\nCodepredict(bw_lm1, se.fit = TRUE, interval = \"predict\")$fit |&gt; as_tibble()\n\nWarning in predict.lm(bw_lm1, se.fit = TRUE, interval = \"predict\"): predictions on current data refer to _future_ responses\n\n\n# A tibble: 24 × 3\n     fit   lwr   upr\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 3225. 2836. 3615.\n 2 3062. 2675. 3449.\n 3 2984. 2600. 3367.\n 4 2579. 2178. 2980.\n 5 3225. 2836. 3615.\n 6 3062. 2675. 3449.\n 7 2621. 2212. 3030.\n 8 2821. 2436. 3205.\n 9 2742. 2346. 3138.\n10 3304. 2897. 3712.\n# ℹ 14 more rows\n\n\nThe warning from the last command is: “predictions on current data refer to future responses” (since you already know what happened to the current data, and thus don’t need to predict it). You could also supply newdata to get predictions for new combinations of predictors that you didn’t see in your original dataset. See ?predict.lm for more."
  },
  {
    "objectID": "Linear-models-overview.html#residuals",
    "href": "Linear-models-overview.html#residuals",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n10.1 Residuals",
    "text": "10.1 Residuals\n\n10.1.1 Definitions of residuals\n\nResiduals: \\[e_i = y_i - \\hat E[Y|X=x]\\]\nFor Gaussian models, \\(e_i\\) can be seen as an estimate of \\[\\epsilon_i = Y_i - \\text{E}[Y|X=x_i]\\]\nStandardized residuals: \\[r_i = \\frac{e_i}{\\hat{SD}(e_i)}\\]\nFor Gaussian models: \\[\\hat{SD}(e_i) \\approx \\frac{e_i}{\\hat{\\sigma}}\\]\n\n10.1.2 Characteristics of residuals\nWith enough data and a correct model, the residuals will be approximately Guassian distributed, with variance \\(\\sigma^2\\), which we can estimate using \\(\\hat\\sigma^2\\): that is:\n\\[\ne_i\\ \\dot \\sim_{iid}\\ N(0, \\hat\\sigma^2)\n\\]\nHence, with enough data and a correct model, the standardized residuals will be approximately standard Gaussian; that is,\n\\[\nr_i\\ \\dot \\sim_{iid}\\ N(0,1)\n\\]"
  },
  {
    "objectID": "Linear-models-overview.html#marginal-distributions-of-residuals",
    "href": "Linear-models-overview.html#marginal-distributions-of-residuals",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n10.2 Marginal distributions of residuals",
    "text": "10.2 Marginal distributions of residuals\nTo look for problems with our model, we can check whether the residuals \\(e_i\\) and standardized residuals \\(r_i\\) look like they have the distributions that they are supposed to have, according to the model.\nFirst, we need to compute the residuals. R makes this easy:\n\n10.2.1 (non-standardized) residuals in R\n\nCoderesid(bw_lm2)\n\n      1       2       3       4       5       6       7       8       9      10 \n 176.27 -140.73 -144.13  -59.53  177.47 -126.93  -68.93  242.67 -139.33   51.67 \n     11      12      13      14      15      16      17      18      19      20 \n 156.67 -125.13  274.28 -137.71  -27.69 -246.69 -191.67  189.33  -11.67 -242.64 \n     21      22      23      24 \n -47.64  262.36  210.36  -30.62 \n\nCode# check by hand\nbw$weight - fitted(bw_lm2)\n\n      1       2       3       4       5       6       7       8       9      10 \n 176.27 -140.73 -144.13  -59.53  177.47 -126.93  -68.93  242.67 -139.33   51.67 \n     11      12      13      14      15      16      17      18      19      20 \n 156.67 -125.13  274.28 -137.71  -27.69 -246.69 -191.67  189.33  -11.67 -242.64 \n     21      22      23      24 \n -47.64  262.36  210.36  -30.62 \n\n\nSuccess!\n\n10.2.2 Standardized residuals in R\n\nCoderstandard(bw_lm2)\n\n       1        2        3        4        5        6        7        8 \n 1.15982 -0.92601 -0.87479 -0.34723  1.03507 -0.73473 -0.39901  1.43752 \n       9       10       11       12       13       14       15       16 \n-0.82539  0.30606  0.92807 -0.87616  1.91428 -0.86559 -0.16430 -1.46376 \n      17       18       19       20       21       22       23       24 \n-1.11016  1.09658 -0.06761 -1.46159 -0.28696  1.58040  1.26717 -0.19805 \n\nCoderesid(bw_lm2)/sigma(bw_lm2)\n\n       1        2        3        4        5        6        7        8 \n 0.97593 -0.77920 -0.79802 -0.32962  0.98258 -0.70279 -0.38166  1.34357 \n       9       10       11       12       13       14       15       16 \n-0.77144  0.28606  0.86741 -0.69282  1.51858 -0.76244 -0.15331 -1.36584 \n      17       18       19       20       21       22       23       24 \n-1.06123  1.04825 -0.06463 -1.34341 -0.26376  1.45262  1.16471 -0.16954 \n\n\nThese are not quite the same, because R is doing something more complicated and precise to get the standard errors. Let’s not worry about those details for now; the difference is pretty small in this case:\n\nCoderstandard_compare_plot = \n  tibble(\n    x = resid(bw_lm2)/sigma(bw_lm2), \n    y = rstandard(bw_lm2)) |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_point() + \n  theme_bw() +\n  coord_equal() + \n  xlab(\"resid(bw_lm2)/sigma(bw_lm2)\") +\n  ylab(\"rstandard(bw_lm2)\") +\n  geom_abline(\n    aes(\n    intercept = 0,\n    slope = 1, \n    col = \"x=y\")) +\n  labs(colour=\"\") +\n  scale_colour_manual(values=\"red\")\n\n\n\nCodeggplotly(rstandard_compare_plot)\n\n\n\n\n\nLet’s add these residuals to the tibble of our dataset:\n\nCodebw = \n  bw |&gt; \n  mutate(\n    fitted_lm2 = fitted(bw_lm2),\n    \n    resid_lm2 = resid(bw_lm2),\n    # resid_lm2 = weight - fitted_lm2,\n    \n    std_resid_lm2 = rstandard(bw_lm2),\n    # std_resid_lm2 = resid_lm2 / sigma(bw_lm2)\n  )\n\nbw |&gt; \n  select(\n    sex,\n    age,\n    weight,\n    fitted_lm2,\n    resid_lm2,\n    std_resid_lm2\n  )\n\n# A tibble: 24 × 6\n   sex      age weight fitted_lm2 resid_lm2 std_resid_lm2\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 female    36   2729      2553.     176.          1.16 \n 2 female    36   2412      2553.    -141.         -0.926\n 3 female    37   2539      2683.    -144.         -0.875\n 4 female    38   2754      2814.     -59.5        -0.347\n 5 female    38   2991      2814.     177.          1.04 \n 6 female    39   2817      2944.    -127.         -0.735\n 7 female    39   2875      2944.     -68.9        -0.399\n 8 female    40   3317      3074.     243.          1.44 \n 9 female    40   2935      3074.    -139.         -0.825\n10 female    40   3126      3074.      51.7         0.306\n# ℹ 14 more rows\n\n\nNow let’s build histograms:\n\n10.2.3 Marginal distribution of (nonstandardized) residuals\n\nCoderesid_marginal_hist = \n  bw |&gt; \n  ggplot(aes(x = resid_lm2)) +\n  geom_histogram()\n\n\n\nCodeggplotly(resid_marginal_hist)\n\n\n\n\n\nHard to tell with this small amount of data, but I’m a bit concerned that the histogram doesn’t show a bell-curve shape.\n\n10.2.3.1 Marginal distribution of standardized residuals\n\nCodestd_resid_marginal_hist = \n  bw |&gt; \n  ggplot(aes(x = std_resid_lm2)) +\n  geom_histogram()\n\n\n\nCodeggplotly(std_resid_marginal_hist)\n\n\n\n\n\nThis looks similar, although the scale of the x-axis got narrower, because we divided by \\(\\hat\\sigma\\) (roughly speaking).\nStill hard to tell if the distribution is Gaussian."
  },
  {
    "objectID": "Linear-models-overview.html#qq-plot-of-standardized-residuals",
    "href": "Linear-models-overview.html#qq-plot-of-standardized-residuals",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n10.3 QQ plot of standardized residuals",
    "text": "10.3 QQ plot of standardized residuals\nAnother way to assess normality is the QQ plot of the standardized residuals versus normal quantiles:\n\nCodelibrary(ggfortify) \n# needed to make ggplot2::autoplot() work for `lm` objects\n\nqqplot_lm2_auto = \n  bw_lm2 |&gt; \n  autoplot(\n    which = 2, # options are 1:6; can do multiple at once\n    ncol = 1) +\n  theme_classic()\n\n\n\nCodeprint(qqplot_lm2_auto)\n\n\n\n\nIf the Gaussian model were correct, these points should follow the dotted line.\n\n\n\n\n\n\nNote\n\n\n\nFig 2.4 panel (c) in Dobson is a little different; they didn’t specify how they produced it, but other statistical analysis systems do things differently from R.\n\n\n\n10.3.1 QQ plot - how it’s built\nLet’s construct it by hand:\n\nCodebw = bw |&gt; \n  mutate(\n    p = (rank(std_resid_lm2) - 1/2)/n(), # \"Blom's method\"\n    expected_quantiles_lm2 = qnorm(p)\n  )\n\nqqplot_lm2 = \n  bw |&gt; \n  ggplot(\n    aes(\n      x = expected_quantiles_lm2, \n      y = std_resid_lm2, \n      col = sex, \n      shape = sex)\n  ) + \n  geom_point() +\n  theme_classic() +\n  theme(legend.position='none') + # removing the plot legend\n  ggtitle(\"Normal Q-Q\") +\n  xlab(\"Theoretical Quantiles\") + \n  ylab(\"Standardized residuals\")\n\n\nWe find the expected line like so:\n\nCodeps &lt;- c(.25, .75)                  # reference probabilities\na &lt;- quantile(rstandard(bw_lm2), ps)  # empirical quantiles\nb &lt;- qnorm(ps)                     # theoretical quantiles\n\nqq_slope = diff(a)/diff(b)\nqq_intcpt = a[1] - b[1] * qq_slope\n\nqqplot_lm2 = \n  qqplot_lm2 +\n  geom_abline(slope = qq_slope, intercept = qq_intcpt)\n\n\n\nCodeggplotly(qqplot_lm2)"
  },
  {
    "objectID": "Linear-models-overview.html#conditional-distributions-of-residuals",
    "href": "Linear-models-overview.html#conditional-distributions-of-residuals",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n10.4 Conditional distributions of residuals",
    "text": "10.4 Conditional distributions of residuals\nIf our Gaussian linear regression model is correct, the residuals \\(e_i\\) and standardized residuals \\(r_i\\) should have:\n\nan approximately Gaussian distribution, with:\na mean of 0\na constant variance\n\nThis should be true regardless of the value of \\(X\\).\nBut if we didn’t correctly guess the functional form of linear component of the mean, \\[\\text{E}[Y|X=x] = \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p\\]\nThen the the residuals might have nonzero mean or nonconstant variance for some values of \\(x\\).\n\n10.4.1 Residuals versus fitted values\nTo look for these issues, we can plot the residuals \\(e_i\\) against the fitted values \\(\\hat y_i\\):\n\nCodeautoplot(bw_lm2, which = 1, ncol = 1) |&gt; print()\n\n\n\n\nIf the model is correct, the blue line should stay flat and close to 0, and the cloud of dots should have the same vertical spread regardless of the fitted value.\nIf not, we probably need to change the functional form of linear component of the mean, \\[\\text{E}[Y|X=x] = \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p\\]\n\n10.4.2 Scale-location plot\nWe can also plot the square roots of the absolute values of the standardized residuals against the fitted values:\n\nCodeautoplot(bw_lm2, which = 3, ncol = 1) |&gt; print()\n\n\n\n\nHere, the blue line doesn’t need to be near 0, but it should be flat. If not, the residual variance \\(\\sigma^2\\) might not be constant, and we might need to transform our outcome \\(Y\\) (or use a model that allows non-constant variance).\n\n10.4.3 Residuals versus leverage\nWe can also plot our standardized residuals against “leverage”, which roughly speaking is a measure of how unusual each \\(x_i\\) value is. Very unusual \\(x_i\\) values can have extreme effects on the model fit, so we might want ot remove those observations as outliers, particularly if they have large residuals.\n\nCodeautoplot(bw_lm2, which = 5, ncol = 1) |&gt; print()\n\n\n\n\nThe blue line should be relatively flat and close to 0 here."
  },
  {
    "objectID": "Linear-models-overview.html#diagnostics-constructed-by-hand",
    "href": "Linear-models-overview.html#diagnostics-constructed-by-hand",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n10.5 Diagnostics constructed by hand",
    "text": "10.5 Diagnostics constructed by hand\n\nCodebw = \n  bw |&gt; \n  mutate(\n    predlm2 = predict(bw_lm2),\n    residlm2 = weight - predlm2,\n    std_resid = residlm2 / sigma(bw_lm2),\n    # std_resid_builtin = rstandard(bw_lm2), # uses leverage\n    sqrt_abs_std_resid = std_resid |&gt; abs() |&gt; sqrt()\n    \n  )\n\n\n\n10.5.0.1 Residuals vs fitted\n\nCoderesid_vs_fit = bw |&gt; \n  ggplot(\n    aes(x = predlm2, y = residlm2, col = sex, shape = sex)\n  ) + \n  geom_point() +\n  theme_classic() +\n  geom_hline(yintercept = 0)\n\n\n\nCodeprint(resid_vs_fit)\n\n\n\n\n\n10.5.0.2 Standardized residuals vs fitted\n\nCodebw |&gt; \n  ggplot(\n    aes(x = predlm2, y = std_resid, col = sex, shape = sex)\n  ) + \n  geom_point() +\n  theme_classic() +\n  geom_hline(yintercept = 0)\n\n\n\n\n\n10.5.0.3 Standardized residuals vs gestational age\n\nCodebw |&gt; \n  ggplot(\n    aes(x = age, y = std_resid, col = sex, shape = sex)\n  ) + \n  geom_point() +\n  theme_classic() +\n  geom_hline(yintercept = 0)\n\n\n\n\n\n10.5.0.4 sqrt(abs(rstandard())) vs fitted\nCompare with autoplot(bw_lm2, 3)\n\nCodebw |&gt; \n  ggplot(\n    aes(x = predlm2, y = sqrt_abs_std_resid, col = sex, shape = sex)\n  ) + \n  geom_point() +\n  theme_classic() +\n  geom_hline(yintercept = 0)"
  },
  {
    "objectID": "Linear-models-overview.html#mean-squared-error",
    "href": "Linear-models-overview.html#mean-squared-error",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n11.1 Mean squared error",
    "text": "11.1 Mean squared error\nWe might want to minimize the mean squared error, \\(\\text E[(y-\\hat y)^2]\\), for new observations that weren’t in our data set when we fit the model.\nUnfortunately, \\[\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat y_i)^2\\] gives a biased estimate of \\(\\text E[(y-\\hat y)^2]\\) for new data. If we want an unbiased estimate, we will have to be clever."
  },
  {
    "objectID": "Linear-models-overview.html#cross-validation",
    "href": "Linear-models-overview.html#cross-validation",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n11.2 Cross-validation",
    "text": "11.2 Cross-validation\n\nCodedata(\"carbohydrate\", package = \"dobson\")\nlibrary(cvTools)\nfull.model &lt;- lm(carbohydrate ~ ., data = carbohydrate)\ntemp = \n  cvFit(full.model, data = carbohydrate, K = 5, R = 10,\ny = carbohydrate$carbohydrate)"
  },
  {
    "objectID": "Linear-models-overview.html#section-1",
    "href": "Linear-models-overview.html#section-1",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n12.1 ",
    "text": "12.1 \nIn the birthweight example, the variable sex had only two observed values:\n\nCodeunique(bw$sex)\n\n[1] \"female\" \"male\"  \n\n\nIf there are more than two observed values, we can’t just use a single variable with 0s and 1s."
  },
  {
    "objectID": "Linear-models-overview.html#section-2",
    "href": "Linear-models-overview.html#section-2",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n12.2 ",
    "text": "12.2 \nFor example, here’s the (in)famous iris data:\n\nCodeiris |&gt; tibble()\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\nCodesummary(iris)\n\n  Sepal.Length   Sepal.Width    Petal.Length   Petal.Width        Species  \n Min.   :4.30   Min.   :2.00   Min.   :1.00   Min.   :0.1   setosa    :50  \n 1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60   1st Qu.:0.3   versicolor:50  \n Median :5.80   Median :3.00   Median :4.35   Median :1.3   virginica :50  \n Mean   :5.84   Mean   :3.06   Mean   :3.76   Mean   :1.2                  \n 3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10   3rd Qu.:1.8                  \n Max.   :7.90   Max.   :4.40   Max.   :6.90   Max.   :2.5"
  },
  {
    "objectID": "Linear-models-overview.html#section-3",
    "href": "Linear-models-overview.html#section-3",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n12.3 ",
    "text": "12.3 \nThere are three species:\n\nCodeiris$Species |&gt; unique()\n\n[1] setosa     versicolor virginica \nLevels: setosa versicolor virginica"
  },
  {
    "objectID": "Linear-models-overview.html#section-4",
    "href": "Linear-models-overview.html#section-4",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n12.4 ",
    "text": "12.4 \nIf we want to model Sepal.Length by species, we could create a variable \\(X\\) that represents “setosa” as \\(X=1\\), “virginica” as \\(X=2\\), and “versicolor” as \\(X=3\\).\n\nCodedata(iris) # this step is not always necessary, but ensures you're starting  \n# from the original version of a dataset stored in a loaded package\n\niris = \n  iris |&gt; \n  tibble() |&gt;\n  mutate(\n    X = case_when(\n      Species == \"setosa\" ~ 1,\n      Species == \"virginica\" ~ 2,\n      Species == \"versicolor\" ~ 3\n    )\n  )\n\niris |&gt; \n  distinct(Species, X) |&gt; \n  print()\n\n# A tibble: 3 × 2\n  Species        X\n  &lt;fct&gt;      &lt;dbl&gt;\n1 setosa         1\n2 versicolor     3\n3 virginica      2\n\n\nThen we could fit a model like:\n\nCodeiris_lm1 = lm(Sepal.Length ~ X, data = iris)\niris_lm1 |&gt; parameters() |&gt; print_md()\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(148)\np\n\n\n\n(Intercept)\n4.91\n0.16\n(4.60, 5.23)\n30.83\n&lt; .001\n\n\nX\n0.47\n0.07\n(0.32, 0.61)\n6.30\n&lt; .001"
  },
  {
    "objectID": "Linear-models-overview.html#lets-see-how-that-model-looks",
    "href": "Linear-models-overview.html#lets-see-how-that-model-looks",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n12.5 Let’s see how that model looks:",
    "text": "12.5 Let’s see how that model looks:\n\nCodeiris_plot1 = iris |&gt; \n  ggplot(\n    aes(\n      x = X, \n      y = Sepal.Length)\n  ) +\n  geom_point(alpha = .1) +\n  geom_abline(\n    intercept = coef(iris_lm1)[1], \n    slope = coef(iris_lm1)[2]) +\n  theme_bw(base_size = 18)\n\n\n\nCodeggplotly(iris_plot1)\n\n\n\n\n\nWe have forced the model to use a straight line for the three estimated means. Maybe not a good idea?"
  },
  {
    "objectID": "Linear-models-overview.html#lets-see-what-r-does-with-categorical-variables-by-default",
    "href": "Linear-models-overview.html#lets-see-what-r-does-with-categorical-variables-by-default",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n12.6 Let’s see what R does with categorical variables by default:",
    "text": "12.6 Let’s see what R does with categorical variables by default:\n\nCodeiris_lm2 = lm(Sepal.Length ~ Species, data = iris)\niris_lm2 |&gt; parameters() |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(147)\np\n\n\n\n(Intercept)\n5.01\n0.07\n(4.86, 5.15)\n68.76\n&lt; .001\n\n\nSpecies (versicolor)\n0.93\n0.10\n(0.73, 1.13)\n9.03\n&lt; .001\n\n\nSpecies (virginica)\n1.58\n0.10\n(1.38, 1.79)\n15.37\n&lt; .001"
  },
  {
    "objectID": "Linear-models-overview.html#re-parametrize-with-no-intercept",
    "href": "Linear-models-overview.html#re-parametrize-with-no-intercept",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n12.7 Re-parametrize with no intercept",
    "text": "12.7 Re-parametrize with no intercept\nIf you don’t want the default and offset option, you can use “-1” like we’ve seen previously:\n\nCodeiris.lm2b = lm(Sepal.Length ~ Species - 1, data = iris)\niris.lm2b |&gt; parameters() |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(147)\np\n\n\n\nSpecies (setosa)\n5.01\n0.07\n(4.86, 5.15)\n68.76\n&lt; .001\n\n\nSpecies (versicolor)\n5.94\n0.07\n(5.79, 6.08)\n81.54\n&lt; .001\n\n\nSpecies (virginica)\n6.59\n0.07\n(6.44, 6.73)\n90.49\n&lt; .001"
  },
  {
    "objectID": "Linear-models-overview.html#lets-see-what-these-new-models-look-like",
    "href": "Linear-models-overview.html#lets-see-what-these-new-models-look-like",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n12.8 Let’s see what these new models look like:",
    "text": "12.8 Let’s see what these new models look like:\n\nCodeiris_plot2 = \n  iris |&gt; \n  mutate(\n    predlm2 = predict(iris_lm2)) |&gt; \n  arrange(X) |&gt; \n  ggplot(aes(x = X, y = Sepal.Length)) +\n  geom_point(alpha = .1) +\n  geom_line(aes(y = predlm2), col = \"red\") +\n  geom_abline(\n    intercept = coef(iris_lm1)[1], \n    slope = coef(iris_lm1)[2]) + \n  theme_bw(base_size = 18)\n\n\n\nCodeggplotly(iris_plot2)"
  },
  {
    "objectID": "Linear-models-overview.html#lets-see-how-r-did-that",
    "href": "Linear-models-overview.html#lets-see-how-r-did-that",
    "title": "\n3  Linear (Gaussian) Models\n",
    "section": "\n12.9 Let’s see how R did that:",
    "text": "12.9 Let’s see how R did that:\n\nCodeformula(iris_lm2)\n\nSepal.Length ~ Species\n\nCodemodel.matrix(iris_lm2) |&gt; as_tibble() |&gt; unique()\n\n# A tibble: 3 × 3\n  `(Intercept)` Speciesversicolor Speciesvirginica\n          &lt;dbl&gt;             &lt;dbl&gt;            &lt;dbl&gt;\n1             1                 0                0\n2             1                 1                0\n3             1                 0                1\n\n\nThis is called a “corner point parametrization”.\n\nCodeformula(iris.lm2b)\n\nSepal.Length ~ Species - 1\n\nCodemodel.matrix(iris.lm2b) |&gt; as_tibble() |&gt; unique()\n\n# A tibble: 3 × 3\n  Speciessetosa Speciesversicolor Speciesvirginica\n          &lt;dbl&gt;             &lt;dbl&gt;            &lt;dbl&gt;\n1             1                 0                0\n2             0                 1                0\n3             0                 0                1\n\n\nThis can be called a “group point parametrization”.\nThere are more options; see Dobson & Barnett §6.4.1."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]