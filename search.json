[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression Models for Epidemiology",
    "section": "",
    "text": "Preface\nThis web-book is derived from my lecture slides for the Spring 2023 session of Epidemiology 204: “Quantitative Epidemiology III: Statistical Models”, at UC Davis.\nI have drawn these materials from many sources, including but not limited to:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Regression Models for Epidemiology",
    "section": "",
    "text": "David Rocke’s materials from the 2021 edition of Epi 204\nRegression methods in biostatistics: linear, logistic, survival, and repeated measures models, 2nd edition (Vittinghoff et al. 2012)\nAn Introduction to Generalized Linear Models, 4th edition (Dobson and Barnett 2018)\n\n\nLicense\nThis book is licensed to you under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.\nThe code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e. public domain.\n\n\n\n\n\n\nDobson, Annette J, and Adrian G Barnett. 2018. An Introduction to Generalized Linear Models. 4th ed. CRC press. https://doi.org/10.1201/9781315182780.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. 2nd ed. Springer. https://doi.org/10.1007/978-1-4614-1353-0.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro-to-GLMs.html",
    "href": "intro-to-GLMs.html",
    "title": "\n1  Introduction\n",
    "section": "",
    "text": "Configuring R\nFunctions from these packages will be used throughout this document:\n\nShow R codelibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(plotly) # interactive graphics\nlibrary(dplyr) # manipulate data\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(haven) # import Stata files\nlibrary(pander) # format tables for markdown\nlibrary(knitr) # format R output for markdown\nlibrary(kableExtra) # more markdown formatting\nlibrary(parameters) # format model output tables for markdown\nlibrary(reactable) # interactive tables\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(conflicted) # check for conflicting function definitions\n\n\nHere are some R settings I use in this document:\n\nShow R coderm(list = ls()) # delete any data that's already loaded into R\nknitr::opts_chunk$set(message = FALSE)\nknitr::opts_chunk$set(warning = FALSE)\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\noptions('digits' = 4)\nconflicts_prefer(plotly::filter)\nconflicts_prefer(ggplot2::autoplot)\n\n\n\n1.0.1 Introduction to Epi 204\nWelcome to Epidemiology 204: Quantitative Epidemiology III (Statistical Models).\nIn this course, we will start where Epi 203 left off: with linear regression models.\n\n\n\n\n\n\nNote\n\n\n\nEpi 203/STA 130B/STA 131B is a prerequisite for this course. If you haven’t passed one of these courses, please talk to me after class.\n\n\nWhat you should know\nEpi 202: probability models for different data types\n\nbinomial\nPoisson\nGaussian\nexponential\n\nEpi 203: inference for one or several homogenous populations\n\nthe maximum likelihood inference framework:\n\nlikelihood functions\nlog-likelihood functions\nscore functions\nestimating equations\ninformation matrices\npoint estimates\nstandard errors\nconfidence intervals\nhypothesis tests\np-values\n\n\nHypothesis tests for one, two, and &gt;2 groups:\n\nt-tests/ANOVA for Gaussian models\nchi-square tests for binomial and Poisson models\n\n\nSome linear regression\n\nStat 108: linear regression models\n\nbuilding models for Gaussian outcomes\n\nmultiple predictors\ninteractions\n\n\nregression diagnostics\nfundamentals of R programming; e.g.:\n\nR for Data Science (Wickham, Cetinkaya-Rundel, Grolemund 2023)\nIntroductory Statistics with R (Dalgaard 2008)\n\n\nRMarkdown or Quarto for formatting homework\nLaTeX for writing math in RMarkdown/Quarto\nWhat we will cover in this course\n\nLinear (Gaussian) regression models (review and more details)\n\nRegression models for non-Gaussian outcomes\n\nbinary\ncount\ntime to event\n\n\nStatistical analysis using R\n\n1.0.2 Regression models\nWhy do we need them?\n\ncontinuous predictors\nnot enough data to analyze some subgroups individually\n\nExample: Adelie penguins\n\nShow R codelibrary(ggplot2)\nlibrary(plotly)\nlibrary(dplyr)\nggpenguins &lt;- \n  palmerpenguins::penguins |&gt; \n  dplyr::filter(species == \"Adelie\") |&gt; \n  ggplot(\n    aes(x = bill_length_mm , y = body_mass_g)) +\n  geom_point() + \n  xlab(\"Bill length (mm)\") + \n  ylab(\"Body mass (g)\")\n\n\n\n\n\nPalmer penguins\n\n\nLinear regression\n\nShow R codeggpenguins2 = \n  ggpenguins +\n  stat_smooth(method = \"lm\",\n              formula = y ~ x,\n              geom = \"smooth\")\n\n\n\n\n\nPalmer penguins with linear regression fit\n\n\nCurved regression lines\n\nShow R codeggpenguins2 = ggpenguins +\n  stat_smooth(\n    method = \"lm\",\n    formula = y ~ log(x),\n    geom = \"smooth\") +\n  xlab(\"Bill length (mm)\") + \n  ylab(\"Body mass (g)\")\n\n\n\n\n\n\n\n\nMultiple regression\n\nShow R codeggpenguins =\n  palmerpenguins::penguins |&gt; \n  ggplot(\n    aes(x = bill_length_mm , \n        y = body_mass_g,\n        color = species\n    )\n  ) +\n  geom_point() +\n  stat_smooth(\n    method = \"lm\",\n    formula = y ~ x,\n    geom = \"smooth\") +\n  xlab(\"Bill length (mm)\") + \n  ylab(\"Body mass (g)\")\n\n\n\n\n\n\n\n\nModeling non-Gaussian outcomes\n\nShow R codelibrary(glmx)\ndata(BeetleMortality)\nbeetles = BeetleMortality |&gt;\n  mutate(\n    pct = died/n,\n    survived = n - died\n  )\n\nplot1 = \n  beetles |&gt; \n  ggplot(aes(x = dose, y = pct)) +\n  geom_point(aes(size = n)) +\n  xlab(\"Dose (log mg/L)\") +\n  ylab(\"Mortality rate (%)\") +\n  scale_y_continuous(labels = scales::percent) +\n  # xlab(bquote(log[10]), bquote(CS[2])) +\n  scale_size(range = c(1,2))\n\n\n\n\n\nFigure 1.1: Mortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\n\n\n\n\n\nWhy don’t we use linear regression?\n\nShow R codebeetles_long = \n  beetles  |&gt; \n  reframe(.by = everything(),\n          outcome = c(\n            rep(1, times = died), \n            rep(0, times = survived))\n  )\n\nlm1 = \n  beetles_long |&gt; \n  lm(\n    formula = outcome ~ dose, \n    data = _)\n\n\nrange1 = range(beetles$dose) + c(-.2, .2)\n\nf.linear = function(x) predict(lm1, newdata = data.frame(dose = x))\n\nplot2 = \n  plot1 + \n  geom_function(fun = f.linear, aes(col = \"Straight line\")) +\n  labs(colour=\"Model\", size = \"\")\n\n\n\n\n\nMortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\nZoom out\n\n\n\nMortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\nlog transformation of dose?\n\nShow R codelm2 = \n  beetles_long |&gt; \n  lm(formula = outcome ~ log(dose), data = _)\n\nf.linearlog = function(x) predict(lm2, newdata = data.frame(dose = x))\n\nplot3 = plot2 + \n  expand_limits(x = c(1.6, 2)) +\n  geom_function(fun = f.linearlog, aes(col = \"Log-transform dose\"))\n\n\n\n\n\nMortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\nLogistic regression\n\nShow R codeglm1 = beetles |&gt; \n  glm(formula = cbind(died, survived) ~ dose, family = \"binomial\")\n\nf = function(x) predict(glm1, newdata = data.frame(dose = x), type = \"response\")\n\nplot4 = plot3 + geom_function(fun = f, aes(col = \"Logistic regression\"))\n\n\n\n\n\nMortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\nThree parts to regression models\n\nWhat distribution does the outcome have for a specific subpopulation defined by covariates? (outcome model)\nHow does the combination of covariates relate to the mean? (link function)\nHow do the covariates combine? (linear predictor, interactions)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "glms.html",
    "href": "glms.html",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "This section is primarily adapted starting from the textbook “An Introduction to Generalized Linear Models” (4th edition, 2018) by Annette J. Dobson and Adrian G. Barnett:\nhttps://doi.org/10.1201/9781315182780\nThe type of predictive model one uses depends on several issues; one is the type of response.\n\nMeasured values such as quantity of a protein, age, weight usually can be handled in an ordinary linear regression model, possibly after a log transformation.\nPatient survival, which may be censored, calls for a different method (survival analysis, Cox regression).\nIf the response is binary, then can we use logistic regression models\nIf the response is a count, we can use Poisson regression\nIf the count has a higher variance than is consistent with the Poisson, we can use a negative binomial or over-dispersed Poisson\nOther forms of response can generate other types of generalized linear models\n\nWe need a linear predictor of the same form as in linear regression βx. In theory, such a linear predictor can generate any type of number as a prediction, positive, negative, or zero\nWe choose a suitable distribution for the type of data we are predicting (normal for any number, gamma for positive numbers, binomial for binary responses, Poisson for counts)\nWe create a link function which maps the mean of the distribution onto the set of all possible linear prediction results, which is the whole real line (-∞, ∞). The inverse of the link function takes the linear predictor to the actual prediction.\n\nOrdinary linear regression has identity link (no transformation by the link function) and uses the normal distribution\nIf one is predicting an inherently positive quantity, one may want to use the log link since ex is always positive.\nAn alternative to using a generalized linear model with a log link, is to transform the data using the log. This is a device that works well with measurement data and may be usable in other cases, but it cannot be used for 0/1 data or for count data that may be 0.\n\n\nR glm() Families\n\n\n\n\n\n\nFamily\nLinks\n\n\n\n\ngaussian\nidentity, log, inverse\n\n\nbinomial\nlogit, probit, cauchit, log, cloglog\n\n\ngamma\ninverse, identity, log\n\n\ninverse.gaussian\n1/mu^2, inverse, identity, log\n\n\nPoisson\nlog, identity, sqrt\n\n\nquasi\nidentity, logit, probit, cloglog, inverse, log, 1/mu^2 and sqrt\n\n\nquasibinomial\nlogit, probit, identity, cloglog, inverse, log, 1/mu^2 and sqrt\n\n\nquasipoisson\nlog, identity, logit, probit, cloglog, inverse, 1/mu^2 and sqrt\n\n\n\n\nR glm() Link Functions; \\(\\eta = X\\beta = g(\\mu)\\)\n\n\n\n\n\n\n\n\n\nName\nDomain\nRange\nLink Function\nInverse Link Function\n\n\n\n\nidentity\n\\((-\\infty, \\infty)\\)\n\\((-\\infty, \\infty)\\)\n\\(\\eta = \\mu\\)\n\\(\\mu = \\eta\\)\n\n\nlog\n\\((0,\\infty)\\)\n\\((-\\infty, \\infty)\\)\n\\(\\eta = \\log{\\mu}\\)\n\\(\\mu = \\exp{\\eta}\\)\n\n\ninverse\n\\((0, \\infty)\\)\n\\((0,\\infty)\\)\n\\(\\eta = 1/\\mu\\)\n\\(\\mu = 1/\\eta\\)\n\n\nlogit\n\\((0,1)\\)\n\\((-\\infty, \\infty)\\)\n\\(\\eta = \\log{\\mu/(1-\\mu)}\\)\n\\(\\mu = exp{\\eta}/(1+exp{\\eta})\\)\n\n\nprobit\n\\((0,1)\\)\n\\((-\\infty, \\infty)\\)\n\\(\\eta = \\Phi^-1{\\mu}\\)\n\\(\\mu = \\Phi(\\eta)\\)\n\n\ncloglog\n\\((0,1)\\)\n\\((-\\infty, \\infty)\\)\n\\(\\eta = \\log{-\\log{1-\\mu}}\\)\n\\(\\mu = {1-\\exp{-\\exp{\\eta}}}\\)\n\n\n1/mu^2\n\\((0,\\infty)\\)\n\\((0, \\infty)\\)\n\\(\\eta = 1/\\mu^2\\)\n\\(\\mu = 1/\\sqrt(\\eta)\\)\n\n\nsqrt\n\\((0,\\infty)\\)\n\\((0,\\infty)\\)\n\\(\\eta = \\sqrt{\\mu}\\)\n\\(\\mu = \\eta^2\\)",
    "crumbs": [
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "Linear-models-overview.html",
    "href": "Linear-models-overview.html",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "",
    "text": "2.0.1 Understanding Gaussian Linear Regression Models",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear (Gaussian) Models</span>"
    ]
  },
  {
    "objectID": "Linear-models-overview.html#footnotes",
    "href": "Linear-models-overview.html#footnotes",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "",
    "text": "using the definite article “the” would mean there is only one slope.↩︎",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear (Gaussian) Models</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html",
    "href": "logistic-regression.html",
    "title": "3  Models for Binary Outcomes (Logistic regression and variations)",
    "section": "",
    "text": "Acknowledgements\nThis content is adapted from:\n\nDobson and Barnett (2018), Chapter 7\nVittinghoff et al. (2012), Chapter 5\nhttps://dmrocke.ucdavis.edu/Class/EPI204-Spring-2021/EPI204-Spring-2021.html\n\n\n\nConfiguring R\nFunctions from these packages will be used throughout this document:\n\n\nShow R code\nlibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(conflicted) # check for conflicting function definitions\n\n\nHere are some R settings I use in this document:\n\n\nShow R code\nrm(list = ls()) # delete any data that's already loaded into R\nknitr::opts_chunk$set(message = FALSE)\noptions('digits' = 4)\n\n\n\n\n3.0.1 Introduction\n\nWhat is logistic regression?\nLogistic regression is a framework for modeling binary outcomes, conditional on one or more predictors (a.k.a. covariates).\n\nExercise 3.1 (Examples of binary outcomes) What are some examples of binary outcomes in the health sciences?\n\nSolution. Examples of binary outcomes include:\n\nexposure (exposed vs unexposed)\ndisease (diseased vs healthy)\nrecovery (recovered vs unrecovered)\nrelapse (relapse vs remission)\nreturn to hospital (returned vs not)\nvital status (dead vs alive)\n\n\n\nLogistic regression uses the Bernoulli distribution to model the outcome variable, conditional on one or more covariates.\n\nExercise 3.2 Write down a mathematical definition of the Bernoulli distribution.\n\nSolution. See Definition A.6.\n\n\n\n\nLogistic regression versus linear regression\nLogistic regression differs from linear regression, which uses the Gaussian (“normal”) distribution to model the outcome variable, conditional on the covariates.\n\nExercise 3.3 Recall: what kinds of outcomes is linear regression used for?\n\nSolution. Linear regression is typically used for numerical outcomes that aren’t event counts or waiting times for an event. Examples of outcomes that are often analyzed using linear regression include include weight, height, and income.\n\n\n\n\n\n3.0.2 Risk Estimation and Prediction\nIn Epi 203, you have already seen methods for modeling binary outcomes using one covariate that is also binary (such as exposure/non-exposure). In this section, we review one-covariate analyses, with a special focus on risk ratios and odds ratios, which are important concepts for interpreting logistic regression.\n\nExample 3.1 (Oral Contraceptive Use and Heart Attack)  \n\nResearch question: how does oral contraceptive (OC) use affect the risk of myocardial infarction (MI; a.k.a. heart attack)?\n\n\nThis was an issue when oral contraceptives were first developed, because the original formulations used higher concentrations of hormones. Modern OCs don’t have this issue.\n\nTable 3.1 contains simulated data for an imaginary follow-up (a.k.a. prospective) study in which two groups are identified, one using OCs and another not using OCs, and both groups are tracked for three years to determine how many in each groups have MIs.\n\n\nShow R code\nlibrary(dplyr)\noc_mi = \n  tribble(\n    ~OC, ~MI, ~Total,\n    \"OC use\", 13, 5000,\n    \"No OC use\", 7, 10000\n  ) |&gt; \n  mutate(`No MI` = Total - MI) |&gt; \n  relocate(`No MI`, .after = MI)\n\ntotals = \n  oc_mi |&gt; \n  summarize(across(c(MI, `No MI`, Total), sum)) |&gt; \n  mutate(OC = \"Total\")\n\noc_mi = bind_rows(oc_mi, totals)\n\npander(oc_mi)\n\n\n\n\nTable 3.1: Simulated data from study of oral contraceptive use and heart attack risk\n\n\n\n\n\n\n\n\n\n\n\n\nOC\nMI\nNo MI\nTotal\n\n\n\n\nOC use\n13\n4987\n5000\n\n\nNo OC use\n7\n9993\n10000\n\n\nTotal\n20\n14980\n15000\n\n\n\n\n\n\n\n\n\n\nExercise 3.4 Review: estimate the probabilities of MI for OC users and non-OC users in Example 3.1.\n\nSolution. \\[\\hat{p}(MI|OC) = \\frac{13}{5000} = 0.0026\\]\n\\[\\hat{p}(MI|\\neg OC) = \\frac{7}{10000} = 7\\times 10^{-4}\\]\n\n\n\n\n\n\n\n\nTwo meanings of “controls”\n\n\n\nDepending on context, “controls” can mean either individuals who don’t experience an exposure of interest, or individuals who don’t experience an outcome of interest.\n\nDefinition 3.1 (cases and controls in retrospective studies) In retrospective studies, participants who experience the outcome of interest are called cases, while participants who don’t experience that outcome are called controls.\n\n\nDefinition 3.2 (treatment groups and control groups in prospective studies) In prospective studies, the group of participants who experience the treatment or exposure of interest is called the treatment group, while the participants who receive the baseline or comparison treatment (for example, clinical trial participants who receive a placebo or a standard-of-care treatment rather than an experimental treatment) are called controls.\n\n\n\n\nComparing probabilities\n\nRisk differences\nThe simplest comparison of two probabilities, \\(\\pi_1\\), and \\(\\pi_2\\), is the difference of their values:\n\n\nDefinition 3.3 (Risk difference) The risk difference of two probabilities, \\(\\pi_1\\), and \\(\\pi_2\\), is the difference of their values: \\[\\delta(\\pi_1,\\pi_2) \\stackrel{\\text{def}}{=}\\pi_1 - \\pi_2\\]\n\nExample 3.2 In Example 3.1, the estimated risk difference in MI risk between OC users and OC non-users is:\n\\[\n\\begin{aligned}\n\\hat\\delta(\\pi(OC), \\pi(\\neg OC))\n&= \\delta(\\hat\\pi(OC), \\hat\\pi(\\neg OC))\\\\\n&= \\hat\\pi(OC) - \\hat\\pi(\\neg OC)\\\\\n&= 0.0026 - 7\\times 10^{-4}\\\\\n&= 0.0019\n\\end{aligned}\n\\]\n\n\n\n\nRisk ratios\n\n\nDefinition 3.4 (Relative risk ratios) The relative risk of probability \\(\\pi_1\\) compared to another probability \\(\\pi_2\\), also called the risk ratio, relative risk ratio, probability ratio, or rate ratio, is the ratio of those probabilities:\n\\[\\rho(\\pi_1,\\pi_2) = \\frac{\\pi_1}{\\pi_2}\\]\n\nExample 3.3 Above, we estimated that: \\[\\hat{p}(MI|OC) = 0.0026\\]\n\\[\\hat{p}(MI|\\neg OC) = 7\\times 10^{-4}\\]\nSo we might estimate that the relative risk of MI for OC versus non-OC is:\n\n\nShow R code\nrr = (13/5000)/(7/10000)\n\n\n\\[\n\\begin{aligned}\n\\hat\\rho(OC, \\neg OC)\n&=\\frac{\\hat{p}(MI|OC)}{\\hat{p}(MI|\\neg OC)}\\\\\n&= \\frac{0.0026}{7\\times 10^{-4}}\\\\\n&= 3.71428571\n\\end{aligned}\n\\]\nWe might summarize this result by saying that “the estimated probability of MI among OC users was 3.71428571 as high as the estimated probability among OC non-users.\n\n\nSometimes, we divide the risk difference by the comparison probability, or equivalently, subtract 1 from the risk ratio; the result is called the relative risk difference:\n\\[\n\\begin{aligned}\n\\xi(\\pi_1,\\pi_2)\n&= \\frac{\\pi_1-\\pi_2}{\\pi_2}\\\\\n&= \\frac{\\pi_1}{\\pi_2} - 1\n\\end{aligned}\\]\nRisk differences, risk ratios, and relative risk differences are defined by two probabilities, plus a choice of which probability is the baseline or reference probability (i.e., which probability is the subtrahend of the risk difference or the denominator of the risk ratio). To switch which one is the reference probability, invert the ratio and multiply the difference by -1.\n\nExample 3.4 Above, we estimated that the risk ratio of OC versus non-OC is:\n\\[\n\\begin{aligned}\n\\rho(OC, \\neg OC)\n&= 3.71428571\n\\end{aligned}\n\\]\nIn comparison, the risk ratio for non-OC versus OC is:\n\\[\n\\begin{aligned}\n\\rho(\\neg OC, OC)\n&=\\frac{\\hat{p}(MI|\\neg OC)}{\\hat{p}(MI|OC)}\\\\\n&= \\frac{7\\times 10^{-4}}{0.0026}\\\\\n&= 0.26923077\\\\\n&= \\frac{1}{\\rho(OC, \\neg OC)}\n\\end{aligned}\n\\]\n\n\n\n\nOdds and probabilities\nIn logistic regression, we will make use of a transformation (rescaling) of probability, called odds.\n\nDefinition 3.5 (Odds) The odds of an outcome, denoted \\(\\omega\\) (“omega”), is the probability that the outcome occurs, divided by the probability that it doesn’t occur.\nThat is, if the probability of an outcome is \\(\\pi\\), then the corresponding odds of that outcome is\n\\[\\omega(\\pi) \\stackrel{\\text{def}}{=}\\frac{\\pi}{1-\\pi}\\]\nThis function, which transforms probabilities into odds, is called the odds function (see ?fig-odds-probs).\n\n\n\nShow R code\nodds = function(pi) pi/(1-pi)\nlibrary(ggplot2)\nodds_plot = ggplot() + \n  geom_function(fun = odds, aes(col = \"odds function\")) +\n  xlim(0, .5) +\n  xlab(\"Probability\") +\n  ylab(\"Odds\") +\n  geom_abline(aes(intercept = 0, slope = 1, col = \"y=x\")) + \n  theme_bw() +\n  labs(colour = \"\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nShow R code\nplotly::ggplotly(odds_plot) |&gt;  \n  plotly::layout(legend = list(orientation = 'h'))\n\n\n\n\n\n\n\nExample 3.5 (Calculating odds) In Exercise 3.4, we estimated that the probability of MI, given OC use, is \\(\\pi(OC) \\stackrel{\\text{def}}{=}\\Pr(MI|OC) = 0.0026\\). If this estimate is correct, then the odds of MI, given OC use, is:\n\\[\n\\begin{aligned}\n\\omega(OC)\n&\\stackrel{\\text{def}}{=}\\frac{\\Pr(MI|OC)}{\\Pr(\\neg MI|OC)}\\\\\n&=\\frac{\\Pr(MI|OC)}{1-\\Pr(MI|OC)}\\\\\n&=\\frac{\\pi(OC)}{1-\\pi(OC)}\\\\\n&=\\frac{0.0026}{1-0.0026}\\\\\n&=0.00260678\n\\end{aligned}\n\\]\n\n\nExercise 3.5 (Calculating odds) Estimate the odds of MI, for non-OC users.\n\nSolution. \\[\n\\omega_(\\neg OC) = 7.00490343\\times 10^{-4}\n\\]\n\n\n\nA shortcut for calculating odds\nThe usual estimate for a probability of an event is “# events/# observations”. We often denote # events as \\(x\\) and # observations as \\(n\\). So: \\[\\hat\\pi = \\frac{x}{n}\\] Thus, the usual estimate for the probability of a nonevent is:\n\\[\n\\begin{aligned}\n1-\\hat\\pi\n&= 1-\\frac{x}{n}\\\\\n&= \\frac{n}{n} - \\frac{x}{n}\\\\\n&= \\frac{n - x}{n}\n\\end{aligned}\n\\]\nThus, the estimated odds is: \\[\n\\begin{aligned}\n\\frac{\\hat\\pi}{1-\\hat\\pi}\n&= \\frac{\\left(\\frac{x}{n}\\right)}{\\left(\\frac{n-x}{n}\\right)}\\\\\n&= \\frac{x}{n-x}\n\\end{aligned}\n\\] That is, odds can be calculated directly as “# events” divided by “# nonevents” (without needing to calculate \\(\\hat\\pi\\) and \\(1-\\hat\\pi\\) first).\n\nExample 3.6 (calculating odds using the shortcut) In Example 3.5, we calculated \\[\n\\begin{aligned}\n\\omega(OC)\n&=0.00260678\n\\end{aligned}\n\\] Let’s recalculate this result using our shortcut:\n\\[\n\\begin{aligned}\n\\omega(OC)\n&=\\frac{13}{5000-13}\\\\\n&=0.00260678\n\\end{aligned}\n\\]\nSame answer!\n\n\n\nOdds of rare events\nFor rare events (small \\(\\pi\\)), odds and probabilities are nearly equal, because \\(1-\\pi \\approx 1\\) (see ?fig-odds-probs).\nFor example, in Example 3.5, the probability and odds differ by \\(6.77762182\\times 10^{-6}\\).\n\nExercise 3.6 What odds value corresponds to the probability \\(\\pi = 0.2\\), and what is the numerical difference between these two values?\n\nSolution. \\[\n\\omega = \\frac{\\pi}{1-\\pi}\n=\\frac{.2}{.8}\n= .25\n\\]\n\n\n\n\n\nThe inverse odds function\n\nDefinition 3.6 (inverse odds function) The inverse odds function, \\[\\pi(\\omega)\\stackrel{\\text{def}}{=}\\frac{\\omega}{1+\\omega}\\] converts odds into their corresponding probabilities (Figure 3.1).\n\nThe inverse-odds function takes an odds as input and produces a probability as output. Its domain of inputs is \\([0,\\infty)\\) and its range of outputs is \\([0,1]\\).\n\n\nShow R code\nodds_inv = function(omega) (1 + omega^-1)^-1\nggplot() +\n  geom_function(fun = odds_inv, aes(col = \"inverse-odds\")) +\n  xlab(\"Odds\") +\n  ylab(\"Probability\") +\n  xlim(0,5) +\n  ylim(0,1) +\n  geom_abline(aes(intercept = 0, slope = 1, col = \"x=y\"))\n\n\n\n\n\nFigure 3.1: The inverse odds function, \\(\\pi(\\omega)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAn equivalent expression for the inverse odds function is \\[\\pi(\\omega) = (1-\\omega^{-1})^{-1} \\tag{3.1}\\]\n\n\n\nExercise 3.7 Prove that Equation 3.1 is equivalent to Definition 3.6.\n\n\nExercise 3.8 What probability corresponds to an odds of \\(\\omega = 1\\), and what is the numerical difference between these two values?\n\nSolution. \\[\n\\pi(1) = \\frac{1}{1+1}\n=\\frac{1}{2}\n= .5\n\\] \\[\n1 - \\pi(1) = 1 - .5 = .5\n\\]\n\n\n\n\nOdds ratios\nNow that we have defined odds, we can introduce another way of comparing event probabilities: odds ratios.\n\nDefinition 3.7 (Odds ratio) The odds ratio for two odds \\(\\omega_1\\), \\(\\omega_2\\) is their ratio:\n\\[\\theta(\\omega_1, \\omega_2) = \\frac{\\omega_1}{\\omega_2}\\]\n\n\nExample 3.7 (Calculating odds ratios) In Example 3.1, the odds ratio for OC users versus OC-non-users is:\n\\[\n\\begin{aligned}\n\\theta(\\omega(OC), \\omega(\\neg OC))\n&= \\frac{\\omega(OC)}{\\omega(\\neg OC)}\\\\\n&= \\frac{0.0026}{7\\times 10^{-4}}\\\\\n&= 3.71428571\\\\\n\\end{aligned}\n\\]\n\nWhen the outcome is rare (i.e., its probability is small) for both groups being compared in an odds ratio, the odds of the outcome will be similar to the probability of the outcome, and thus the risk ratio will be similar to the odds ratio.\nFor example, in Example 3.1, the outcome is rare for both OC and non-OC participants, so the odds for both groups are similar to the corresponding probabilities, and the odds ratio is similar the risk ratio.\n\nA shortcut for calculating odds ratio estimates\nThe general form of a two-by-two table is shown in Table 3.2.\n\n\n\nTable 3.2: A generic 2x2 table\n\n\n\n\n\n\nEvent\nNon-Event\nTotal\n\n\n\n\nExposed\na\nb\na+b\n\n\nNon-exposed\nc\nd\nc+d\n\n\nTotal\na+c\nb+d\na+b+c+d\n\n\n\n\n\n\nFrom this table, we have:\n\n\\(\\hat\\pi(Event|Exposed) = a/(a+b)\\)\n\\(\\hat\\pi(\\neg Event|Exposed) = b/(a+b)\\)\n\\(\\hat\\omega(Event|Exposed) = \\frac{\\left(\\frac{a}{a+b}\\right)}{\\left(\\frac{b}{a+b}\\right)}=\\frac{a}{b}\\)\n\\(\\hat\\omega(Event|\\neg Exposed) = \\frac{c}{d}\\) (see Exercise 3.9)\n\\(\\theta(Exposed,\\neg Exposed) = \\frac{\\frac{a}{b}}{\\frac{c}{d}} = \\frac{ad}{bc}\\)\n\n\nExercise 3.9 Given Table 3.2, show that \\(\\hat\\omega(Event|\\neg Exposed) = \\frac{c}{d}\\).\n\n\n\nProperties of odds ratios\nOdds ratios have a special property: we can swap a covariate with the outcome, and the odds ratio remains the same.\n\nExample 3.8 In Example 3.1, we have:\n\\[\n\\begin{aligned}\n\\theta(MI; OC)\n&\\stackrel{\\text{def}}{=}\n\\frac{\\omega(MI|OC)}{\\omega(MI|\\neg OC)}\\\\\n&\\stackrel{\\text{def}}{=}\\frac\n{\\left(\\frac{\\Pr(MI|OC)}{\\Pr(\\neg MI|OC)}\\right)}\n{\\left(\\frac{\\Pr(MI|\\neg OC)}{\\Pr(\\neg MI|\\neg OC)}\\right)}\\\\\n&= \\frac\n{\\left(\\frac{\\Pr(MI,OC)}{\\Pr(\\neg MI,OC)}\\right)}\n{\\left(\\frac{\\Pr(MI,\\neg OC)}{\\Pr(\\neg MI,\\neg OC)}\\right)}\\\\\n&= \\left(\\frac{\\Pr(MI,OC)}{\\Pr(\\neg MI,OC)}\\right)\n\\left(\\frac{\\Pr(\\neg MI,\\neg OC)}{\\Pr(MI,\\neg OC)}\\right)\\\\\n&= \\left(\\frac{\\Pr(MI,OC)}{\\Pr(MI,\\neg OC)}\\right)\n\\left(\\frac{\\Pr(\\neg MI,\\neg OC)}{\\Pr(\\neg MI,OC)}\\right)\\\\\n&= \\left(\\frac{\\Pr(OC,MI)}{\\Pr(\\neg OC,MI)}\\right)\n\\left(\\frac{\\Pr(\\neg OC,\\neg MI)}{\\Pr(OC,\\neg MI)}\\right)\\\\\n&= \\left(\\frac{\\Pr(OC|MI)}{\\Pr(\\neg OC|MI)}\\right)\n\\left(\\frac{\\Pr(\\neg OC|\\neg MI)}{\\Pr(OC|\\neg MI)}\\right)\\\\\n&= \\frac{\\left(\\frac{\\Pr(OC|MI)}{\\Pr(\\neg OC|MI)}\\right)}\n{\\left(\\frac{\\Pr(OC|\\neg MI)}{\\Pr(\\neg OC|\\neg MI)}\\right)}\\\\\n&\\stackrel{\\text{def}}{=}\\frac{\\omega(OC|MI)}\n{\\omega(OC|\\neg MI)}\\\\\n&\\stackrel{\\text{def}}{=}\\theta(OC; MI)\n\\end{aligned}\n\\]\n\n\nExercise 3.10 For Table 3.2, show that \\(\\hat\\theta(Exposed, Unexposed) = \\hat\\theta(Event, \\neg Event)\\).\n\n\n\n\nEffect of study design\nTable 3.1 simulates a follow-up study in which two populations were followed and the number of MI’s was observed. The risks are \\(P(MI|OC)\\) and \\(P(MI|\\neg OC)\\) and we can estimate these risks from the data.\nBut suppose we had a case-control study in which we had 100 women with MI and selected a comparison group of 100 women without MI (matched as groups on age, etc.). Then MI is not random, and we cannot compute P(MI|OC) and we cannot compute the risk ratio. However, the odds ratio however can be computed.\nThe disease odds ratio is the odds for the disease in the exposed group divided by the odds for the disease in the unexposed group, and we cannot validly compute and use these separate parts.\nBut we can validly compute and use the exposure odds ratio, which is the odds for exposure in the disease group divided by the odds for exposure in the non-diseased group (because exposure can be treated as random):\n\\[\n\\hat\\theta(OC|MI) =\n\\frac{\\hat{\\omega}(OC|MI)}{\\hat{\\omega}(OC|\\neg MI)}\n\\]\nAnd these two odds ratios, \\(\\hat\\theta(MI|OC)\\) and \\(\\hat\\theta(OC|MI)\\) are mathematically equivalent, as we saw in Section 3.0.2.4.2.\n\nExercise 3.11 Calculate the odds ratio of MI with respect to OC use, assuming that Table 3.1 comes from a case-control study. Confirm that the result is the same as in Example 3.7.\n\nSolution. \n\n\\(\\omega(OC|MI) = P(OC|MI)/(1 – P(OC|MI) = \\frac{13}{7} = 1.85714286\\)\n\\(\\omega(OC|\\neg MI) = P(OC|\\neg MI)/(1 – P(OC|\\neg MI) = \\frac{4987}{9993} = 0.49904933\\)\n\\(\\theta(OC,MI) = \\frac{\\omega(OC|MI)}{\\omega(OC|\\neg MI)} = \\frac{13/7}{4987/9993} = 3.72136125\\)\n\nThis is the same estimate we calculated in Example 3.7.\n\n\n\nCross-Sectional Studies\n\nIf a cross-sectional study is a probability sample of a population (which it rarely is) then we can estimate risks.\nIf it is a sample, but not an unbiased probability sample, then we need to treat it in the same way as a case-control study.\nWe can validly estimate odds ratios in either case.\nBut we can usually not validly estimate risks and risk ratios.\n\n\n\n\n\n3.0.3 Introduction to logistic regression\n\nIn Example 3.1, we estimated the risk and the odds of MI for two discrete cases, as to whether of not the individual used oral contraceptives.\nIf the predictor is quantitative (dose) or there is more than one predictor, the task becomes more difficult.\nIn this case, we will use logistic regression, which is a generalization of the linear regression models you have been using that can account for a binary response instead of a continuous one.\n\n\nBinary outcomes models - one group, no covariates\n\\[\n\\begin{aligned}\np(Y=1) &= \\pi\\\\\np(Y=0) &= 1-\\pi\\\\\np(Y=y) &= \\pi^y (1-\\pi)^{1-y}\\\\\n\\mathbf y  &= (y_1, ..., y_n)\\\\\n\\mathcal L(\\pi;\\mathbf y) &= \\pi^{\\sum y_i} (1-\\pi)^{n - \\sum y_i}\\\\\n\\ell(\\pi, \\mathbf y) &= \\left({\\sum y_i}\\right) \\text{log}\\left\\{\\pi\\right\\} + \\left(n - \\sum y_i\\right) \\text{log}\\left\\{1-\\pi\\right\\}\\\\\n&= \\left({\\sum y_i}\\right) \\left(\\text{log}\\left\\{\\pi\\right\\} - \\text{log}\\left\\{1-\\pi\\right\\}\\right) + n \\cdot \\text{log}\\left\\{1-\\pi\\right\\}\\\\\n&= \\left({\\sum y_i}\\right) \\text{log}\\left\\{\\frac{\\pi}{ 1-\\pi}\\right\\} + n \\cdot \\text{log}\\left\\{1-\\pi\\right\\}\n\\end{aligned}\n\\]\n\n\nBinary outcomes - general\n\\[\n\\begin{aligned}\np(Y_i=1) &= \\pi_i\\\\\np(Y_i=0) &= 1-\\pi_i\\\\\np(Y_i=y) &= (\\pi_i)^y (1-\\pi_i)^{1-y}\\\\\n\\mathbf y  &= (y_1, ..., y_n)\\\\\n\\mathcal L(\\pi;\\mathbf y) &= \\prod_{i=1}^n\n(\\pi_i)^{y_i} (1-\\pi_i)^{1 - y_i}\\\\\n\\ell(\\pi, \\mathbf y) &=\n\\sum_{i=1}^n\ny_i \\text{log}\\left\\{\\pi_i\\right\\} + (1 - y_i) \\text{log}\\left\\{1-\\pi_i\\right\\}\n\\end{aligned}\n\\]\n\n\nModeling \\(\\pi_i\\) as a function of \\(X_i\\)\nIf there are only a few distinct \\(X_i\\) values, we can model each one separately.\nOtherwise, we need regression.\n\\[\n\\begin{aligned}\n\\pi(x) &\\equiv \\text{E}(Y=1|X=x)\\\\\n&= f(x^\\top\\beta)\n\\end{aligned}\n\\]\nTypically, we use \\(f(\\eta) = \\text{expit}(\\eta)\\).\n\nDefinition 3.8 (expit function) The expit function (Figure 3.2), also known as the inverse-logit or logistic function, is:\n\\[\n\\begin{aligned}\n\\text{expit}(\\eta)\n&\\stackrel{\\text{def}}{=}\\frac{\\text{exp}\\left\\{\\eta\\right\\}}{1+\\text{exp}\\left\\{\\eta\\right\\}}\\\\\n&= (1 + \\text{exp}\\left\\{-\\eta\\right\\})^{-1}\n\\end{aligned}\n\\]\n\n\nShow R code\nexpit = function(eta) exp(eta)/(1+exp(eta))\nlibrary(ggplot2)\nexpit_plot = \n  ggplot() + \n  geom_function(fun = expit) + \n  xlim(-5, 5) + \n  ylim(0,1) +\n  ylab(expression(expit(eta))) +\n  xlab(expression(eta)) +\n  theme_bw()\nprint(expit_plot)\n\n\n\n\n\nFigure 3.2: The expit function\n\n\n\n\n\n\n\n\n\n\nDefinition 3.9 (logit function) The inverse of the expit function is the logit function:\n\\[g(p) = f^{-1}(p) = \\text{logit}(p) = \\text{log}\\left\\{\\frac{p}{1-p}\\right\\}\\]\n\n\n\nShow R code\nlogit = function(p) log(odds(p))\n\nlogit_plot = \n  ggplot() + \n  geom_function(fun = logit) + \n  xlim(.01, .99) + \n  ylab(\"logit(p)\") +\n  xlab(\"p\") +\n  theme_bw()\nprint(logit_plot)\n\n\n\n\n\nFigure 3.3: the logit function\n\n\n\n\n\n\n\n\n\n\nDiagram of expit and logit\n\\[\n\\underbrace{\\pi}_{\\atop{\\Pr(Y=1)} }\n\\overbrace{\n\\underbrace{\n\\underset{\n\\xleftarrow[\\frac{\\omega}{1+\\omega}]{}\n}\n{\n\\xrightarrow{\\frac{\\pi}{1-\\pi}}\n}\n\\underbrace{\\omega}_{\\text{odds}(Y=1)}\n\\underset{\n\\xleftarrow[\\text{exp}\\left\\{\\eta\\right\\}]{}\n}\n{\n\\xrightarrow{\\text{log}\\left\\{\\omega\\right\\}}\n}\n}_{\\text{expit}(\\eta)}\n}^{\\text{logit}(\\pi)}\n\\underbrace{\\eta}_{\\atop{\\text{log-odds}(Y=1)}}\n\\]\n\n\nMeet the beetles\n\n\nShow R code\nlibrary(glmx)\n\ndata(BeetleMortality, package = \"glmx\")\nbeetles = BeetleMortality |&gt;\n  mutate(\n    pct = died/n,\n    survived = n - died\n  )\n\nplot1 = \n  beetles |&gt; \n  ggplot(aes(x = dose, y = pct)) +\n  geom_point(aes(size = n)) +\n  xlab(\"Dose (log mg/L)\") +\n  ylab(\"Mortality rate (%)\") +\n  scale_y_continuous(labels = scales::percent) +\n  scale_size(range = c(1,2)) +\n  theme_bw(base_size = 18)\n\nprint(plot1)\n\n\n\n\n\n\n\n\n\nMortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\nWhy don’t we use linear regression?\n\n\nShow R code\nbeetles_glm_grouped = beetles |&gt; \n  glm(formula = cbind(died, survived) ~ dose, family = \"binomial\")\n\nlm1 = \n  beetles  |&gt; \n  reframe(\n    .by = everything(),\n    outcome = c(\n      rep(1, times = died), \n      rep(0, times = survived))\n  ) |&gt; \n  lm(\n    formula = outcome ~ dose, \n    data = _)\n\nlm2 = \n  beetles  |&gt; \n  reframe(\n    .by = everything(),\n    outcome = c(\n      rep(1, times = died), \n      rep(0, times = survived))\n  ) |&gt; \n  lm(\n    formula = outcome ~ log(dose), \n    data = _)\n\nrange1 = range(beetles$dose) + c(-.2, .2)\nf = function(x) predict(beetles_glm_grouped, newdata = data.frame(dose = x), type = \"response\")\nf.linear = function(x) predict(lm1, newdata = data.frame(dose = x))\nf.linearlog = function(x) predict(lm2, newdata = data.frame(dose = x))\n\nplot2 = \n  plot1 + \n  geom_function(\n    fun = f.linear, \n    aes(col = \"Straight line\")) +\n  labs(colour=\"Model\", size = \"\")\n\nplot2 |&gt; print()\n\n\n\n\n\n\n\n\n\n\n\nZoom out\n\n\nShow R code\n(plot2 + expand_limits(x = c(1.6, 2))) |&gt; print()\n\n\n\n\n\n\n\n\n\n\n\nlog transformation of dose?\n\n\nShow R code\nplot3 = plot2 + \n  expand_limits(x = c(1.6, 2)) +\n  geom_function(fun = f.linearlog, aes(col = \"Log-transform dose\"))\n(plot3 + expand_limits(x = c(1.6, 2))) |&gt; print()\n\n\n\n\n\n\n\n\n\n\n\nLogistic regression\n\n\nShow R code\nplot4 = plot3 + geom_function(fun = f, aes(col = \"Logistic regression\"))\nplot4 |&gt; print()\n\n\n\n\n\n\n\n\n\n\n\nThree parts to regression models\n\nWhat distribution does the outcome have for a specific subpopulation defined by covariates? (outcome model)\nHow does the combination of covariates relate to the mean? (link function)\nHow do the covariates combine? (linear predictor, interactions)\n\n\n\nLogistic regression in R\n\n\nShow R code\nbeetles_glm_grouped = \n  beetles |&gt; \n  glm(\n    formula = cbind(died, survived) ~ dose, \n    family = \"binomial\")\n\nbeetles_glm_grouped |&gt; \n  parameters() |&gt; \n  print_md()\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n-60.72\n5.18\n(-71.44, -51.08)\n-11.72\n&lt; .001\n\n\ndose\n34.27\n2.91\n(28.85, 40.30)\n11.77\n&lt; .001\n\n\n\n\n\nFitted values:\n\n\nShow R code\nfitted.values(beetles_glm_grouped)\n\n\n     1      2      3      4      5      6      7      8 \n0.0586 0.1640 0.3621 0.6053 0.7952 0.9032 0.9552 0.9790 \n\n\nShow R code\npredict(beetles_glm_grouped, type = \"response\")\n\n\n     1      2      3      4      5      6      7      8 \n0.0586 0.1640 0.3621 0.6053 0.7952 0.9032 0.9552 0.9790 \n\n\nShow R code\npredict(beetles_glm_grouped, type = \"link\")\n\n\n      1       2       3       4       5       6       7       8 \n-2.7766 -1.6286 -0.5662  0.4277  1.3564  2.2337  3.0596  3.8444 \n\n\nShow R code\nfit_y = beetles$n * fitted.values(beetles_glm_grouped)\n\n\n\n\nIndividual observations\n\n\nShow R code\nbeetles_long = \n  beetles  |&gt; \n  reframe(\n    .by = everything(),\n    outcome = c(\n      rep(1, times = died), \n      rep(0, times = survived))\n  )\nbeetles_long |&gt; tibble() |&gt; print()\n\n\n# A tibble: 481 × 6\n    dose  died     n   pct survived outcome\n   &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;    &lt;int&gt;   &lt;dbl&gt;\n 1  1.69     6    59 0.102       53       1\n 2  1.69     6    59 0.102       53       1\n 3  1.69     6    59 0.102       53       1\n 4  1.69     6    59 0.102       53       1\n 5  1.69     6    59 0.102       53       1\n 6  1.69     6    59 0.102       53       1\n 7  1.69     6    59 0.102       53       0\n 8  1.69     6    59 0.102       53       0\n 9  1.69     6    59 0.102       53       0\n10  1.69     6    59 0.102       53       0\n# ℹ 471 more rows\n\n\nHere’s the model with individual data\n\n\nShow R code\nbeetles_glm_ungrouped = \n  beetles_long |&gt; \n  glm(\n    formula = outcome ~ dose, \n    family = \"binomial\")\n\nbeetles_glm_ungrouped |&gt; parameters() |&gt; print_md()\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n-60.72\n5.18\n(-71.44, -51.08)\n-11.72\n&lt; .001\n\n\ndose\n34.27\n2.91\n(28.85, 40.30)\n11.77\n&lt; .001\n\n\n\n\n\nHere’s the previous version again:\n\n\nShow R code\nbeetles_glm_grouped |&gt; parameters() |&gt; print_md()\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n-60.72\n5.18\n(-71.44, -51.08)\n-11.72\n&lt; .001\n\n\ndose\n34.27\n2.91\n(28.85, 40.30)\n11.77\n&lt; .001\n\n\n\n\n\nThey seem the same! But not quite:\n\n\nShow R code\nlogLik(beetles_glm_grouped)\n\n\n'log Lik.' -18.72 (df=2)\n\n\nShow R code\nlogLik(beetles_glm_ungrouped)\n\n\n'log Lik.' -186.2 (df=2)\n\n\nThe difference is due to the binomial coefficient \\(\\left(n\\atop x \\right)\\) which isn’t included in the individual-observations (Bernoulli) version of the model.\n\n\n\n3.0.4 Multiple logistic regression\n\nCoronary heart disease (WCGS) study data\nLet’s use the data from the following study to explore multiple logistic regression:\n\nSummary of study\nFrom Vittinghoff et al. (2012):\n“The Western Collaborative Group Study (WCGS) was a large epidemiological study designed to investigate the association between the”type A” behavior pattern and coronary heart disease (CHD) (Rosenman et al. 1964).”\nFrom Wikipedia, “Type A and Type B personality theory”:\n“The hypothesis describes Type A individuals as outgoing, ambitious, rigidly organized, highly status-conscious, impatient, anxious, proactive, and concerned with time management….\nThe hypothesis describes Type B individuals as a contrast to those of Type A. Type B personalities, by definition, are noted to live at lower stress levels. They typically work steadily and may enjoy achievement, although they have a greater tendency to disregard physical or mental stress when they do not achieve.”\n\n\nStudy design\nfrom ?faraway::wcgs:\n3154 healthy young men aged 39-59 from the San Francisco area were assessed for their personality type. All were free from coronary heart disease at the start of the research. Eight and a half years later change in CHD status was recorded.\nDetails (from faraway::wcgs)\nThe WCGS began in 1960 with 3,524 male volunteers who were employed by 11 California companies. Subjects were 39 to 59 years old and free of heart disease as determined by electrocardiogram. After the initial screening, the study population dropped to 3,154 and the number of companies to 10 because of various exclusions. The cohort comprised both blue- and white-collar employees.\nAt baseline the following information was collected:\n\nsocio-demographic including:\nage\neducation\nmarital status\nincome\noccupation\nphysical and physiological including:\nheight\nweight\nblood pressure\nelectrocardiogram\ncorneal arcus;\nbiochemical including:\ncholesterol and lipoprotein fractions;\nmedical and family history and use of medications;\nbehavioral data including\nType A interview,\nsmoking,\nexercise\nalcohol use.\n\nLater surveys added data on:\n\nanthropometry\ntriglycerides\nJenkins Activity Survey\ncaffeine use\n\nAverage follow-up continued for 8.5 years with repeat examinations.\nReference: Coronary Heart Disease in the Western Collaborative Group Study Final Follow-up Experience of 8 1/2 Years Ray H. Rosenman, MD; Richard J. Brand, PhD; C. David Jenkins, PhD; Meyer Friedman, MD; Reuben Straus, MD; Moses Wurm, MD JAMA. 1975;233(8):872-877. doi:10.1001/jama.1975.03260080034016.\n\n\n\nLoad the data\nHere, I load the data:\n\n\nShow R code\n### load the data directly from a UCSF website:\n# library(haven)\n# url = paste0( \n#     # I'm breaking up the url into two chunks for readability\n#     \"https://regression.ucsf.edu/sites/g/files/\",\n#     \"tkssra6706/f/wysiwyg/home/data/wcgs.dta\")\n# wcgs = haven::read_dta(url)\n\n\n# I presaved the data in my project's `data` folder\nlibrary(here) # provides the `here()` function\nlibrary(fs) # provides the `path()` function\nhere::here() |&gt; \n  fs::path('data/wcgs.rda') |&gt; \n  load()\n\n\n\n\nNow let’s do some data cleaning\n\n\nShow R code\nlibrary(arsenal) # provides `set_labels()`\n\nwcgs = wcgs |&gt; \n  mutate(\n    age = age |&gt; \n      arsenal::set_labels(\"Age (years)\"),\n    \n    arcus = \n      arcus |&gt; \n      as.logical() |&gt; \n      arsenal::set_labels(\"Arcus Senilis\"),\n    \n    time169 = \n      time169 |&gt; \n      as.numeric() |&gt; \n      arsenal::set_labels(\"Observation (follow up) time (days)\"),\n    \n    dibpat =\n      dibpat |&gt; \n      as_factor() |&gt; \n      relevel(ref = \"Type A\") |&gt; \n      arsenal::set_labels(\"Behavioral Pattern\"),\n    \n    typchd69 = typchd69 |&gt; \n      labelled(\n        label = \"Type of CHD Event\",\n        labels = \n          c(\n            \"None\" = 0, \n            \"infdeath\" = 1,\n            \"silent\" = 2,\n            \"angina\" = 3)),\n    \n    # turn stata-style labelled variables in to R-style factors:\n    across(\n      where(is.labelled), \n      haven::as_factor)\n  )\n\n\n\n\nWhat’s in the data\nHere’s a table of the data:\n\nShow R code\nwcgs |&gt;\n  select(-c(id, uni, t1)) |&gt;\n  tableby(chd69 ~ ., data = _) |&gt;\n  summary(\n    pfootnote = TRUE,\n    title =\n      \"Baseline characteristics by CHD status at end of follow-up\")\n\n\nBaseline characteristics by CHD status at end of follow-up\n\n\n\n\n\n\n\n\n\n\nNo (N=2897)\nYes (N=257)\nTotal (N=3154)\np value\n\n\n\n\nAge (years)\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n46.082 (5.457)\n48.490 (5.801)\n46.279 (5.524)\n\n\n\n   Range\n39.000 - 59.000\n39.000 - 59.000\n39.000 - 59.000\n\n\n\nArcus Senilis\n\n\n\n&lt; 0.0012\n\n\n   N-Miss\n0\n2\n2\n\n\n\n   FALSE\n2058 (71.0%)\n153 (60.0%)\n2211 (70.1%)\n\n\n\n   TRUE\n839 (29.0%)\n102 (40.0%)\n941 (29.9%)\n\n\n\nBehavioral Pattern\n\n\n\n&lt; 0.0012\n\n\n   A1\n234 (8.1%)\n30 (11.7%)\n264 (8.4%)\n\n\n\n   A2\n1177 (40.6%)\n148 (57.6%)\n1325 (42.0%)\n\n\n\n   B3\n1155 (39.9%)\n61 (23.7%)\n1216 (38.6%)\n\n\n\n   B4\n331 (11.4%)\n18 (7.0%)\n349 (11.1%)\n\n\n\nBody Mass Index (kg/m2)\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n24.471 (2.561)\n25.055 (2.579)\n24.518 (2.567)\n\n\n\n   Range\n11.191 - 37.653\n19.225 - 38.947\n11.191 - 38.947\n\n\n\nTotal Cholesterol\n\n\n\n&lt; 0.0011\n\n\n   N-Miss\n12\n0\n12\n\n\n\n   Mean (SD)\n224.261 (42.217)\n250.070 (49.396)\n226.372 (43.420)\n\n\n\n   Range\n103.000 - 400.000\n155.000 - 645.000\n103.000 - 645.000\n\n\n\nDiastolic Blood Pressure\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n81.723 (9.621)\n85.315 (10.311)\n82.016 (9.727)\n\n\n\n   Range\n58.000 - 150.000\n64.000 - 122.000\n58.000 - 150.000\n\n\n\nBehavioral Pattern\n\n\n\n&lt; 0.0012\n\n\n   Type A\n1411 (48.7%)\n178 (69.3%)\n1589 (50.4%)\n\n\n\n   Type B\n1486 (51.3%)\n79 (30.7%)\n1565 (49.6%)\n\n\n\nHeight (inches)\n\n\n\n0.2901\n\n\n   Mean (SD)\n69.764 (2.539)\n69.938 (2.410)\n69.778 (2.529)\n\n\n\n   Range\n60.000 - 78.000\n63.000 - 77.000\n60.000 - 78.000\n\n\n\nLn of Systolic Blood Pressure\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n4.846 (0.110)\n4.900 (0.125)\n4.850 (0.112)\n\n\n\n   Range\n4.585 - 5.438\n4.605 - 5.298\n4.585 - 5.438\n\n\n\nLn of Weight\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n5.126 (0.123)\n5.155 (0.118)\n5.128 (0.123)\n\n\n\n   Range\n4.357 - 5.670\n4.868 - 5.768\n4.357 - 5.768\n\n\n\nCigarettes per day\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n11.151 (14.329)\n16.665 (15.657)\n11.601 (14.518)\n\n\n\n   Range\n0.000 - 99.000\n0.000 - 60.000\n0.000 - 99.000\n\n\n\nSystolic Blood Pressure\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n128.034 (14.746)\n135.385 (17.473)\n128.633 (15.118)\n\n\n\n   Range\n98.000 - 230.000\n100.000 - 200.000\n98.000 - 230.000\n\n\n\nCurrent smoking\n\n\n\n&lt; 0.0012\n\n\n   No\n1554 (53.6%)\n98 (38.1%)\n1652 (52.4%)\n\n\n\n   Yes\n1343 (46.4%)\n159 (61.9%)\n1502 (47.6%)\n\n\n\nObservation (follow up) time (days)\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n2775.158 (562.205)\n1654.700 (859.297)\n2683.859 (666.524)\n\n\n\n   Range\n238.000 - 3430.000\n18.000 - 3229.000\n18.000 - 3430.000\n\n\n\nType of CHD Event\n\n\n\n\n\n\n   None\n0 (0.0%)\n0 (0.0%)\n0 (0.0%)\n\n\n\n   infdeath\n2897 (100.0%)\n0 (0.0%)\n2897 (91.9%)\n\n\n\n   silent\n0 (0.0%)\n135 (52.5%)\n135 (4.3%)\n\n\n\n   angina\n0 (0.0%)\n71 (27.6%)\n71 (2.3%)\n\n\n\n   4\n0 (0.0%)\n51 (19.8%)\n51 (1.6%)\n\n\n\nWeight (lbs)\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n169.554 (21.010)\n174.463 (21.573)\n169.954 (21.096)\n\n\n\n   Range\n78.000 - 290.000\n130.000 - 320.000\n78.000 - 320.000\n\n\n\nWeight Category\n\n\n\n&lt; 0.0012\n\n\n   &lt; 140\n217 (7.5%)\n15 (5.8%)\n232 (7.4%)\n\n\n\n   140-170\n1440 (49.7%)\n98 (38.1%)\n1538 (48.8%)\n\n\n\n   170-200\n1049 (36.2%)\n122 (47.5%)\n1171 (37.1%)\n\n\n\n   &gt; 200\n191 (6.6%)\n22 (8.6%)\n213 (6.8%)\n\n\n\nRECODE of age (Age)\n\n\n\n&lt; 0.0012\n\n\n   35-40\n512 (17.7%)\n31 (12.1%)\n543 (17.2%)\n\n\n\n   41-45\n1036 (35.8%)\n55 (21.4%)\n1091 (34.6%)\n\n\n\n   46-50\n680 (23.5%)\n70 (27.2%)\n750 (23.8%)\n\n\n\n   51-55\n463 (16.0%)\n65 (25.3%)\n528 (16.7%)\n\n\n\n   56-60\n206 (7.1%)\n36 (14.0%)\n242 (7.7%)\n\n\n\n\n\nLinear Model ANOVA\nPearson’s Chi-squared test\n\n\n\nData by age and personality type\nFor now, we will look at the interaction between age and personality type (dibpat). To make it easier to visualize the data, we summarize the event rates for each combination of age:\n\n\nShow R code\nchd_grouped_data = \n  wcgs |&gt; \n  summarize(\n    .by = c(age, dibpat),\n    n = n(),\n    `p(chd)` = mean(chd69 == \"Yes\") |&gt; \n      labelled(label = \"CHD Event by 1969\"),\n    `odds(chd)` = `p(chd)`/(1-`p(chd)`),\n    `logit(chd)` = log(`odds(chd)`)\n  )\n\nchd_grouped_data\n\n\n\n\n\nage\ndibpat\nn\np(chd)\nodds(chd)\nlogit(chd)\n\n\n\n\n50\nType A\n76\n0.105263\n0.1176\n-2.140\n\n\n51\nType A\n67\n0.164179\n0.1964\n-1.627\n\n\n59\nType A\n30\n0.233333\n0.3043\n-1.190\n\n\n44\nType A\n113\n0.079646\n0.0865\n-2.447\n\n\n47\nType A\n72\n0.097222\n0.1077\n-2.228\n\n\n40\nType A\n133\n0.067669\n0.0726\n-2.623\n\n\n41\nType A\n108\n0.064815\n0.0693\n-2.669\n\n\n43\nType A\n97\n0.072165\n0.0778\n-2.554\n\n\n54\nType A\n53\n0.132075\n0.1522\n-1.883\n\n\n48\nType A\n80\n0.150000\n0.1765\n-1.735\n\n\n39\nType A\n128\n0.085938\n0.0940\n-2.364\n\n\n49\nType A\n67\n0.238806\n0.3137\n-1.159\n\n\n55\nType A\n55\n0.163636\n0.1957\n-1.631\n\n\n56\nType A\n49\n0.244898\n0.3243\n-1.126\n\n\n42\nType A\n101\n0.039604\n0.0412\n-3.188\n\n\n45\nType A\n77\n0.090909\n0.1000\n-2.303\n\n\n46\nType A\n91\n0.065934\n0.0706\n-2.651\n\n\n57\nType A\n31\n0.129032\n0.1481\n-1.909\n\n\n53\nType A\n62\n0.112903\n0.1273\n-2.061\n\n\n52\nType A\n65\n0.200000\n0.2500\n-1.386\n\n\n58\nType A\n34\n0.147059\n0.1724\n-1.758\n\n\n45\nType B\n109\n0.045872\n0.0481\n-3.035\n\n\n41\nType B\n125\n0.040000\n0.0417\n-3.178\n\n\n47\nType B\n75\n0.013333\n0.0135\n-4.304\n\n\n39\nType B\n138\n0.057971\n0.0615\n-2.788\n\n\n49\nType B\n67\n0.074627\n0.0806\n-2.518\n\n\n51\nType B\n56\n0.107143\n0.1200\n-2.120\n\n\n42\nType B\n121\n0.008264\n0.0083\n-4.787\n\n\n50\nType B\n59\n0.050847\n0.0536\n-2.927\n\n\n44\nType B\n122\n0.032787\n0.0339\n-3.384\n\n\n56\nType B\n27\n0.000000\n0.0000\n-Inf\n\n\n40\nType B\n144\n0.020833\n0.0213\n-3.850\n\n\n58\nType B\n22\n0.045455\n0.0476\n-3.045\n\n\n48\nType B\n84\n0.083333\n0.0909\n-2.398\n\n\n43\nType B\n118\n0.050847\n0.0536\n-2.927\n\n\n53\nType B\n43\n0.116279\n0.1316\n-2.028\n\n\n54\nType B\n54\n0.074074\n0.0800\n-2.526\n\n\n46\nType B\n79\n0.063291\n0.0676\n-2.695\n\n\n52\nType B\n48\n0.041667\n0.0435\n-3.135\n\n\n55\nType B\n25\n0.040000\n0.0417\n-3.178\n\n\n57\nType B\n32\n0.093750\n0.1034\n-2.269\n\n\n59\nType B\n17\n0.235294\n0.3077\n-1.179\n\n\n\n\n\n\n\nGraphical exploration\n\nProbability scale\n\n\nShow R code\nlibrary(ggplot2)\nlibrary(ggeasy)\nlibrary(scales)\nchd_plot_probs = \n  chd_grouped_data |&gt; \n  ggplot(\n    aes(\n      x = age, \n      y = `p(chd)`, \n      col = dibpat)\n  ) +\n  geom_point(aes(size = n), alpha = .7) + \n  scale_size(range = c(1,4)) +\n  geom_line() +\n  theme_bw() +\n  ylab(\"P(CHD Event by 1969)\") +\n  scale_y_continuous(\n    labels = scales::label_percent(),\n    sec.axis = sec_axis(\n      ~ odds(.),\n      name = \"odds(CHD Event by 1969)\")) +\n  ggeasy::easy_labs() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nShow R code\nggplotly(chd_plot_probs)\n\n\n\n\nFigure 3.4: CHD rates by age group, probability scale\n\n\n\n\n\n\n\n\nShow R code\n# print(chd_plot_probs)\n\n\n\n\nOdds scale\n\n\nShow R code\ntrans_odds = trans_new(\n  name = \"odds\", \n  transform = odds, \n  inverse = odds_inv)\n\nchd_plot_probs + \n  scale_y_continuous(\n    trans = trans_odds, \n    name = paste(chd_plot_probs$labels$y, \"(odds spacing)\"),\n    sec.axis = sec_axis(\n      ~ odds(.),\n      name = \"odds(CHD Event by 1969)\"))\n\nchd_plot_odds = \n  chd_grouped_data |&gt; \n  ggplot(\n    aes(\n      x = age, \n      y = `odds(chd)`, \n      col = dibpat)\n  ) +\n  geom_point(aes(size = n), alpha = .7) + \n  scale_size(range = c(1,4)) +\n  geom_line() +\n  theme_bw() +\n  ylab(\"odds(CHD Event by 1969)\") +\n  ggeasy::easy_labs()\n\n\n\n\n\nFigure 3.5: CHD rates by age group, odds scale\n\n\n\n\n\n\n\n\n\n\nShow R code\nggplotly(chd_plot_odds)\n\n\n\n\n\n\n\n\nLog-odds (logit) scale\n\n\nShow R code\nchd_plot_logit = \n  chd_grouped_data |&gt; \n  ggplot(\n    aes(\n      x = age, \n      y = `logit(chd)`, \n      col = dibpat)\n  ) +\n  geom_point(aes(size = n), alpha = .7) + \n  scale_size(range = c(1,4)) +\n  geom_line() +\n  theme_bw() +\n  ylab(\"log{odds(CHD Event by 1969)}\") +\n  ggeasy::easy_labs()\n\n\n\n\nShow R code\nggplotly(chd_plot_logit)\n\n\n\n\n\n\n\n\n\nLogistic regression models for CHD data\nHere, we fit stratified models for CHD by personality type.\n\n\nShow R code\nchd_glm_strat = glm(\n  \"formula\" = chd69 == \"Yes\" ~ dibpat + dibpat:age - 1, \n  \"data\" = wcgs,\n  \"family\" = binomial(link = \"logit\")\n)\n\nchd_glm_strat |&gt; parameters() |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\ndibpat (Type A)\n-5.50\n0.67\n(-6.83, -4.19)\n-8.18\n&lt; .001\n\n\ndibpat (Type B)\n-5.80\n0.98\n(-7.73, -3.90)\n-5.95\n&lt; .001\n\n\ndibpat (Type A) × age\n0.07\n0.01\n(0.05, 0.10)\n5.24\n&lt; .001\n\n\ndibpat (Type B) × age\n0.06\n0.02\n(0.02, 0.10)\n3.01\n0.003\n\n\n\n\n\nWe can get the corresponding odds ratios (\\(e^{\\beta}\\)s) by passing exponentiate = TRUE to parameters():\n\n\nShow R code\nchd_glm_strat |&gt; \n  parameters(exponentiate = TRUE) |&gt; \n  print_md()\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nOdds Ratio\nSE\n95% CI\nz\np\n\n\n\n\ndibpat (Type A)\n4.09e-03\n2.75e-03\n(1.08e-03, 0.02)\n-8.18\n&lt; .001\n\n\ndibpat (Type B)\n3.02e-03\n2.94e-03\n(4.40e-04, 0.02)\n-5.95\n&lt; .001\n\n\ndibpat (Type A) × age\n1.07\n0.01\n(1.05, 1.10)\n5.24\n&lt; .001\n\n\ndibpat (Type B) × age\n1.06\n0.02\n(1.02, 1.11)\n3.01\n0.003\n\n\n\n\n\n\n\nModels superimposed on data\nWe can graph our fitted models on each scale (probability, odds, log-odds).\n\nprobability scale\n\n\nShow R code\ncurve_type_A = function(x) \n{\n  chd_glm_strat |&gt; predict(\n    type = \"response\",\n    newdata = tibble(age = x, dibpat = \"Type A\"))\n}\n\ncurve_type_B = function(x) \n{\n  chd_glm_strat |&gt; predict(\n    type = \"response\",\n    newdata = tibble(age = x, dibpat = \"Type B\"))\n}\n\nchd_plot_probs_2 =\n  chd_plot_probs +\n  geom_function(\n    fun = curve_type_A,\n    aes(col = \"Type A\")\n  ) +\n  geom_function(\n    fun = curve_type_B,\n    aes(col = \"Type B\")\n  )\n\n\n\n\nShow R code\nggplotly(chd_plot_probs_2)\n\n\n\n\n\n\n\n\nodds scale\n\n\nShow R code\ncurve_type_A = function(x) \n{\n  chd_glm_strat |&gt; predict(\n    type = \"link\",\n    newdata = tibble(age = x, dibpat = \"Type A\")) |&gt; exp()\n}\ncurve_type_B = function(x) \n{\n  chd_glm_strat |&gt; predict(\n    type = \"link\",\n    newdata = tibble(age = x, dibpat = \"Type B\")) |&gt; exp()\n}\n\nchd_plot_odds_2 =\n  chd_plot_odds +\n  geom_function(\n    fun = curve_type_A,\n    aes(col = \"Type A\")\n  ) +\n  geom_function(\n    fun = curve_type_B,\n    aes(col = \"Type B\")\n  )\n\n\n\n\nShow R code\nggplotly(chd_plot_odds_2)\n\n\n\n\n\n\n\n\nlog-odds (logit) scale\n\n\nShow R code\ncurve_type_A = function(x) \n{\n  chd_glm_strat |&gt; predict(\n    type = \"link\",\n    newdata = tibble(age = x, dibpat = \"Type A\"))\n}\ncurve_type_B = function(x) \n{\n  chd_glm_strat |&gt; predict(\n    type = \"link\",\n    newdata = tibble(age = x, dibpat = \"Type B\"))\n}\n\nchd_plot_logit_2 =\n  chd_plot_logit +\n  geom_function(\n    fun = curve_type_A,\n    aes(col = \"Type A\")\n  ) +\n  geom_function(\n    fun = curve_type_B,\n    aes(col = \"Type B\")\n  )\n\n\n\n\nShow R code\nggplotly(chd_plot_logit_2)\n\n\n\n\n\n\n\n\n\nreference-group and contrast parametrization\nWe can also use the corner-point parametrization (with reference groups and contrasts):\n\n\nShow R code\nchd_glm_contrasts = \n  wcgs |&gt; \n  glm(\n    \"data\" = _,\n    \"formula\" = chd69 == \"Yes\" ~ dibpat*age, \n    \"family\" = binomial(link = \"logit\")\n  )\n\nchd_glm_contrasts |&gt; \n  parameters() |&gt; \n  print_md()\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n-5.50\n0.67\n(-6.83, -4.19)\n-8.18\n&lt; .001\n\n\ndibpat (Type B)\n-0.30\n1.18\n(-2.63, 2.02)\n-0.26\n0.797\n\n\nage\n0.07\n0.01\n(0.05, 0.10)\n5.24\n&lt; .001\n\n\ndibpat (Type B) × age\n-0.01\n0.02\n(-0.06, 0.04)\n-0.42\n0.674\n\n\n\n\n\nCompare with what we had before:\n\n\nShow R code\nchd_glm_strat |&gt; \n  parameters() |&gt; \n  print_md()\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\ndibpat (Type A)\n-5.50\n0.67\n(-6.83, -4.19)\n-8.18\n&lt; .001\n\n\ndibpat (Type B)\n-5.80\n0.98\n(-7.73, -3.90)\n-5.95\n&lt; .001\n\n\ndibpat (Type A) × age\n0.07\n0.01\n(0.05, 0.10)\n5.24\n&lt; .001\n\n\ndibpat (Type B) × age\n0.06\n0.02\n(0.02, 0.10)\n3.01\n0.003\n\n\n\n\n\n\nExercise 3.12 If I give you model 1, how would you get the coefficients of model 2?\n\n\n\n\n3.0.5 Fitting logistic regression models\n\n\nIn general, the estimating equation \\(\\ell'(\\beta; \\mathbf x) = 0\\) cannot be solved analytically.\nInstead, we have to use a variant of the Newton-Raphson method, which was discussed briefly in Epi 203. We won’t go over it in this class; if you need to learn it, see Dobson and Barnett (2018), Chapter 4.\nFor now, all you need to know is that we make an iterative series of guesses, and each guess helps us make the next guess better (higher log-likelihood).\nYou can see some information about this process like so:\n\n\nShow R code\noptions(digits = 8)\ntemp = \n  wcgs |&gt; \n  glm(\n    control = glm.control(trace = TRUE),\n    \"data\" = _,\n    \"formula\" = chd69 == \"Yes\" ~ dibpat*age, \n    \"family\" = binomial(link = \"logit\")\n  )\n\n\nDeviance = 1775.7899 Iterations - 1\nDeviance = 1708.5396 Iterations - 2\nDeviance = 1704.0434 Iterations - 3\nDeviance = 1703.9833 Iterations - 4\nDeviance = 1703.9832 Iterations - 5\nDeviance = 1703.9832 Iterations - 6\n\n\nAfter each iteration of the fitting procedure, the deviance (\\(2(\\ell_{\\text{full}} - \\ell(\\hat\\beta))\\) ) is printed. You can see that the algorithm took six iterations to converge to a solution where the likelihood wasn’t changing much anymore.\n\n\n\n3.0.6 Model comparisons for logistic models\n\nDeviance test\nWe can compare the maximized log-likelihood of our model, \\(\\ell(\\hat\\beta; \\mathbf x)\\), versus the log-likelihood of the full model (aka saturated model aka maximal model), \\(\\ell_{\\text{full}}\\), which has one parameter per covariate pattern. With enough data, \\(2(\\ell_{\\text{full}} - \\ell(\\hat\\beta; \\mathbf x)) \\dot \\sim \\chi^2(N - p)\\), where \\(N\\) is the number of distinct covariate patterns and \\(p\\) is the number of \\(\\beta\\) parameters in our model. A significant p-value for this deviance statistic indicates that there’s some detectable pattern in the data that our model isn’t flexible enough to catch.\n\n\n\n\n\n\nCaution\n\n\n\nThe deviance statistic needs to have a large amount of data for each covariate pattern for the \\(\\chi^2\\) approximation to hold. A guideline from Dobson is that if there are \\(q\\) distinct covariate patterns \\(x_1...,x_q\\), with \\(n_1,...,n_q\\) observations per pattern, then the expected frequencies \\(n_k \\cdot \\pi(x_k)\\) should be at least 1 for every pattern \\(k\\in 1:q\\).\n\n\nIf you have covariates measured on a continuous scale, you may not be able to use the deviance tests to assess goodness of fit.\n\n\nHosmer-Lemeshow test\nIf our covariate patterns produce groups that are too small, a reasonable solution is to make bigger groups by merging some of the covariate-pattern groups together.\nHosmer and Lemeshow (1980) proposed that we group the patterns by their predicted probabilities according to the model of interest. For example, you could group all of the observations with predicted probabilities of 10% or less together, then group the observations with 11%-20% probability together, and so on; \\(g=10\\) categories in all.\nThen we can construct a statistic \\[X^2 = \\sum_{c=1}^g \\frac{(o_c - e_c)^2}{e_c}\\] where \\(o_c\\) is the number of events observed in group \\(c\\), and \\(e_c\\) is the number of events expected in group \\(c\\) (based on the sum of the fitted values \\(\\hat\\pi_i\\) for observations in group \\(c\\)).\nIf each group has enough observations in it, you can compare \\(X^2\\) to a \\(\\chi^2\\) distribution; by simulation, the degrees of freedom has been found to be approximately \\(g-2\\).\nFor our CHD model, this procedure would be:\n\n\nShow R code\nwcgs = \n  wcgs |&gt; \n  mutate(\n    pred_probs_glm1 = chd_glm_strat |&gt; fitted(),\n    pred_prob_cats1 = \n      pred_probs_glm1 |&gt; \n      cut(breaks = seq(0, 1, by = .1), \n          include.lowest = TRUE))\n\nHL_table = \n  wcgs |&gt; \n  summarize(\n    .by = pred_prob_cats1,\n    n = n(),\n    o = sum(chd69 == \"Yes\"),\n    e = sum(pred_probs_glm1)\n  )\n\nHL_table |&gt; pander()\n\n\n\n\n\n\n\n\n\n\n\npred_prob_cats1\nn\no\ne\n\n\n\n\n(0.1,0.2]\n785\n116\n108\n\n\n(0.2,0.3]\n64\n12\n13.77\n\n\n[0,0.1]\n2305\n129\n135.2\n\n\n\n\n\nShow R code\nX2 = HL_table |&gt; \n  summarize(\n    `X^2` = sum((o-e)^2/e)\n  ) |&gt; \n  pull(`X^2`)\nprint(X2)\n\n\n[1] 1.1102871\n\n\nShow R code\npval1 = pchisq(X2, lower = FALSE, df = nrow(HL_table) - 2)\n\n\nOur statistic is \\(X^2 = 1.11028711\\); \\(p(\\chi^2(1) &gt; 1.11028711) = 0.29201955\\), which is our p-value for detecting a lack of goodness of fit.\nUnfortunately that grouping plan left us with just three categories with any observations, so instead of grouping by 10% increments of predicted probability, typically analysts use deciles of the predicted probabilities:\n\n\nShow R code\nwcgs = \n  wcgs |&gt; \n  mutate(\n    pred_probs_glm1 = chd_glm_strat |&gt; fitted(),\n    pred_prob_cats1 = \n      pred_probs_glm1 |&gt; \n      cut(breaks = quantile(pred_probs_glm1, seq(0, 1, by = .1)), \n          include.lowest = TRUE))\n\nHL_table = \n  wcgs |&gt; \n  summarize(\n    .by = pred_prob_cats1,\n    n = n(),\n    o = sum(chd69 == \"Yes\"),\n    e = sum(pred_probs_glm1)\n  )\n\nHL_table |&gt; pander()\n\n\n\n\n\n\n\n\n\n\n\npred_prob_cats1\nn\no\ne\n\n\n\n\n(0.114,0.147]\n275\n48\n36.81\n\n\n(0.147,0.222]\n314\n51\n57.19\n\n\n(0.0774,0.0942]\n371\n27\n32.56\n\n\n(0.0942,0.114]\n282\n30\n29.89\n\n\n(0.0633,0.069]\n237\n17\n15.97\n\n\n(0.069,0.0774]\n306\n20\n22.95\n\n\n(0.0487,0.0633]\n413\n27\n24.1\n\n\n(0.0409,0.0487]\n310\n14\n14.15\n\n\n[0.0322,0.0363]\n407\n16\n13.91\n\n\n(0.0363,0.0409]\n239\n7\n9.48\n\n\n\n\n\nShow R code\nX2 = HL_table |&gt; \n  summarize(\n    `X^2` = sum((o-e)^2/e)\n  ) |&gt; \n  pull(`X^2`)\n\nprint(X2)\n\n\n[1] 6.7811383\n\n\nShow R code\npval1 = pchisq(X2, lower = FALSE, df = nrow(HL_table) - 2)\n\n\nNow we have more evenly split categories. The p-value is \\(0.56041994\\), still not significant.\nGraphically, we have compared:\n\n\nShow R code\nHL_plot = \n  HL_table |&gt; \n  ggplot(aes(x = pred_prob_cats1)) + \n  geom_line(aes(y = e, x = pred_prob_cats1, group = \"Expected\", col = \"Expected\")) +\n  geom_point(aes(y = e, size = n, col = \"Expected\")) +\n  geom_point(aes(y = o, size = n, col = \"Observed\")) +\n  geom_line(aes(y = o, col = \"Observed\", group = \"Observed\")) +\n  scale_size(range = c(1,4)) +\n  theme_bw() +\n  ylab(\"number of CHD events\") +\n  theme(axis.text.x = element_text(angle = 45))\n\n\n\n\nShow R code\nggplotly(HL_plot)\n\n\n\n\n\n\n\n\nComparing models\n\nAIC = \\(-2 * \\ell(\\hat\\theta) + 2 * p\\) [lower is better]\nBIC = \\(-2 * \\ell(\\hat\\theta) + p * \\text{log}(n)\\) [lower is better]\nlikelihood ratio [higher is better]\n\n\n\n\n3.0.7 Residual-based diagnostics\n\nLogistic regression residuals only work for grouped data\nResiduals only work if there is more than one observation for most covariate patterns.\nHere we will create the grouped-data version of our CHD model from the WCGS study:\n\n\nShow R code\nwcgs_grouped = \n  wcgs |&gt; \n  summarize(\n    .by = c(dibpat, age),\n    n = n(),\n    chd = sum(chd69 == \"Yes\"),\n    `!chd` = sum(chd69 == \"No\")\n  )\n\nchd_glm_strat_grouped = glm(\n  \"formula\" = cbind(chd, `!chd`) ~ dibpat + dibpat:age - 1, \n  \"data\" = wcgs_grouped,\n  \"family\" = binomial(link = \"logit\")\n)\n\nchd_glm_strat_grouped |&gt; parameters() |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\ndibpat (Type A)\n-5.50\n0.67\n(-6.83, -4.19)\n-8.18\n&lt; .001\n\n\ndibpat (Type B)\n-5.80\n0.98\n(-7.73, -3.90)\n-5.95\n&lt; .001\n\n\ndibpat (Type A) × age\n0.07\n0.01\n(0.05, 0.10)\n5.24\n&lt; .001\n\n\ndibpat (Type B) × age\n0.06\n0.02\n(0.02, 0.10)\n3.01\n0.003\n\n\n\n\n\n\n\n(Response) residuals\n\\[e_k \\stackrel{\\text{def}}{=}\\bar y_k - \\hat{\\pi}(x_k)\\]\n(\\(k\\) indexes the covariate patterns)\nWe can graph these residuals \\(e_k\\) against the fitted values \\(\\hat\\pi(x_k)\\):\n\n\nShow R code\nwcgs_grouped = \n  wcgs_grouped |&gt; \n  mutate(\n    fitted = chd_glm_strat_grouped |&gt; fitted(),\n    fitted_logit = fitted |&gt; logit(),\n    response_resids = \n      chd_glm_strat_grouped |&gt; resid(type = \"response\")\n  )\n\nwcgs_response_resid_plot = \n  wcgs_grouped |&gt; \n  ggplot(\n    mapping = aes(\n      x = fitted,\n      y = response_resids\n    )\n  ) + \n  geom_point(\n    aes(col = dibpat)\n  ) +\n  geom_hline(yintercept = 0) + \n1  geom_smooth(\n    se = TRUE,\n    method.args = list(\n      span=2/3,\n      degree=1,\n      family=\"symmetric\",\n      iterations=3),\n    method = stats::loess)\n\n\n\n1\n\nDon’t worry about these options for now; I chose them to match autoplot() as closely as I can. plot.glm and autoplot use stats::lowess instead of stats::loess; stats::lowess is older, hard to use with geom_smooth, and hard to match exactly with stats::loess; see https://support.bioconductor.org/p/2323/.]\n\n\n\n\n\n\nShow R code\nwcgs_response_resid_plot |&gt; ggplotly()\n\n\n\n\n\n\nWe can see a slight fan-shape here: observations on the right have larger variance (as expected since \\(var(\\bar y) = \\pi(1-\\pi)/n\\) is maximized when \\(\\pi = 0.5\\)).\n\n\nPearson residuals\nThe fan-shape in the response residuals plot isn’t necessarily a concern here, since we haven’t made an assumption of constant residual variance, as we did for linear regression.\nHowever, we might want to divide by the standard error in order to make the graph easier to interpret. Here’s one way to do that:\nThe Pearson (chi-squared) residual for covariate pattern \\(k\\) is: \\[\n\\begin{aligned}\nX_k &= \\frac{\\bar y_k - \\hat\\pi_k}{\\sqrt{\\hat \\pi_k (1-\\hat\\pi_k)/n_k}}\n\\end{aligned}\n\\]\nwhere \\[\n\\begin{aligned}\n\\hat\\pi_k\n&\\stackrel{\\text{def}}{=}\\hat\\pi(x_k)\\\\\n&\\stackrel{\\text{def}}{=}\\hat P(Y=1|X=x_k)\\\\\n&\\stackrel{\\text{def}}{=}\\text{expit}(x_i'\\hat \\beta)\\\\\n&\\stackrel{\\text{def}}{=}\\text{expit}(\\hat \\beta_0 + \\sum_{j=1}^p \\hat \\beta_j x_{ij})\n\\end{aligned}\n\\]\nLet’s take a look at the Pearson residuals for our CHD model from the WCGS data (graphed against the fitted values on the logit scale):\n\n\nShow R code\nlibrary(ggfortify)\n\n\n\n\nShow R code\nautoplot(chd_glm_strat_grouped, which = 1, ncol = 1) |&gt; print()\n\n\n\n\n\n\n\n\n\nThe fan-shape is gone, and these residuals don’t show any obvious signs of model fit issues.\n\nPearson residuals plot for beetles data\nIf we create the same plot for the beetles model, we see some strong evidence of a lack of fit:\n\n\nShow R code\nautoplot(beetles_glm_grouped, which = 1, ncol = 1) |&gt; print()\n\n\n\n\n\n\n\n\n\n\n\nPearson residuals with individual (ungrouped) data\nWhat happens if we try to compute residuals without grouping the data by covariate pattern?\n\n\nShow R code\nlibrary(ggfortify)\n\n\n\n\nShow R code\nautoplot(chd_glm_strat, which = 1, ncol = 1) |&gt; print()\n\n\n\n\n\n\n\n\n\nMeaningless.\n\n\nResiduals plot by hand (optional section)\nIf you want to check your understanding of what these residual plots are, try building them yourself:\n\n\nShow R code\nwcgs_grouped = \n  wcgs_grouped |&gt; \n  mutate(\n    fitted = chd_glm_strat_grouped |&gt; fitted(),\n    fitted_logit = fitted |&gt; logit(),\n    resids = chd_glm_strat_grouped |&gt; resid(type = \"pearson\")\n  )\n\nwcgs_resid_plot1 = \n  wcgs_grouped |&gt; \n  ggplot(\n    mapping = aes(\n      x = fitted_logit,\n      y = resids\n      \n    ) \n    \n  ) + \n  geom_point(\n    aes(col = dibpat)\n  ) +\n  geom_hline(yintercept = 0) + \n  geom_smooth(se = FALSE, \n              method.args = list(\n                span=2/3,\n                degree=1,\n                family=\"symmetric\",\n                iterations=3,\n                surface=\"direct\"\n                # span = 2/3, \n                # iterations = 3\n              ),\n              method = stats::loess)\n# plot.glm and autoplot use stats::lowess, which is hard to use with \n# geom_smooth and hard to match exactly; \n# see https://support.bioconductor.org/p/2323/\n\n\n\n\nShow R code\nwcgs_resid_plot1 |&gt; ggplotly()\n\n\n\n\n\n\n\n\n\nPearson chi-squared goodness of fit test\nThe Pearson chi-squared goodness of fit statistic is: \\[\nX^2 = \\sum_{k=1}^m X_k^2\n\\] Under the null hypothesis that the model in question is correct (i.e., sufficiently complex), \\(X^2\\ \\dot \\sim\\ \\chi^2(N-p)\\).\n\n\nShow R code\nX = chd_glm_strat_grouped |&gt; \n  resid(type = \"pearson\")\n\nchisq_stat = sum(X^2)\n\npval = pchisq(\n  chisq_stat, \n  lower = FALSE, \n  df = length(X) - length(coef(chd_glm_strat_grouped)))\n\n\nFor our CHD model, the p-value for this test is 0.26523556; no significant evidence of a lack of fit at the 0.05 level.\n\nStandardized Pearson residuals\nEspecially for small data sets, we might want to adjust our residuals for leverage (since outliers in \\(X\\) add extra variance to the residuals):\n\\[r_{P_k} = \\frac{X_k}{\\sqrt{1-h_k}}\\]\nwhere \\(h_k\\) is the leverage of \\(X_k\\). The functions autoplot() and plot.lm() use these for some of their graphs.\n\n\n\nDeviance residuals\nFor large sample sizes, the Pearson and deviance residuals will be approximately the same. For small sample sizes, the deviance residuals from covariate patterns with small sample sizes can be unreliable (high variance).\n\\[d_k = \\text{sign}(y_k - n_k \\hat \\pi_k)\\left\\{\\sqrt{2[\\ell_{\\text{full}}(x_k) - \\ell(\\hat\\beta; x_k)]}\\right\\}\\]\n\nStandardized deviance residuals\n\\[r_{D_k} = \\frac{d_k}{\\sqrt{1-h_k}}\\]\n\n\n\nDiagnostic plots\nLet’s take a look at the full set of autoplot() diagnostics now for our CHD model:\n\n\nShow R code\nchd_glm_strat_grouped |&gt; autoplot(which = 1:6) |&gt; print()\n\n\n\n\n\n\n\n\n\nThings look pretty good here. The QQ plot is still usable; with large samples; the residuals should be approximately Gaussian.\n\nBeetles\nLet’s look at the beetles model diagnostic plots for comparison:\n\n\nShow R code\nbeetles_glm_grouped |&gt; autoplot(which = 1:6) |&gt; print()\n\n\n\n\n\n\n\n\n\nHard to tell much from so little data, but there might be some issues here.\n\n\n\n\n3.0.8 Odds Ratios vs Probability (Risk) Ratios\n\nCase 1: rare events\nFor rare events, odds ratios and probability (a.k.a. risk, a.k.a. prevalence) ratios will be close:\n\\(\\pi_1 = .01\\) \\(\\pi_2 = .02\\)\n\n\nShow R code\npi1 = .01\npi2 = .02\npi2/pi1\n\n\n[1] 2\n\n\nShow R code\nodds(pi2)/odds(pi1)\n\n\n[1] 2.0204082\n\n\n\n\nCase 2: frequent events\n\\(\\pi_1 = .4\\) \\(\\pi_2 = .5\\)\nFor more frequently-occurring outcomes, this won’t be the case:\n\n\nShow R code\npi1 = .4\npi2 = .5\npi2/pi1\n\n\n[1] 1.25\n\n\nShow R code\nodds(pi2)/odds(pi1)\n\n\n[1] 1.5\n\n\nIf you want risk ratios, you can sometimes get them by changing the link function:\n\n\nShow R code\ndata(anthers, package = \"dobson\")\nanthers.sum&lt;-aggregate(\n  anthers[c(\"n\",\"y\")], \n  by=anthers[c(\"storage\")],FUN=sum) \n\nanthers_glm_log = glm(\n  formula = cbind(y,n-y)~storage,\n  data=anthers.sum, \n  family=binomial(link=\"log\"))\n\nanthers_glm_log |&gt; parameters() |&gt; print_md()\n\n\n\n\n\nParameter\nLog-Risk\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n-0.80\n0.12\n(-1.04, -0.58)\n-6.81\n&lt; .001\n\n\nstorage\n0.17\n0.07\n(0.02, 0.31)\n2.31\n0.021\n\n\n\n\n\nNow \\(\\text{exp}\\left\\{\\beta\\right\\}\\) gives us risk ratios instead of odds ratios:\n\n\nShow R code\nanthers_glm_log |&gt; parameters(exponentiate = TRUE) |&gt; print_md()\n\n\n\n\n\nParameter\nRisk Ratio\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n0.45\n0.05\n(0.35, 0.56)\n-6.81\n&lt; .001\n\n\nstorage\n1.18\n0.09\n(1.03, 1.36)\n2.31\n0.021\n\n\n\n\n\nLet’s compare this model with a logistic model:\n\n\nShow R code\nanthers_glm_logit = glm(\n  formula = cbind(y, n - y) ~ storage,\n  data = anthers.sum,\n  family = binomial(link = \"logit\"))\n\nanthers_glm_logit |&gt; parameters(exponentiate = TRUE) |&gt; print_md()\n\n\n\n\n\nParameter\nOdds Ratio\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n0.76\n0.20\n(0.45, 1.27)\n-1.05\n0.296\n\n\nstorage\n1.49\n0.26\n(1.06, 2.10)\n2.29\n0.022\n\n\n\n\n\n[to add: fitted plots on each outcome scale]\nWhen I try to use link =\"log\" in practice, I often get errors about not finding good starting values for the estimation procedure. This is likely because the model is producing fitted probabilities greater than 1.\nWhen this happens, you can try to fit Poisson regression models instead (we will see those soon!). But then the outcome distribution isn’t quite right, and you won’t get warnings about fitted probabilities greater than 1. In my opinion, the Poisson model for binary outcomes is confusing and not very appealing.\n\n\n\n\n\n\nDobson, Annette J, and Adrian G Barnett. 2018. An Introduction to Generalized Linear Models. 4th ed. CRC press. https://doi.org/10.1201/9781315182780.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. 2nd ed. Springer. https://doi.org/10.1007/978-1-4614-1353-0.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models for Binary Outcomes (Logistic regression and variations)</span>"
    ]
  },
  {
    "objectID": "count-regression.html",
    "href": "count-regression.html",
    "title": "\n4  Models for Count Outcomes (Poisson regression and variations)\n",
    "section": "",
    "text": "Acknowledgements\nThis content is adapted from:\n\n\nDobson and Barnett (2018), Chapter 9\n\nVittinghoff et al. (2012), Chapter 8\nConfiguring R\nFunctions from these packages will be used throughout this document:\n\nShow R codelibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(conflicted) # check for conflicting function definitions\n\n\nHere are some R settings I use in this document: ::: {.cell}\nShow R coderm(list = ls()) # delete any data that's already loaded into R\nknitr::opts_chunk$set(message = FALSE)\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\noptions('digits' = 4)\n:::\n\n4.0.1 Introduction\nExamples of count outcomes\n\nCyclones per season\nSeconds of tooth-brushing per session (if rounded)\nInfections per person-year\nVisits to ER per person-month\nCar accidents per 1000 miles driven\n\n\n\n\n\n\n\nNote\n\n\n\nIn many count outcomes, there is some sense of “exposure magnitude” or “duration of observation”: person-year, time at risk, session, miles driven, etc.\n\n\nPoisson distribution\n\\[P(Y=y) = \\frac{\\mu^y e^{-\\mu}}{y!}\\]\nProperties\n\n\\(\\mathbb{E}[Y] = \\mu\\)\n\\(\\text{Var}[Y] = \\mu\\)\nAccounting for exposure\nIf the exposures/observation durations, denoted \\(T=t\\), are not all equal, we model \\[\\mu = \\lambda t\\]\n\\(\\lambda\\) is interpreted as the “expected event rate per unit of exposure”; that is,\n\\[\\lambda = \\frac{\\mathbb E[Y|T=t]}{t}\\]\n\n\n\n\n\n\nImportant\n\n\n\nThe exposure magnitude, \\(T\\), is similar to a covariate in linear or logistic regression. However, there is an important difference: in count regression, there is no intercept corresponding to \\(\\mathbb E[Y|T=0]\\). In other words, this model assumes that if there is no exposure, there can’t be any events.\n\n\nAdding covariates\nWith covariates, \\(\\lambda\\) becomes a function of the covariates \\(\\tilde X = (X_1, \\dots,X_n)\\), with a \\(\\text{log}\\left\\{\\right\\}\\) link function (and thus an \\(\\text{exp}\\left\\{\\right\\}\\) inverse-link). That is:\n\\[\n\\begin{aligned}\n\\mathbb E[Y | \\tilde X = \\tilde x,T=t]\n&= \\mu(\\tilde x,t)\\\\\n\\mu(\\tilde x,t)\n&= \\lambda(\\tilde x)\\cdot t\\\\\n\\lambda(\\tilde x)\n&= \\text{exp}\\left\\{\\eta(\\tilde x)\\right\\}\\\\\n\\eta(\\tilde x)\n&= \\tilde x'\\tilde \\beta = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\end{aligned}\n\\]\nTherefore, \\[\n\\begin{aligned}\n\\text{log}\\left\\{\\{\\mathbb E[Y | \\tilde X = \\tilde x,T=t] \\}\\right\\}\n&= \\text{log}\\left\\{\\{\\mu(\\tilde x)\\}\\right\\}\\\\\n&=\\text{log}\\left\\{\\{\\lambda(\\tilde x) \\cdot t \\}\\right\\}\\\\\n&=\\text{log}\\left\\{\\lambda(\\tilde x)\\right\\} + \\text{log}\\left\\{t\\right\\}\\\\\n&=\\text{log}\\left\\{\\text{exp}\\left\\{\\eta(\\tilde x)\\right\\}\\right\\} + \\text{log}\\left\\{t\\right\\}\\\\\n&=\\eta(\\tilde x) + \\text{log}\\left\\{t\\right\\}\\\\\n&=\\tilde x'\\tilde\\beta + \\text{log}\\left\\{t\\right\\}\\\\\n&=(\\beta_0 +\\beta_1 x_1+\\dots + \\beta_p x_p) + \\text{log}\\left\\{t\\right\\}\\\\\n\\end{aligned}\n\\]\nIn contrast with the \\(X\\)s, \\(T\\) enters this expression with a \\(\\text{log}\\left\\{\\right\\}\\) transformation and without a corresponding \\(\\beta\\) coefficient.\n\n\n\n\n\n\nNote\n\n\n\nTerms that enter the linear component of a model without a coefficient, such as \\(\\text{log}\\left\\{t\\right\\}\\) here, are called offsets.\n\n\nRate ratios\nDifferences on the log-rate scale become ratios on the rate scale.\n\n\n\n\n\n\nTip\n\n\n\n\\[\\text{exp}\\left\\{a-b\\right\\} = \\frac{\\text{exp}\\left\\{a\\right\\}}{\\text{exp}\\left\\{b\\right\\}}\\]\n\n\nTherefore, according to this model, differences of \\(\\delta\\) in covariate \\(x_j\\) correspond to rate ratios of \\(\\text{exp}\\left\\{\\beta_j \\cdot\\delta\\right\\}\\).\nThat is, letting \\(\\tilde X_{-j}\\) denote vector \\(\\tilde X\\) with element \\(j\\) removed:\n\\[\n\\begin{aligned}\n&{\n\\left\\{\n    \\text{log}\\left\\{\\mathbb E[Y |{\\color{red}{X_j = a}}, \\tilde X_{-j}=\\tilde x_{-j},T=t]\\right\\}\n    \\atop\n    {-\\text{log}\\left\\{\\mathbb E[Y |{\\color{red}{X_j = b}}, \\tilde X_{-j}=\\tilde x_{-j},T=t]\\right\\}}\n    \\right\\}\n}\\\\\n&=\n{\\left\\{\n\\text{log}\\left\\{t\\right\\} + \\beta_0 + \\beta_1 x_1 + ... + {\\color{red}{\\beta_j (a)}} + ...+\\beta_p x_p\n\\atop\n{-\\text{log}\\left\\{t\\right\\} + \\beta_0 + \\beta_1 x_1 + ... + {\\color{red}{\\beta_j (b)}} + ...+\\beta_p x_p}\n\\right\\}}\\\\\n&= \\color{red}{\\beta_j(a-b)}\n\\end{aligned}\n\\]\nAnd accordingly,\n\\[\n\\begin{aligned}\n\\frac\n{\\mathbb{E}[Y |{\\color{red}{X_j = a}}, \\tilde X_{-j} = \\tilde x_{-j}, T = t]\n}\n{\n\\mathbb E[Y |{\\color{red}{X_j = b}}, \\tilde X_{-j}=\\tilde x_{-j},T=t]\n}\n=\n\\text{exp}\\left\\{{\\color{red}{\\beta_j(a-b)}}\\right\\}\n\\end{aligned}\n\\]\n\n4.0.2 Inference for count regression models\nConfidence intervals for regression coefficients and rate ratios\nAs usual:\n\\[\n\\beta \\in \\left[\\hat\\beta{\\color{red}\\pm} z_{1 - \\frac{\\alpha}{2}}\\cdot\\hat{\\text{se}}\\left(\\hat\\beta\\right)\\right]\n\\]\nRate ratios: exponentiate CI endpoints\n\\[\n\\text{exp}\\left\\{\\beta\\right\\} \\in \\left[\\text{exp}\\left\\{\\hat\\beta{\\color{red}\\pm} z_{1 - \\frac{\\alpha}{2}}\\cdot\\hat{\\text{se}}\\left(\\hat\\beta\\right)\\right\\} \\right]\n\\]\nHypothesis tests for regression coefficients\n\\[\nt = \\frac{\\hat \\beta - \\beta_0}{\\hat{\\text{se}}\\left(\\hat\\beta\\right)}\n\\]\nCompare \\(t\\) or \\(|t|\\) to the tails of the standard Gaussian distribution, according to the null hypothesis.\nComparing nested models\nlog(likelihood ratio) tests, as usual.\n\n4.0.3 Prediction\n\\[\n\\begin{aligned}\n\\hat y\n&\\stackrel{\\text{def}}{=}\\hat{\\mathbb E}[Y|\\tilde X= \\tilde x,T=t]\\\\\n&=\\hat\\mu(\\tilde x, t)\\\\\n&=\\hat\\lambda(\\tilde x) \\cdot t\\\\\n&=\\text{exp}\\left\\{\\hat\\eta(\\tilde x)\\right\\} \\cdot t\\\\\n&=\\text{exp}\\left\\{\\tilde x'\\hat{\\boldsymbol{\\beta}}\\right\\} \\cdot t\n\\end{aligned}\n\\]\n\n4.0.4 Diagnostics\nResiduals\nObservation residuals\n\\[e \\stackrel{\\text{def}}{=}y - \\hat y\\]\nPearson residuals\n\\[r = \\frac{e}{\\hat{\\text{se}}\\left(e\\right)} \\approx \\frac{e}{\\sqrt{\\hat y}}\\]\nStandardized Pearson residuals\n\\[r_p = \\frac{r}{\\sqrt{1-h}}\\] where \\(h\\) is the “leverage” (which we will continue to leave undefined).\nDeviance residuals\n\\[\nd_k = \\text{sign}(y - \\hat y)\\left\\{\\sqrt{2[\\ell_{\\text{full}}(y) - \\ell(\\hat\\beta; y)]}\\right\\}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\\[\\text{sign}(x) \\stackrel{\\text{def}}{=}\\frac{x}{|x|}\\] In other words:\n\n\n\\(\\text{sign}(x) = -1\\) if \\(x &lt; 0\\)\n\n\n\\(\\text{sign}(x) = 0\\) if \\(x = 0\\)\n\n\n\\(\\text{sign}(x) = 1\\) if \\(x &gt; 0\\)\n\n\n\n\n\n4.0.5 Zero-inflation\nModels for zero-inflated counts\nWe assume a latent (unobserved) binary variable, \\(Z\\), which we model using logistic regression:\n\\[P(Z=1|X=x) = \\pi(x) = \\text{expit}(\\gamma_0 + \\gamma_1x_1 +...)\\]\nAccording to this model, if \\(Z=1\\), then \\(Y\\) will always be zero, regardless of \\(X\\) and \\(T\\):\n\\[P(Y=0|Z=1,X=x,T=t) = 1\\]\nOtherwise (if \\(Z=0\\)), \\(Y\\) will have a Poisson distribution, conditional on \\(X\\) and \\(T\\), as above.\nEven though we never observe \\(Z\\), we can estimate the parameters \\(\\gamma_0\\)-\\(\\gamma_p\\), via maximum likelihood:\n\\[\n\\begin{aligned}\nP(Y=y|X=x,T=t) &= P(Y=y,Z=1|...) + P(Y=y,Z=0|...)\n\\end{aligned}\n\\] (by the Law of Total Probability)\nwhere \\[\n\\begin{aligned}\nP(Y=y,Z=z|...)\n&= P(Y=y|Z=z,...)P(Z=z|...)\n\\end{aligned}\n\\]\nExercise\nExpand \\(P(Y=0|X=x,T=t)\\), \\(P(Y=1|X=x,T=t)\\) and \\(P(Y=y|X=x,T=t)\\) into expressions involving \\(P(Z=1|X=x,T=t)\\) and \\(P(Y=y|Z=0,X=x,T=t)\\).\nExercise\nDerive the expected value and variance of \\(Y\\), conditional on \\(X\\) and \\(T\\), as functions of \\(P(Z=1|X=x,T=t)\\) and \\(\\mathbb E[Y|Z=0,X=x,T=t]\\).\n\n4.0.6 Over-dispersion\nNegative binomial models\nThe Poisson distribution model forces the variance to equal the mean. In practice, many count distributions will have a variance substantially larger than the mean (or occasionally smaller).\nWhen we encounter this, we can try to reduce the residual variance by adding more covariates. However, there are also alternatives to the Poisson model.\nMost notably, the negative binomial model:\n\\[P(Y=y) = \\frac{\\mu^y}{y!} \\cdot \\frac{\\Gamma(\\rho + y)}{\\Gamma(\\rho) \\cdot (\\rho + \\mu)^y} \\cdot \\left(1+\\frac{\\mu}{\\rho}\\right)^{-\\rho}\\]\nwhere \\(\\rho\\) is an overdispersion parameter and \\(\\Gamma(x) = (x-1)!\\) for integers \\(x\\).\nYou don’t need to memorize or understand this expression, but as \\(\\rho \\rightarrow \\infty\\), the second term converges to 1 and the third term converges to \\(\\text{exp}\\left\\{-\\mu\\right\\}\\), which brings us back to the Poisson distribution.\nFor this distribution, \\(\\mathbb E[Y] = \\mu\\) and \\(\\text{Var}(Y) = \\mu + \\frac{\\mu^2}{\\rho} &gt; \\mu\\).\nWe can still model \\(\\mu\\) as a function of \\(X\\) and \\(T\\) as before, and we can combine this model with zero-inflation by using it in place of the Poisson distribution for \\(P(Y=y|Z=0,X=x,T=t)\\).\nQuasipoisson\nAn alternative to Negative binomial is the “quasipoisson” distribution. I’ve never used it, but it seems to be a method-of-moments type approach rather than maximum likelihood. It models the variance as \\(\\text{Var}(Y) = \\mu\\theta\\), and estimates \\(\\theta\\) accordingly.\nSee ?quasipoisson in R for more.\n\n\n\n\n\n\nDobson, Annette J, and Adrian G Barnett. 2018. An Introduction to Generalized Linear Models. 4th ed. CRC press. https://doi.org/10.1201/9781315182780.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. 2nd ed. Springer. https://doi.org/10.1007/978-1-4614-1353-0.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models for Count Outcomes (Poisson regression and variations)</span>"
    ]
  },
  {
    "objectID": "time-to-event-models.html#footnotes",
    "href": "time-to-event-models.html#footnotes",
    "title": "Time to Event Models",
    "section": "",
    "text": "Binary outcomes are typically defined for a specific time-point. It is important to clearly define whether we are interested in outcome status at end of study, at end of life, or at some other time.↩︎",
    "crumbs": [
      "Time to Event Models"
    ]
  },
  {
    "objectID": "intro-to-survival-analysis.html",
    "href": "intro-to-survival-analysis.html",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "",
    "text": "Configuring R\nFunctions from these packages will be used throughout this document:\nShow R codelibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(scales) # scales formatting\nlibrary(dplyr) # manipulate data\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(conflicted) # check for conflicting function definitions\nHere are some R settings I use in this document:\nShow R coderm(list = ls()) # delete any data that's already loaded into R\nknitr::opts_chunk$set(message = FALSE)\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\npander::panderOptions(\"table.split.table\", Inf)\nconflicts_prefer(dplyr::filter) # use the `filter()` function from dplyr() by default\noptions('digits' = 4)",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Survival Analysis</span>"
    ]
  },
  {
    "objectID": "intro-to-survival-analysis.html#summary.survfit",
    "href": "intro-to-survival-analysis.html#summary.survfit",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.1 Summary of a Survival Curve",
    "text": "5.1 Summary of a Survival Curve\n\n5.1.1 Description\nReturns a list containing the survival curve, confidence limits for the curve, and other information.\n\n5.1.2 Usage\n  ## S3 method for class 'survfit'\nsummary(object, times, censored=FALSE, scale=1,\n  extend=FALSE, rmean=getOption('survfit.rmean'), ...)\n  \n\n5.1.3 Arguments\n\n\nobject\nthe result of a call to the survfit function.\n\n\ntimes\nvector of times; the returned matrix will contain 1 row for each time. The vector will be sorted into increasing order; missing values are not allowed. If censored=T, the default times vector contains all the unique times in fit, otherwise the default times vector uses only the event (death) times.\n\n\ncensored\nlogical value: should the censoring times be included in the output? This is ignored if the times argument is present.\n\n\nscale\nnumeric value to rescale the survival time, e.g., if the input data to survfit were in days, scale = 365.25 would scale the output to years.\n\n\nextend\nlogical value: if TRUE, prints information for all specified times, even if there are no subjects left at the end of the specified times. This is only used if the times argument is present.\n\n\nrmean\nShow restricted mean: see print.survfit for details\n\n\n...\nfor future methods",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Survival Analysis</span>"
    ]
  },
  {
    "objectID": "proportional-hazards-models.html",
    "href": "proportional-hazards-models.html",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "",
    "text": "Configuring R\nFunctions from these packages will be used throughout this document:\n\nShow R codelibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(conflicted) # check for conflicting function definitions\nconflicts_prefer(dplyr::filter)\n\n\nHere are some R settings I use in this document:\n\nShow R coderm(list = ls()) # delete any data that's already loaded into R\nknitr::opts_chunk$set(message = FALSE)\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\noptions('digits' = 4)\n\n\n\n6.0.1 The proportional hazards model\nBackground on the Proportional Hazards Model\nThe exponential distribution has constant hazard:\n\\[\n\\begin{aligned}\nf(t) &= \\lambda e^{-\\lambda t}\\\\\nS(t) &= e^{-\\lambda t}\\\\\nh(t) &= \\lambda\n\\end{aligned}\n\\]\nLet’s make two generalizations. First, we let the hazard depend on some covariates \\(x_1,x_2, \\dots, x_p\\); we will indicate this dependence by extending our notation for hazard:\n\\[h(t|\\boldsymbol x) \\stackrel{\\text{def}}{=}p(T=t|T\\ge t, \\boldsymbol X = \\boldsymbol x)\\]\nSecond, we let the base hazard depend on \\(t\\), but not on the covariates (for now). We can do this using either parametric or semi-parametric approaches.\nCox’s Proportional Hazards Model\nThe generalization is that the hazard function is\n\\[\n\\begin{aligned}\nh(t|x)&= h_0(t)\\theta(x)\\\\\n\\theta(x) &= \\text{exp}\\left\\{\\eta(x)\\right\\}\\\\\n\\eta(x) &= x'\\beta\\\\\n&\\stackrel{\\text{def}}{=}\\beta_1x_1+\\cdots+\\beta_px_p\n\\end{aligned}\n\\]\nThe relationship between \\(h(t|x)\\) and \\(\\eta(x)\\) has a log link (that is, \\(\\text{log}\\left\\{h(t|x)\\right\\} = \\text{log}\\left\\{h_0(t)\\right\\} + \\eta(x)\\)), as in a generalized linear model.\nThis model is semi-parametric, because the linear predictor depends on estimated parameters but the base hazard function is unspecified. There is no constant term in \\(\\eta(x)\\), because it is absorbed in the base hazard.\nAlternatively, we could define \\(\\beta_0(t) = \\text{log}\\left\\{h_0(t)\\right\\}\\), and then \\(\\eta(x,t) = \\beta_0(t) + \\beta_1x_1+\\cdots+\\beta_px_p\\).\nFor two different individuals with covariate patterns \\(\\boldsymbol x_1\\) and \\(\\boldsymbol x_2\\), the ratio of the hazard functions (a.k.a. hazard ratio, a.k.a. relative hazard) is:\n\\[\n\\begin{aligned}\n\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\n&=\\frac{h_0(t)\\theta(\\boldsymbol x_1)}{h_0(t)\\theta(\\boldsymbol x_2)}\\\\\n&=\\frac{\\theta(\\boldsymbol x_1)}{\\theta(\\boldsymbol x_2)}\\\\\n\\end{aligned}\n\\]\nUnder the proportional hazards model, this ratio (a.k.a. proportion) does not depend on \\(t\\). This property is a structural limitation of the model; it is called the proportional hazards assumption.\n\nDefinition 6.1 (proportional hazards) A conditional probability distribution \\(p(T|X)\\) has proportional hazards if the hazard ratio \\(h(t|\\boldsymbol x_1)/h(t|\\boldsymbol x_2)\\) does not depend on \\(t\\). Mathematically, it can be written as:\n\\[\n\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\n= \\theta(\\boldsymbol x_1,\\boldsymbol x_2)\n\\]\n\nAs we saw above, Cox’s proportional hazards model has this property, with \\(\\theta(\\boldsymbol x_1,\\boldsymbol x_2) = \\frac{\\theta(\\boldsymbol x_1)}{\\theta(\\boldsymbol x_2)}\\).\n\n\n\n\n\n\nNote\n\n\n\nWe are using two similar notations, \\(\\theta(\\boldsymbol x_1,\\boldsymbol x_2)\\) and \\(\\theta(\\boldsymbol x)\\). We can link these notations if we define \\(\\theta(\\boldsymbol x) \\stackrel{\\text{def}}{=}\\theta(\\boldsymbol x, \\boldsymbol 0)\\) and \\(\\theta(\\boldsymbol 0) = 1\\).\n\n\nIt also has additional notable properties:\n\\[\n\\begin{aligned}\n\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\n&=\\frac{\\theta(\\boldsymbol x_1)}{\\theta(\\boldsymbol x_2)}\\\\\n&=\\frac{\\text{exp}\\left\\{\\eta(\\boldsymbol x_1)\\right\\}}{\\text{exp}\\left\\{\\eta(\\boldsymbol x_2)\\right\\}}\\\\\n&=\\text{exp}\\left\\{\\eta(\\boldsymbol x_1)-\\eta(\\boldsymbol x_2)\\right\\}\\\\\n&=\\text{exp}\\left\\{\\boldsymbol x_1'\\beta-\\boldsymbol x_2'\\beta\\right\\}\\\\\n&=\\text{exp}\\left\\{(\\boldsymbol x_1 - \\boldsymbol x_2)'\\beta\\right\\}\\\\\n\\end{aligned}\n\\]\nHence on the log scale, we have:\n\\[\n\\begin{aligned}\n\\text{log}\\left\\{\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\\right\\}\n&=\\eta(\\boldsymbol x_1)-\\eta(\\boldsymbol x_2)\\\\\n&= \\boldsymbol x_1'\\beta-\\boldsymbol x_2'\\beta\\\\\n&= (\\boldsymbol x_1 - \\boldsymbol x_2)'\\beta\n\\end{aligned}\n\\]\nIf only one covariate \\(x_j\\) is changing, then:\n\\[\n\\begin{aligned}\n\\text{log}\\left\\{\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\\right\\}\n&=  (x_{1j} - x_{2j}) \\cdot \\beta_j\\\\\n&\\propto (x_{1j} - x_{2j})\n\\end{aligned}\n\\]\nThat is, under Cox’s model \\(h(t|\\boldsymbol x) = h_0(t)\\text{exp}\\left\\{\\boldsymbol x'\\beta\\right\\}\\), the log of the hazard ratio is proportional to the difference in \\(x_j\\), with the proportionality coefficient equal to \\(\\beta_j\\).\nFurther,\n\\[\n\\begin{aligned}\n\\text{log}\\left\\{h(t|\\boldsymbol x)\\right\\}\n&=\\text{log}\\left\\{h_0(t)\\right\\}  + x'\\beta\n\\end{aligned}\n\\]\nThat is, the covariate effects are additive on the log-hazard scale.\nSee also:\nhttps://en.wikipedia.org/wiki/Proportional_hazards_model#Why_it_is_called_%22proportional%22\nAdditional properties of the proportional hazards model\nIf \\(h(t|x)= h_0(t)\\theta(x)\\), then:\nCumulative hazards are also proportional to \\(H_0(t)\\)\n\n\\[\n\\begin{aligned}\nH(t|x)\n&\\stackrel{\\text{def}}{=}\\int_{u=0}^t h(u)du\\\\\n&= \\int_{u=0}^t h_0(u)\\theta(x)du\\\\\n&= \\theta(x)\\int_{u=0}^t h_0(u)du\\\\\n&= \\theta(x)H_0(t)\n\\end{aligned}\n\\]\nwhere \\(H_0(t) \\stackrel{\\text{def}}{=}H(t|0) = \\int_{u=0}^t h_0(u)du\\).\nSurvival functions are exponential multiples of \\(S_0(t)\\)\n\n\\[\n\\begin{aligned}\nS(t|x)\n&= \\text{exp}\\left\\{-H(t|x)\\right\\}\\\\\n&= \\text{exp}\\left\\{-\\theta(x)\\cdot H_0(t)\\right\\}\\\\\n&= \\left(\\text{exp}\\left\\{- H_0(t)\\right\\}\\right)^{\\theta(x)}\\\\\n&= \\left(S_0(t)\\right)^{\\theta(x)}\\\\\n\\end{aligned}\n\\]\nwhere \\(S_0(t) \\stackrel{\\text{def}}{=}P(T\\ge t | \\boldsymbol X = 0)\\) is the survival function for an individual whose covariates are all equal to their default values.\nTesting the proportional hazards assumption\nThe Nelson-Aalen estimate of the cumulative hazard is usually used for estimates of the hazard and often the cumulative hazard.\nIf the hazards of the three groups are proportional, that means that the ratio of the hazards is constant over \\(t\\). We can test this using the ratios of the estimated cumulative hazards, which also would be proportional, as shown above.\n\nShow R codelibrary(KMsurv)\nlibrary(survival)\ndata(bmt)\n\nbmt = \n  bmt |&gt; \n  as_tibble() |&gt; \n  mutate(\n    group = \n      group |&gt; \n      factor(\n        labels = c(\"ALL\",\"Low Risk AML\",\"High Risk AML\")))\n\nnafit = survfit(\n  formula = Surv(t2,d3) ~ group,\n  type = \"fleming-harrington\",\n  data = bmt)\n\nbmt_curves = tibble(timevec = 1:1000)\nsf1 &lt;- with(nafit[1], stepfun(time,c(1,surv)))\nsf2 &lt;- with(nafit[2], stepfun(time,c(1,surv)))\nsf3 &lt;- with(nafit[3], stepfun(time,c(1,surv)))\n\nbmt_curves = \n  bmt_curves |&gt; \n  mutate(\n    cumhaz1 = -log(sf1(timevec)),\n    cumhaz2 = -log(sf2(timevec)),\n    cumhaz3 = -log(sf3(timevec)))\n\n\n\nShow R codelibrary(ggplot2)\nbmt_rel_hazard_plot = \n  bmt_curves |&gt; \n  ggplot(\n    aes(\n      x = timevec,\n      y = cumhaz1/cumhaz2)\n  ) +\n  geom_line(aes(col = \"ALL/Low Risk AML\")) + \n  ylab(\"Hazard Ratio\") +\n  xlab(\"Time\") + \n  ylim(0,6) +\n  geom_line(aes(y = cumhaz3/cumhaz1, col = \"High Risk AML/ALL\")) +\n  geom_line(aes(y = cumhaz3/cumhaz2, col = \"High Risk AML/Low Risk AML\")) +\n  theme_bw() +\n  labs(colour = \"Comparison\") +\n  theme(legend.position=\"bottom\")\n\nprint(bmt_rel_hazard_plot)\n\n\nHazard Ratios by Disease Group\n\n\n\n\nWe can zoom in on 30-300 days to take a closer look:\n\nShow R codebmt_rel_hazard_plot + xlim(c(30,300))\n\n\nHazard Ratios by Disease Group (30-300 Days)\n\n\n\n\nSmoothed hazard functions\nThe Nelson-Aalen estimate of the cumulative hazard is usually used for estimates of the hazard. Since the hazard is the derivative of the cumulative hazard, we need a smooth estimate of the cumulative hazard, which is provided by smoothing the step-function cumulative hazard.\nThe R package muhaz handles this for us. What we are looking for is whether the hazard function is more or less the same shape, increasing, decreasing, constant, etc. Are the hazards “proportional”?\n\nShow R codeplot(\n  survfit(Surv(t2,d3)~group,data=bmt),\n  col=1:3,\n  lwd=2,\n  fun=\"cumhaz\",\n  mark.time = TRUE)\nlegend(\"bottomright\",c(\"ALL\",\"Low Risk AML\",\"High Risk AML\"),col=1:3,lwd=2)\n\n\nDisease-Free Cumulative Hazard by Disease Group\n\n\n\n\n\nShow R codelibrary(muhaz)\n\nmuhaz(bmt$t2,bmt$d3,bmt$group==\"High Risk AML\") |&gt; plot(lwd=2,col=3)\nmuhaz(bmt$t2,bmt$d3,bmt$group==\"ALL\") |&gt; lines(lwd=2,col=1)\nmuhaz(bmt$t2,bmt$d3,bmt$group==\"Low Risk AML\") |&gt; lines(lwd=2,col=2)\nlegend(\"topright\",c(\"ALL\",\"Low Risk AML\",\"High Risk AML\"),col=1:3,lwd=2)\n\n\nSmoothed Hazard Rate Estimates by Disease Group\n\n\n\n\nGroup 3 was plotted first because it has the highest hazard.\nWe will see that except for an initial blip in the high risk AML group, the hazards look roughly proportional . They are all strongly decreasing.\nFitting the Proportional Hazards Model\nHow do we fit a proportional hazards regression model? We need to estimate the coefficients of the covariates, and we need to estimate the base hazard \\(h_0(t)\\). For the covariates, supposing for simplicity that there are no tied event times, let the event times for the whole data set be \\(t_1, t_2,\\ldots,t_D\\). Let the risk set at time \\(t_i\\) be \\(R(t_i)\\) and\n\\[\n\\begin{aligned}\n\\eta(\\boldsymbol{x}) &= \\beta_1x_{1}+\\cdots+\\beta_p x_{p}\\\\\n\\theta(\\boldsymbol{x}) &= e^{\\eta(\\boldsymbol{x})}\\\\\nh(t|X=x)&= h_0(t)e^{\\eta(\\boldsymbol{x})}=\\theta(\\boldsymbol{x}) h_0(t)\n\\end{aligned}\n\\]\nConditional on a single failure at time \\(t\\), the probability that the event is due to subject \\(f\\in R(t)\\) is approximately\n\\[\n\\begin{aligned}\n\\Pr(f \\text{ fails}|\\text{1 failure at } t)\n&= \\frac{h_0(t)e^{\\eta(\\boldsymbol{x}_f)}}{\\sum_{k \\in R(t)}h_0(t)e^{\\eta(\\boldsymbol{x}_f)}}\\\\\n&=\\frac{\\theta(\\boldsymbol{x}_f)}{\\sum_{k \\in R(t)} \\theta(\\boldsymbol{x}_k)}\n\\end{aligned}\n\\]\nThe logic behind this has several steps. We first fix (ex post) the failure times and note that in this discrete context, the probability \\(p_j\\) that a subject \\(j\\) in the risk set fails at time \\(t\\) is just the hazard of that subject at that time.\nIf all of the \\(p_j\\) are small, the chance that exactly one subject fails is\n\\[\n\\sum_{k\\in R(t)}p_k\\left[\\prod_{m\\in R(t), m\\ne k} (1-p_m)\\right]\\approx\\sum_{k\\in R(t)}p_k\n\\]\nIf subject \\(i\\) is the one who experiences the event of interest at time \\(t_i\\), then the partial likelihood is\n\\[\n\\mathcal L^*(\\beta|T)=\n\\prod_i \\frac{\\theta(x_i)}{\\sum_{k \\in R(t_i)} \\theta(\\boldsymbol{x}_k)}\n\\]\nand we can numerically maximize this with respect to the coefficients \\(\\boldsymbol{\\beta}\\) that specify \\(\\eta(\\boldsymbol{x}) = \\boldsymbol{x}'\\boldsymbol{\\beta}\\). When there are tied event times adjustments need to be made, but the likelihood is still similar. Note that we don’t need to know the base hazard to solve for the coefficients.\nOnce we have coefficient estimates \\(\\hat{\\boldsymbol{\\beta}} =(\\hat \\beta_1,\\ldots,\\hat\\beta_p)\\), this also defines \\(\\hat\\eta(x)\\) and \\(\\hat\\theta(x)\\) and then the estimated base cumulative hazard function is \\[\\hat H(t)=\n\\sum_{t_i &lt; t} \\frac{d_i}{\\sum_{k\\in R(t_i)} \\theta(x_k)}\\] which reduces to the Nelson-Aalen estimate when there are no covariates. There are numerous other estimates that have been proposed as well.\n\n6.0.2 Cox Model for the bmt data\nFit the model\n\nShow R codebmt.cox &lt;- coxph(Surv(t2, d3) ~ group, data = bmt)\nsummary(bmt.cox)\n\nCall:\ncoxph(formula = Surv(t2, d3) ~ group, data = bmt)\n\n  n= 137, number of events= 83 \n\n                     coef exp(coef) se(coef)     z Pr(&gt;|z|)  \ngroupLow Risk AML  -0.574     0.563    0.287 -2.00    0.046 *\ngroupHigh Risk AML  0.383     1.467    0.267  1.43    0.152  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                   exp(coef) exp(-coef) lower .95 upper .95\ngroupLow Risk AML      0.563      1.776     0.321     0.989\ngroupHigh Risk AML     1.467      0.682     0.869     2.478\n\nConcordance= 0.625  (se = 0.03 )\nLikelihood ratio test= 13.4  on 2 df,   p=0.001\nWald test            = 13  on 2 df,   p=0.001\nScore (logrank) test = 13.8  on 2 df,   p=0.001\n\n\nThe table provides hypothesis tests comparing groups 2 and 3 to group 1. Group 3 has the highest hazard, so the most significant comparison is not directly shown.\nThe coefficient 0.3834 is on the log-hazard-ratio scale, as in log-risk-ratio. The next column gives the hazard ratio 1.4673, and a hypothesis (Wald) test.\nThe (not shown) group 3 vs. group 2 log hazard ratio is 0.3834 + 0.5742 = 0.9576. The hazard ratio is then exp(0.9576) or 2.605.\nInference on all coefficients and combinations can be constructed using coef(bmt.cox) and vcov(bmt.cox) as with logistic and poisson regression.\nConcordance is agreement of first failure between pairs of subjects and higher predicted risk between those subjects, omitting non-informative pairs.\nThe Rsquare value is Cox and Snell’s pseudo R-squared and is not very useful.\nsummary() prints three tests for whether the model with the group covariate is better than the one without\n\n\nLikelihood ratio test (chi-squared)\n\nWald test (also chi-squared), obtained by adding the squares of the z-scores\n\nScore = log-rank test, as with comparison of survival functions.\n\nThe likelihood ratio test is probably best in smaller samples, followed by the Wald test.\nSurvival Curves from the Cox Model\nWe can take a look at the resulting group-specific curves:\n\nShow R code#| fig-cap: \"Survival Functions for Three Groups by KM and Cox Model\"\n\nkm_fit = survfit(Surv(t2, d3) ~ group, data = as.data.frame(bmt))\n\ncox_fit = survfit(\n  bmt.cox, \n  newdata = \n    data.frame(\n      group = unique(bmt$group), \n      row.names = unique(bmt$group)))\n\nlibrary(survminer)\n\nlist(KM = km_fit, Cox = cox_fit) |&gt; \n  survminer::ggsurvplot(\n    # facet.by = \"group\",\n    legend = \"bottom\", \n    legend.title = \"\",\n    combine = TRUE, \n    fun = 'pct', \n    size = .5,\n    ggtheme = theme_bw(), \n    conf.int = FALSE, \n    censor = FALSE) |&gt; \n  suppressWarnings() # ggsurvplot() throws some warnings that aren't too worrying\n\n\n\n\n\n\n\nWhen we use survfit() with a Cox model, we have to specify the covariate levels we are interested in; the argument newdata should include a data.frame with the same named columns as the predictors in the Cox model and one or more levels of each.\nOtherwise (that is, if the newdata argument is missing), a curve is produced for a single “pseudo” subject with covariate values equal to the means component of the fit.\nThe resulting curve(s) almost never make sense, but the default remains due to an unwarranted attachment to the option shown by some users and by other packages.\nTwo particularly egregious examples are factor variables and interactions. Suppose one were studying interspecies transmission of a virus, and the data set has a factor variable with levels (“pig”, “chicken”) and about equal numbers of observations for each. The “mean” covariate level will be 0.5 – is this a flying pig?\nExamining survfit\n\n\nShow R codesurvfit(Surv(t2, d3)~group,data=bmt)\n\nCall: survfit(formula = Surv(t2, d3) ~ group, data = bmt)\n\n                     n events median 0.95LCL 0.95UCL\ngroup=ALL           38     24    418     194      NA\ngroup=Low Risk AML  54     25   2204     704      NA\ngroup=High Risk AML 45     34    183     115     456\n\n\n\nShow R codesurvfit(Surv(t2, d3)~group,data=bmt) |&gt; summary()\n\nCall: survfit(formula = Surv(t2, d3) ~ group, data = bmt)\n\n                group=ALL \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1     38       1    0.974  0.0260        0.924        1.000\n   55     37       1    0.947  0.0362        0.879        1.000\n   74     36       1    0.921  0.0437        0.839        1.000\n   86     35       1    0.895  0.0498        0.802        0.998\n  104     34       1    0.868  0.0548        0.767        0.983\n  107     33       1    0.842  0.0592        0.734        0.966\n  109     32       1    0.816  0.0629        0.701        0.949\n  110     31       1    0.789  0.0661        0.670        0.930\n  122     30       2    0.737  0.0714        0.609        0.891\n  129     28       1    0.711  0.0736        0.580        0.870\n  172     27       1    0.684  0.0754        0.551        0.849\n  192     26       1    0.658  0.0770        0.523        0.827\n  194     25       1    0.632  0.0783        0.495        0.805\n  230     23       1    0.604  0.0795        0.467        0.782\n  276     22       1    0.577  0.0805        0.439        0.758\n  332     21       1    0.549  0.0812        0.411        0.734\n  383     20       1    0.522  0.0817        0.384        0.709\n  418     19       1    0.494  0.0819        0.357        0.684\n  466     18       1    0.467  0.0818        0.331        0.658\n  487     17       1    0.439  0.0815        0.305        0.632\n  526     16       1    0.412  0.0809        0.280        0.605\n  609     14       1    0.382  0.0803        0.254        0.577\n  662     13       1    0.353  0.0793        0.227        0.548\n\n                group=Low Risk AML \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n   10     54       1    0.981  0.0183        0.946        1.000\n   35     53       1    0.963  0.0257        0.914        1.000\n   48     52       1    0.944  0.0312        0.885        1.000\n   53     51       1    0.926  0.0356        0.859        0.998\n   79     50       1    0.907  0.0394        0.833        0.988\n   80     49       1    0.889  0.0428        0.809        0.977\n  105     48       1    0.870  0.0457        0.785        0.965\n  211     47       1    0.852  0.0483        0.762        0.952\n  219     46       1    0.833  0.0507        0.740        0.939\n  248     45       1    0.815  0.0529        0.718        0.925\n  272     44       1    0.796  0.0548        0.696        0.911\n  288     43       1    0.778  0.0566        0.674        0.897\n  381     42       1    0.759  0.0582        0.653        0.882\n  390     41       1    0.741  0.0596        0.633        0.867\n  414     40       1    0.722  0.0610        0.612        0.852\n  421     39       1    0.704  0.0621        0.592        0.837\n  481     38       1    0.685  0.0632        0.572        0.821\n  486     37       1    0.667  0.0642        0.552        0.805\n  606     36       1    0.648  0.0650        0.533        0.789\n  641     35       1    0.630  0.0657        0.513        0.773\n  704     34       1    0.611  0.0663        0.494        0.756\n  748     33       1    0.593  0.0669        0.475        0.739\n 1063     26       1    0.570  0.0681        0.451        0.720\n 1074     25       1    0.547  0.0691        0.427        0.701\n 2204      6       1    0.456  0.1012        0.295        0.704\n\n                group=High Risk AML \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    2     45       1    0.978  0.0220        0.936        1.000\n   16     44       1    0.956  0.0307        0.897        1.000\n   32     43       1    0.933  0.0372        0.863        1.000\n   47     42       2    0.889  0.0468        0.802        0.986\n   48     40       1    0.867  0.0507        0.773        0.972\n   63     39       1    0.844  0.0540        0.745        0.957\n   64     38       1    0.822  0.0570        0.718        0.942\n   74     37       1    0.800  0.0596        0.691        0.926\n   76     36       1    0.778  0.0620        0.665        0.909\n   80     35       1    0.756  0.0641        0.640        0.892\n   84     34       1    0.733  0.0659        0.615        0.875\n   93     33       1    0.711  0.0676        0.590        0.857\n  100     32       1    0.689  0.0690        0.566        0.838\n  105     31       1    0.667  0.0703        0.542        0.820\n  113     30       1    0.644  0.0714        0.519        0.801\n  115     29       1    0.622  0.0723        0.496        0.781\n  120     28       1    0.600  0.0730        0.473        0.762\n  157     27       1    0.578  0.0736        0.450        0.742\n  162     26       1    0.556  0.0741        0.428        0.721\n  164     25       1    0.533  0.0744        0.406        0.701\n  168     24       1    0.511  0.0745        0.384        0.680\n  183     23       1    0.489  0.0745        0.363        0.659\n  242     22       1    0.467  0.0744        0.341        0.638\n  268     21       1    0.444  0.0741        0.321        0.616\n  273     20       1    0.422  0.0736        0.300        0.594\n  318     19       1    0.400  0.0730        0.280        0.572\n  363     18       1    0.378  0.0723        0.260        0.550\n  390     17       1    0.356  0.0714        0.240        0.527\n  422     16       1    0.333  0.0703        0.221        0.504\n  456     15       1    0.311  0.0690        0.201        0.481\n  467     14       1    0.289  0.0676        0.183        0.457\n  625     13       1    0.267  0.0659        0.164        0.433\n  677     12       1    0.244  0.0641        0.146        0.409\n\n\n\nShow R codesurvfit(bmt.cox)\n\nCall: survfit(formula = bmt.cox)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 137     83    422     268      NA\n\nShow R codesurvfit(bmt.cox, newdata = tibble(group = unique(bmt$group)))\n\nCall: survfit(formula = bmt.cox, newdata = tibble(group = unique(bmt$group)))\n\n    n events median 0.95LCL 0.95UCL\n1 137     83    422     268      NA\n2 137     83     NA     625      NA\n3 137     83    268     162     467\n\n\n\nShow R codebmt.cox |&gt; \n  survfit(newdata = tibble(group = unique(bmt$group))) |&gt; \n  summary()\n\nCall: survfit(formula = bmt.cox, newdata = tibble(group = unique(bmt$group)))\n\n time n.risk n.event survival1 survival2 survival3\n    1    137       1     0.993     0.996     0.989\n    2    136       1     0.985     0.992     0.978\n   10    135       1     0.978     0.987     0.968\n   16    134       1     0.970     0.983     0.957\n   32    133       1     0.963     0.979     0.946\n   35    132       1     0.955     0.975     0.935\n   47    131       2     0.941     0.966     0.914\n   48    129       2     0.926     0.957     0.893\n   53    127       1     0.918     0.953     0.882\n   55    126       1     0.911     0.949     0.872\n   63    125       1     0.903     0.944     0.861\n   64    124       1     0.896     0.940     0.851\n   74    123       2     0.881     0.931     0.830\n   76    121       1     0.873     0.926     0.819\n   79    120       1     0.865     0.922     0.809\n   80    119       2     0.850     0.913     0.788\n   84    117       1     0.843     0.908     0.778\n   86    116       1     0.835     0.903     0.768\n   93    115       1     0.827     0.899     0.757\n  100    114       1     0.820     0.894     0.747\n  104    113       1     0.812     0.889     0.737\n  105    112       2     0.797     0.880     0.717\n  107    110       1     0.789     0.875     0.707\n  109    109       1     0.782     0.870     0.697\n  110    108       1     0.774     0.866     0.687\n  113    107       1     0.766     0.861     0.677\n  115    106       1     0.759     0.856     0.667\n  120    105       1     0.751     0.851     0.657\n  122    104       2     0.735     0.841     0.637\n  129    102       1     0.727     0.836     0.627\n  157    101       1     0.720     0.831     0.617\n  162    100       1     0.712     0.826     0.607\n  164     99       1     0.704     0.821     0.598\n  168     98       1     0.696     0.815     0.588\n  172     97       1     0.688     0.810     0.578\n  183     96       1     0.680     0.805     0.568\n  192     95       1     0.672     0.800     0.558\n  194     94       1     0.664     0.794     0.549\n  211     93       1     0.656     0.789     0.539\n  219     92       1     0.648     0.783     0.530\n  230     90       1     0.640     0.778     0.520\n  242     89       1     0.632     0.773     0.511\n  248     88       1     0.624     0.767     0.501\n  268     87       1     0.616     0.761     0.492\n  272     86       1     0.608     0.756     0.482\n  273     85       1     0.600     0.750     0.473\n  276     84       1     0.592     0.745     0.464\n  288     83       1     0.584     0.739     0.454\n  318     82       1     0.576     0.733     0.445\n  332     81       1     0.568     0.727     0.436\n  363     80       1     0.560     0.722     0.427\n  381     79       1     0.552     0.716     0.418\n  383     78       1     0.544     0.710     0.409\n  390     77       2     0.528     0.698     0.392\n  414     75       1     0.520     0.692     0.383\n  418     74       1     0.512     0.686     0.374\n  421     73       1     0.504     0.680     0.366\n  422     72       1     0.496     0.674     0.357\n  456     71       1     0.488     0.667     0.349\n  466     70       1     0.480     0.661     0.340\n  467     69       1     0.472     0.655     0.332\n  481     68       1     0.464     0.649     0.324\n  486     67       1     0.455     0.642     0.315\n  487     66       1     0.447     0.636     0.307\n  526     65       1     0.439     0.629     0.299\n  606     63       1     0.431     0.623     0.291\n  609     62       1     0.423     0.616     0.283\n  625     61       1     0.415     0.609     0.275\n  641     60       1     0.407     0.603     0.267\n  662     59       1     0.399     0.596     0.260\n  677     58       1     0.391     0.589     0.252\n  704     57       1     0.383     0.582     0.244\n  748     56       1     0.374     0.575     0.237\n 1063     47       1     0.365     0.567     0.228\n 1074     46       1     0.356     0.559     0.220\n 2204      9       1     0.313     0.520     0.182\n\n\n\n6.0.3 Adjustment for Ties (optional)\n\nAt each time \\(t_i\\) at which more than one of the subjects has an event, let \\(d_i\\) be the number of events at that time, \\(D_i\\) the set of subjects with events at that time, and let \\(s_i\\) be a covariate vector for an artificial subject obtained by adding up the covariate values for the subjects with an event at time \\(t_i\\). Let \\[\\bar\\eta_i = \\beta_1s_{i1}+\\cdots+\\beta_ps_{ip}\\] and \\(\\bar\\theta_i = \\text{exp}\\left\\{\\bar\\eta_i\\right\\}\\).\nLet \\(s_i\\) be a covariate vector for an artificial subject obtained by adding up the covariate values for the subjects with an event at time \\(t_i\\). Note that\n\\[\n\\begin{aligned}\n\\bar\\eta_i &=\\sum_{j \\in D_i}\\beta_1x_{j1}+\\cdots+\\beta_px_{jp}\\\\\n&= \\beta_1s_{i1}+\\cdots+\\beta_ps_{ip}\\\\\n\\bar\\theta_i &= \\text{exp}\\left\\{\\bar\\eta_i\\right\\}\\\\\n&= \\prod_{j \\in D_i}\\theta_i\n\\end{aligned}\n\\]\nBreslow’s method for ties\nBreslow’s method estimates the partial likelihood as\n\\[\n\\begin{aligned}\nL(\\beta|T) &=\n\\prod_i \\frac{\\bar\\theta_i}{[\\sum_{k \\in R(t_i)} \\theta_k]^{d_i}}\\\\\n&= \\prod_i \\prod_{j \\in D_i}\\frac{\\theta_j}{\\sum_{k \\in R(t_i)} \\theta_k}\n\\end{aligned}\n\\]\nThis method is equivalent to treating each event as distinct and using the non-ties formula. It works best when the number of ties is small. It is the default in many statistical packages, including PROC PHREG in SAS.\nEfron’s method for ties\nThe other common method is Efron’s, which is the default in R.\n\\[L(\\beta|T)=\n\\prod_i \\frac{\\bar\\theta_i}{\\prod_{j=1}^{d_i}[\\sum_{k \\in R(t_i)} \\theta_k-\\frac{j-1}{d_i}\\sum_{k \\in D_i} \\theta_k]}\\] This is closer to the exact discrete partial likelihood when there are many ties.\nThe third option in R (and an option also in SAS as discrete) is the “exact” method, which is the same one used for matched logistic regression.\nExample: Breslow’s method\nSuppose as an example we have a time \\(t\\) where there are 20 individuals at risk and three failures. Let the three individuals have risk parameters \\(\\theta_1, \\theta_2, \\theta_3\\) and let the sum of the risk parameters of the remaining 17 individuals be \\(\\theta_R\\). Then the factor in the partial likelihood at time \\(t\\) using Breslow’s method is\n\n\\[\n\\left(\\frac{\\theta_1}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_2}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_3}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\]\n\nIf on the other hand, they had died in the order 1,2, 3, then the contribution to the partial likelihood would be:\n\n\\[\n\\left(\\frac{\\theta_1}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_2}{\\theta_R+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_3}{\\theta_R+\\theta_3}\\right)\n\\]\n\nas the risk set got smaller with each failure. The exact method roughly averages the results for the six possible orderings of the failures.\nExample: Efron’s method\nBut we don’t know the order they failed in, so instead of reducing the denominator by one risk coefficient each time, we reduce it by the same fraction. This is Efron’s method.\n\n\\[\\left(\\frac{\\theta_1}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_2}{\\theta_R+2(\\theta_1+\\theta_2+\\theta_3)/3}\\right)\n\\left(\\frac{\\theta_3}{\\theta_R+(\\theta_1+\\theta_2+\\theta_3)/3}\\right)\\]",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "coxph-model-building.html",
    "href": "coxph-model-building.html",
    "title": "\n7  Building Cox Proportional Hazards models\n",
    "section": "",
    "text": "Configuring R\nFunctions from these packages will be used throughout this document:\n\nShow R codelibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggfortify) # help with graphics\nlibrary(ggeasy) # help with graphics\nlibrary(survival) # survival analysis\nlibrary(survminer) # survival analysis graphics\nlibrary(dplyr) # manipulate data\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(fs) # filesystem path manipulations\nlibrary(KMsurv) # datasets from Klein and Moeschberger\nlibrary(parameters) # format model output tables for markdown\nlibrary(conflicted) # check for conflicting function definitions\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(ggplot2::autoplot)\n\n\nHere are some R settings I use in this document:\n\nShow R coderm(list = ls()) # delete any data that's already loaded into R\nknitr::opts_chunk$set(message = FALSE)\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\noptions('digits' = 4)\nlegend_text_size = 9\n\n\n\n7.0.1 Building Cox Proportional Hazards models\n\nhodg Lymphoma Data Set from KMsurv\n\nParticipants\n43 bone marrow transplant patients at Ohio State University (Avalos 1993)\nVariables\n\n\ndtype: Disease type (Hodgkin’s or non-Hodgkins lymphoma)\n\ngtype: Bone marrow graft type:\nallogeneic: from HLA-matched sibling\nautologous: from self (prior to chemo)\n\ntime: time to study exit\n\ndelta: study exit reason (death/relapse vs censored)\n\nwtime: waiting time to transplant (in months)\n\nscore: Karnofsky score:\n80–100: Able to carry on normal activity and to work; no special care needed.\n50–70: Unable to work; able to live at home and care for most personal needs; varying amount of assistance needed.\n10–60: Unable to care for self; requires equivalent of institutional or hospital care; disease may be progressing rapidly.\nData\n\nShow R codedata(hodg, package = \"KMsurv\")\nhodg2 = hodg |&gt; \n  as_tibble() |&gt; \n  mutate(\n    # We add factor labels to the categorical variables:\n    gtype = gtype |&gt; \n      case_match(\n        1 ~ \"Allogenic\",\n        2 ~ \"Autologous\"),\n    dtype = dtype |&gt; \n      case_match(\n        1 ~ \"Non-Hodgkins\",\n        2 ~ \"Hodgkins\") |&gt; \n      factor() |&gt; \n      relevel(ref = \"Non-Hodgkins\"), \n    delta = delta |&gt; \n      case_match(\n        1 ~ \"dead\",\n        0 ~ \"alive\"),\n    surv = Surv(\n      time = time, \n      event = delta == \"dead\")\n  )\nhodg2 |&gt; print()\n\n# A tibble: 43 × 7\n   gtype     dtype         time delta score wtime   surv\n   &lt;chr&gt;     &lt;fct&gt;        &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;Surv&gt;\n 1 Allogenic Non-Hodgkins    28 dead     90    24    28 \n 2 Allogenic Non-Hodgkins    32 dead     30     7    32 \n 3 Allogenic Non-Hodgkins    49 dead     40     8    49 \n 4 Allogenic Non-Hodgkins    84 dead     60    10    84 \n 5 Allogenic Non-Hodgkins   357 dead     70    42   357 \n 6 Allogenic Non-Hodgkins   933 alive    90     9   933+\n 7 Allogenic Non-Hodgkins  1078 alive   100    16  1078+\n 8 Allogenic Non-Hodgkins  1183 alive    90    16  1183+\n 9 Allogenic Non-Hodgkins  1560 alive    80    20  1560+\n10 Allogenic Non-Hodgkins  2114 alive    80    27  2114+\n# ℹ 33 more rows\n\n\nProportional hazards model\n\nShow R codehodg.cox1 = coxph(\n  formula = surv ~ gtype * dtype + score + wtime, \n  data = hodg2)\n\nsummary(hodg.cox1)\n\nCall:\ncoxph(formula = surv ~ gtype * dtype + score + wtime, data = hodg2)\n\n  n= 43, number of events= 26 \n\n                                 coef exp(coef) se(coef)     z Pr(&gt;|z|)    \ngtypeAutologous                0.6394    1.8953   0.5937  1.08   0.2815    \ndtypeHodgkins                  2.7603   15.8050   0.9474  2.91   0.0036 ** \nscore                         -0.0495    0.9517   0.0124 -3.98  6.8e-05 ***\nwtime                         -0.0166    0.9836   0.0102 -1.62   0.1046    \ngtypeAutologous:dtypeHodgkins -2.3709    0.0934   1.0355 -2.29   0.0220 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                              exp(coef) exp(-coef) lower .95 upper .95\ngtypeAutologous                  1.8953     0.5276    0.5920     6.068\ndtypeHodgkins                   15.8050     0.0633    2.4682   101.207\nscore                            0.9517     1.0507    0.9288     0.975\nwtime                            0.9836     1.0167    0.9641     1.003\ngtypeAutologous:dtypeHodgkins    0.0934    10.7074    0.0123     0.711\n\nConcordance= 0.776  (se = 0.059 )\nLikelihood ratio test= 32.1  on 5 df,   p=6e-06\nWald test            = 27.2  on 5 df,   p=5e-05\nScore (logrank) test = 37.7  on 5 df,   p=4e-07\n\n\n\n7.0.2 Diagnostic graphs for proportional hazards assumption\nAnalysis plan\n\n\nsurvival function for the four combinations of disease type and graft type.\n\nobserved (nonparametric) vs. expected (semiparametric) survival functions.\n\ncomplementary log-log survival for the four groups.\nKaplan-Meier survival functions\n\nShow R codekm_model = survfit(\n  formula = surv ~ dtype + gtype,\n  data = hodg2)\n\nkm_model |&gt; \n  autoplot(conf.int = FALSE) +\n  theme_bw() +\n  theme(\n    legend.position=\"bottom\",\n    legend.title = element_blank(),\n    legend.text = element_text(size = legend_text_size)\n  ) +\n  guides(col=guide_legend(ncol=2)) +\n  ylab('Survival probability, S(t)') +\n  xlab(\"Time since transplant (days)\")\n\n\nKaplan-Meier Survival Curves for HOD/NHL and Allo/Auto Grafts\n\n\n\n\nObserved and expected survival curves\n\nShow R code# we need to create a tibble of covariate patterns;\n# we will set score and wtime to mean values for disease and graft types:\nmeans = hodg2 |&gt; \n  summarize(\n    .by = c(dtype, gtype), \n    score = mean(score), \n    wtime = mean(wtime)) |&gt; \n  arrange(dtype, gtype) |&gt; \n  mutate(strata = paste(dtype, gtype, sep = \",\")) |&gt; \n  as.data.frame() \n\n# survfit.coxph() will use the rownames of its `newdata`\n# argument to label its output:\nrownames(means) = means$strata\n\ncox_model = \n  hodg.cox1 |&gt; \n  survfit(\n    data = hodg2, # ggsurvplot() will need this\n    newdata = means)\n\n\n\nShow R code# I couldn't find a good function to reformat `cox_model` for ggplot, \n# so I made my own:\nstack_surv_ph = function(cox_model)\n{\n  cox_model$surv |&gt; \n    as_tibble() |&gt; \n    mutate(time = cox_model$time) |&gt; \n    pivot_longer(\n      cols = -time,\n      names_to = \"strata\",\n      values_to = \"surv\") |&gt; \n    mutate(\n      cumhaz = -log(surv),\n      model = \"Cox PH\")\n}\n\nkm_and_cph =\n  km_model |&gt; \n  fortify(surv.connect = TRUE) |&gt; \n  mutate(\n    strata = trimws(strata),\n    model = \"Kaplan-Meier\",\n    cumhaz = -log(surv)) |&gt;\n  bind_rows(stack_surv_ph(cox_model))\n\n\n\nShow R codekm_and_cph |&gt; \n  ggplot(aes(x = time, y = surv, col = model)) +\n  geom_step() +\n  facet_wrap(~strata) +\n  theme_bw() + \n  ylab(\"S(t) = P(T&gt;=t)\") +\n  xlab(\"Survival time (t, days)\") +\n  theme(legend.position = \"bottom\")\n\n\nObserved and expected survival curves for bmt data\n\n\n\n\nCumulative hazard (log-scale) curves\nAlso known as “complementary log-log (clog-log) survival curves”.\n\nShow R codena_model = survfit(\n  formula = surv ~ dtype + gtype,\n  data = hodg2,\n  type = \"fleming\")\n\nna_model |&gt; \n  survminer::ggsurvplot(\n  legend = \"bottom\", \n  legend.title = \"\",\n  ylab = \"log(Cumulative Hazard)\",\n  xlab = \"Time since transplant (days, log-scale)\",\n  fun = 'cloglog', \n  size = .5,\n  ggtheme = theme_bw(),\n  conf.int = FALSE, \n  censor = TRUE) |&gt;  \n  magrittr::extract2(\"plot\") +\n  guides(\n    col = \n      guide_legend(\n        ncol = 2,\n        label.theme = \n          element_text(\n            size = legend_text_size)))\n\n\n\nFigure 7.1: Complementary log-log survival curves - Nelson-Aalen estimates\n\n\n\n\n\n\n\nLet’s compare these empirical (i.e., non-parametric) curves with the fitted curves from our coxph() model:\n\nShow R codecox_model |&gt; \n  survminer::ggsurvplot(\n    facet_by = \"\",\n    legend = \"bottom\", \n    legend.title = \"\",\n    ylab = \"log(Cumulative Hazard)\",\n    xlab = \"Time since transplant (days, log-scale)\",\n    fun = 'cloglog', \n    size = .5,\n    ggtheme = theme_bw(),\n    censor = FALSE, # doesn't make sense for cox model\n    conf.int = FALSE) |&gt;  \n  magrittr::extract2(\"plot\") +\n  guides(\n    col = \n      guide_legend(\n        ncol = 2,\n        label.theme = \n          element_text(\n            size = legend_text_size)))\n\n\n\nFigure 7.2: Complementary log-log survival curves - PH estimates\n\n\n\n\n\n\n\nNow let’s overlay these cumulative hazard curves:\n\nShow R codena_and_cph = \n  na_model |&gt; \n  fortify(fun = \"cumhaz\") |&gt; \n  # `fortify.survfit()` doesn't name cumhaz correctly:\n  rename(cumhaz = surv) |&gt;  \n  mutate(\n    surv = exp(-cumhaz),\n    strata = trimws(strata)) |&gt; \n  mutate(model = \"Nelson-Aalen\") |&gt; \n  bind_rows(stack_surv_ph(cox_model))\n\nna_and_cph |&gt; \n  ggplot(\n    aes(\n      x = time, \n      y = cumhaz, \n      col = model)) +\n  geom_step() +\n  facet_wrap(~strata) +\n  theme_bw() + \n  scale_y_continuous(\n    trans = \"log10\",\n    name = \"Cumulative hazard H(t) (log-scale)\") +\n  scale_x_continuous(\n    trans = \"log10\",\n    name = \"Survival time (t, days, log-scale)\") +\n  theme(legend.position = \"bottom\")\n\n\n\nFigure 7.3: Observed and expected cumulative hazard curves for bmt data (cloglog format)\n\n\n\n\n\n\n\n\n7.0.3 Predictions and Residuals\nReview: Predictions in Linear Regression\n\nIn linear regression, we have a linear predictor for each data point \\(i\\)\n\n\n\\[\n\\begin{aligned}\n\\eta_i &= \\beta_0+\\beta_1x_{1i}+\\cdots+\\beta_px_{pi}\\\\\n\\hat y_i &=\\hat\\eta_i = \\hat\\beta_0+\\hat\\beta_1x_{1i}+\\cdots+\\hat\\beta_px_{pi}\\\\\ny_i &\\sim N(\\eta_i,\\sigma^2)\n\\end{aligned}\n\\]\n\n\n\\(\\hat y_i\\) estimates the conditional mean of \\(y_i\\) given the covariate values \\(\\tilde x_i\\). This together with the prediction error says that we are predicting the distribution of values of \\(y\\).\nReview: Residuals in Linear Regression\n\nThe usual residual is \\(r_i=y_i-\\hat y_i\\), the difference between the actual value of \\(y\\) and a prediction of its mean.\nThe residuals are also the quantities the sum of whose squares is being minimized by the least squares/MLE estimation.\nPredictions and Residuals in survival models\n\nIn survival analysis, the equivalent of \\(y_i\\) is the event time \\(t_i\\), which is unknown for the censored observations.\nThe expected event time can be tricky to calculate:\n\n\\[\n\\hat{\\text{E}}[T|X=x] = \\int_{t=0}^{\\infty} \\hat S(t)dt\n\\]\nWide prediction intervals\nThe nature of time-to-event data results in very wide prediction intervals:\n\nSuppose a cancer patient is predicted to have a mean lifetime of 5 years after diagnosis and suppose the distribution is exponential.\nIf we want a 95% interval for survival, the lower end is at the 0.025 percentage point of the exponential which is qexp(.025, rate = 1/5) = 0.12658904 years, or 1/40 of the mean lifetime.\nThe upper end is at the 0.975 point which is qexp(.975, rate = 1/5) = 18.44439727 years, or 3.7 times the mean lifetime.\nSaying that the survival time is somewhere between 6 weeks and 18 years does not seem very useful, but it may be the best we can do.\nFor survival analysis, something is like a residual if it is small when the model is accurate or if the accumulation of them is in some way minimized by the estimation algorithm, but there is no exact equivalence to linear regression residuals.\nAnd if there is, they are mostly quite large!\nTypes of Residuals in Time-to-Event Models\n\nIt is often hard to make a decision from graph appearances, though the process can reveal much.\nSome diagnostic tests are based on residuals as with other regression methods:\n\nSchoenfeld residuals (via cox.zph) for proportionality.\n\nCox-Snell residuals for goodness of fit.\n\nmartingale residuals for non-linearity.\n\ndfbeta for influence.\nSchoenfeld residuals\n\nThere is a Schoenfeld residual for each subject \\(i\\) with an event (not censored) and for each predictor \\(x_{k}\\).\nAt the event time \\(t\\) for that subject, there is a risk set \\(R\\), and each subject \\(j\\) in the risk set has a risk coefficient \\(\\theta_j\\) and also a value \\(x_{jk}\\) of the predictor.\nThe Schoenfeld residual is the difference between \\(x_{ik}\\) and the risk-weighted average of all the \\(x_{jk}\\) over the risk set.\n\n\\[\nr^S_{ik} =\nx_{ik}-\\frac{\\sum_{k\\in R}x_{jk}\\theta_k}{\\sum_{k\\in R}\\theta_k}\n\\]\nThis residual measures how typical the individual subject is with respect to the covariate at the time of the event. Since subjects should fail more or less uniformly according to risk, the Schoenfeld residuals should be approximately level over time, not increasing or decreasing.\nWe can test this with the correlation with time on some scale, which could be the time itself, the log time, or the rank in the set of failure times.\nThe default is to use the KM curve as a transform, which is similar to the rank but deals better with censoring.\nThe cox.zph() function implements a score test proposed in Grambsch and Therneau (1994).\n\nShow R codehodg.zph = cox.zph(hodg.cox1)\nprint(hodg.zph)\n\n               chisq df      p\ngtype        0.53998  1 0.4624\ndtype        1.80125  1 0.1796\nscore        3.88050  1 0.0489\nwtime        0.01728  1 0.8954\ngtype:dtype  4.04744  1 0.0442\nGLOBAL      13.75732  5 0.0172\n\n\ngtype\n\nShow R codeggcoxzph(hodg.zph, var = \"gtype\")\n\n\n\n\n\n\n\ndtype\n\nShow R codeggcoxzph(hodg.zph, var = \"dtype\")\n\n\n\n\n\n\n\nscore\n\nShow R codeggcoxzph(hodg.zph, var = \"score\")\n\n\n\n\n\n\n\nwtime\n\nShow R codeggcoxzph(hodg.zph, var = \"wtime\")\n\n\n\n\n\n\n\ngtype:dtype\n\nShow R codeggcoxzph(hodg.zph, var = \"gtype:dtype\")\n\n\n\n\n\n\n\nConclusions\n\nFrom the correlation test, the Karnofsky score and the interaction with graft type disease type induce modest but statistically significant non-proportionality.\nThe sample size here is relatively small (26 events in 43 subjects). If the sample size is large, very small amounts of non-proportionality can induce a significant result.\nAs time goes on, autologous grafts are over-represented at their own event times, but those from HOD patients become less represented.\nBoth the statistical tests and the plots are useful.\n\n7.0.4 Goodness of Fit using the Cox-Snell Residuals\n(references: Klein & Moeschberger textbook, §11.2, and Dobson & Barnett textbook, §10.6)\nSuppose that an individual has a survival time \\(T\\) which has survival function \\(S(t)\\), meaning that \\(\\Pr(T&gt; t) = S(t)\\). Then \\(S(T)\\) has a uniform distribution on \\((0,1)\\).\n\\[\n\\begin{aligned}\n\\Pr(S(T_i) \\le u)\n&= \\Pr(T_i &gt; S_i^{-1}(u))\\\\\n&= S_i(S_i^{-1}(u))\\\\\n&= u\n\\end{aligned}\n\\]\nAlso, if \\(U\\) has a uniform distribution on \\((0,1)\\), then what is the distribution of \\(-\\ln(U)\\)?\n\\[\n\\begin{aligned}\n\\Pr(-\\ln(U) &lt; x) &= \\Pr(U&gt;\\text{exp}\\left\\{-x\\right\\})\\\\\n&= 1-e^{-x}\n\\end{aligned}\n\\]\nwhich is the CDF of an exponential distribution with parameter \\(\\lambda=1\\).\nSo,\n\\[\n\\begin{aligned}\nr^{CS}_i&\n\\stackrel{\\text{def}}{=}-\\ln[\\hat S(t_i|x_i)]\n= \\hat H(t_i|\\tilde x_i)\n\\end{aligned}\n\\]\nshould have an exponential distribution with constant hazard \\(\\lambda=1\\) if the estimate \\(\\hat S_i\\) is accurate, which means that these values should look like a censored sample from this exponential distribution. These values are called generalized residuals or Cox-Snell residuals.\n\nShow R codehodg2 = hodg2 |&gt; \n  mutate(cs = predict(hodg.cox1, type = \"expected\"))\n\nsurv.csr = survfit(\n  data = hodg2,\n  formula = Surv(time = cs, event = delta == \"dead\") ~ 1,\n  type = \"fleming-harrington\")\n\nautoplot(surv.csr, fun = \"cumhaz\") + \n  geom_abline(aes(intercept = 0, slope = 1), col = \"red\") +\n  theme_bw()\n\n\nCumulative Hazard of Cox-Snell Residuals\n\n\n\n\nThe line with slope 1 and intercept 0 fits the curve relatively well, so we don’t see lack of fit using this procedure.\n\n7.0.5 Martingale Residuals\nThe martingale residuals are a slight modification of the Cox-Snell residuals. If the censoring indicator is \\(\\delta_i\\), then \\[r^M_i=\\delta_i-r^{CS}_i\\] These residuals can be interpreted as an estimate of the excess number of events seen in the data but not predicted by the model. We will use these to examine the functional forms of continuous covariates.\nUsing Martingale Residuals\nMartingale residuals can be used to examine the functional form of a numeric variable.\n\nWe fit the model without that variable and compute the martingale residuals.\nWe then plot these martingale residuals against the values of the variable.\nWe can see curvature, or a possible suggestion that the variable can be discretized.\n\nLet’s use this to examine the score and wtime variables in the wtime data set.\nKarnofsky score\n\nShow R codehodg2 = hodg2 |&gt; \n  mutate(\n    mres = \n      hodg.cox1 |&gt; \n      update(. ~ . - score) |&gt; \n      residuals(type=\"martingale\"))\n\nhodg2 |&gt; \n  ggplot(aes(x = score, y = mres)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", aes(col = \"loess\")) +\n  geom_smooth(method = 'lm', aes(col = \"lm\")) +\n  theme_classic() +\n  xlab(\"Karnofsky Score\") +\n  ylab(\"Martingale Residuals\") +\n  guides(col=guide_legend(title = \"\"))\n\n\nMartingale Residuals vs. Karnofsky Score\n\n\n\n\nThe line is almost straight. It could be some modest transformation of the Karnofsky score would help, but it might not make much difference.\nWaiting time\n\nShow R codehodg2$mres = \n  hodg.cox1 |&gt; \n  update(. ~ . - wtime) |&gt; \n  residuals(type=\"martingale\")\n\nhodg2 |&gt; \n  ggplot(aes(x = wtime, y = mres)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", aes(col = \"loess\")) +\n  geom_smooth(method = 'lm', aes(col = \"lm\")) +\n  theme_classic() +\n  xlab(\"Waiting Time\") +\n  ylab(\"Martingale Residuals\") +\n  guides(col=guide_legend(title = \"\"))\n\n\nMartingale Residuals vs. Waiting Time\n\n\n\n\nThe line could suggest a step function. To see where the drop is, we can look at the largest waiting times and the associated martingale residual.\nThe martingale residuals are all negative for wtime &gt;83 and positive for the next smallest value. A reasonable cut-point is 80 days.\nUpdating the model\nLet’s reformulate the model with dichotomized wtime.\n\nShow R codehodg2 = \n  hodg2 |&gt; \n  mutate(\n    wt2 = cut(\n      wtime,c(0, 80, 200),\n      labels=c(\"short\",\"long\")))\n\nhodg.cox2 =\n  coxph(\n    formula = \n      Surv(time, event = delta == \"dead\") ~ \n      gtype*dtype + score + wt2,\n    data = hodg2)\n\n\n\nShow R codehodg.cox1 |&gt; drop1(test=\"Chisq\")\n\n\nModel summary table with waiting time on continuous scale\n\n\nDf\nAIC\nLRT\nPr(&gt;Chi)\n\n\n\n\nNA\n152.36285\nNA\nNA\n\n\nscore\n1\n167.59939\n17.2365407\n0.0000330\n\n\nwtime\n1\n153.64206\n3.2792145\n0.0701625\n\n\ngtype:dtype\n1\n155.79856\n5.4357070\n0.0197291\n\n\n\n\n\n\nShow R codehodg.cox2 |&gt; drop1(test=\"Chisq\")\n\n\nModel summary table with dichotomized waiting time\n\n\nDf\nAIC\nLRT\nPr(&gt;Chi)\n\n\n\n\nNA\n149.03401\nNA\nNA\n\n\nscore\n1\n168.63825\n21.6042378\n0.00000335\n\n\nwt2\n1\n153.64206\n6.6080505\n0.01015187\n\n\ngtype:dtype\n1\n152.00368\n4.9696702\n0.02579557\n\n\n\n\n\nThe new model has better (lower) AIC.\n\n7.0.6 Checking for Outliers and Influential Observations\nWe will check for outliers using the deviance residuals. The martingale residuals show excess events or the opposite, but highly skewed, with the maximum possible value being 1, but the smallest value can be very large negative. Martingale residuals can detect unexpectedly long-lived patients, but patients who die unexpectedly early show up only in the deviance residual. Influence will be examined using dfbeta in a similar way to linear regression, logistic regression, or Poisson regression.\nDeviance Residuals\n\\[\n\\begin{aligned}\nr_i^D &= \\textrm{sign}(r_i^M)\\sqrt{-2\\left[ r_i^M+\\delta_i\\ln(\\delta_i-r_i^M)  \\right]}\\\\\nr_i^D &= \\textrm{sign}(r_i^M)\\sqrt{-2\\left[ r_i^M+\\delta_i\\ln(r_i^{CS})  \\right]}\n\\end{aligned}\n\\]\nRoughly centered on 0 with approximate standard deviation 1.\n\n\nShow R codehodg.mart = residuals(hodg.cox2,type=\"martingale\")\nhodg.dev = residuals(hodg.cox2,type=\"deviance\")\nhodg.dfb = residuals(hodg.cox2,type=\"dfbeta\")\nhodg.preds = predict(hodg.cox2)                   #linear predictor\n\n\n\nShow R codeplot(hodg.preds,\n     hodg.mart,\n     xlab=\"Linear Predictor\",\n     ylab=\"Martingale Residual\")\n\n\nMartingale Residuals vs. Linear Predictor\n\n\n\n\nThe smallest three martingale residuals in order are observations 1, 29, and 18.\n\nShow R codeplot(hodg.preds,hodg.dev,xlab=\"Linear Predictor\",ylab=\"Deviance Residual\")\n\n\nDeviance Residuals vs. Linear Predictor\n\n\n\n\nThe two largest deviance residuals are observations 1 and 29. Worth examining.\ndfbeta\n\ndfbeta is the approximate change in the coefficient vector if that observation were dropped\ndfbetas is the approximate change in the coefficients, scaled by the standard error for the coefficients.\n\nGraft type\n\nShow R codeplot(hodg.dfb[,1],xlab=\"Observation Order\",ylab=\"dfbeta for Graft Type\")\n\n\ndfbeta Values by Observation Order for Graft Type\n\n\n\n\nThe smallest dfbeta for graft type is observation 1.\nDisease type\n\nShow R codeplot(hodg.dfb[,2],\n     xlab=\"Observation Order\",\n     ylab=\"dfbeta for Disease Type\")\n\n\ndfbeta Values by Observation Order for Disease Type\n\n\n\n\nThe smallest two dfbeta values for disease type are observations 1 and 16.\nKarnofsky score\n\nShow R codeplot(hodg.dfb[,3],\n     xlab=\"Observation Order\",\n     ylab=\"dfbeta for Karnofsky Score\")\n\n\ndfbeta Values by Observation Order for Karnofsky Score\n\n\n\n\nThe two highest dfbeta values for score are observations 1 and 18. The next three are observations 17, 29, and 19. The smallest value is observation 2.\nWaiting time (dichotomized)\n\nShow R codeplot(\n  hodg.dfb[,4],\n  xlab=\"Observation Order\",\n  ylab=\"dfbeta for `Waiting Time &lt; 80`\")\n\n\ndfbeta Values by Observation Order for Waiting Time (dichotomized)\n\n\n\n\nThe two large values of dfbeta for dichotomized waiting time are observations 15 and 16. This may have to do with the discretization of waiting time.\nInteraction: graft type and disease type\n\nShow R codeplot(hodg.dfb[,5],\n     xlab=\"Observation Order\",\n     ylab=\"dfbeta for dtype:gtype\")\n\n\ndfbeta Values by Observation Order for dtype:gtype\n\n\n\n\nThe two largest values are observations 1 and 16. The smallest value is observation 35.\n\n\nTable 7.1: Observations to Examine by Residuals and Influence\n\n\n\nDiagnostic\nObservations to Examine\n\n\n\nMartingale Residuals\n1, 29, 18\n\n\nDeviance Residuals\n1, 29\n\n\nGraft Type Influence\n1\n\n\nDisease Type Influence\n1, 16\n\n\nKarnofsky Score Influence\n1, 18 (17, 29, 19)\n\n\nWaiting Time Influence\n15, 16\n\n\nGraft by Disease Influence\n1, 16, 35\n\n\n\n\n\n\nThe most important observations to examine seem to be 1, 15, 16, 18, and 29.\n\n\nShow R codewith(hodg,summary(time[delta==1]))\n\n\n\nMin.\n1st Qu.\nMedian\nMean\n3rd Qu.\nMax.\n\n\n2\n41.25\n62.5\n97.615385\n83.25\n524\n\n\n\n\n\nShow R codewith(hodg,summary(wtime))\n\n\n\nMin.\n1st Qu.\nMedian\nMean\n3rd Qu.\nMax.\n\n\n5\n16\n24\n37.697674\n55.5\n171\n\n\n\n\n\nShow R codewith(hodg,summary(score))\n\n\n\nMin.\n1st Qu.\nMedian\nMean\n3rd Qu.\nMax.\n\n\n20\n60\n80\n76.27907\n90\n100\n\n\n\n\n\nShow R codehodg.cox2\n\nCall:\ncoxph(formula = Surv(time, event = delta == \"dead\") ~ gtype * \n    dtype + score + wt2, data = hodg2)\n\n                                   coef exp(coef)  se(coef)       z         p\ngtypeAutologous                0.665092  1.944669  0.594322  1.1191  0.263108\ndtypeHodgkins                  2.327326 10.250497  0.733244  3.1740  0.001503\nscore                         -0.055044  0.946444  0.012344 -4.4592 8.226e-06\nwt2long                       -2.059822  0.127477  1.050658 -1.9605  0.049937\ngtypeAutologous:dtypeHodgkins -2.066842  0.126585  0.925775 -2.2326  0.025578\n\nLikelihood ratio test=35.48  on 5 df, p=1.2052e-06\nn= 43, number of events= 26 \n\n\n\nShow R codehodg2[c(1,15,16,18,29),] |&gt; \n  select(gtype, dtype, time, delta, score, wtime) |&gt; \n  mutate(\n    comment = \n      c(\n        \"early death, good score, low risk\",\n        \"high risk grp, long wait, poor score\",\n        \"high risk grp, short wait, poor score\",\n        \"early death, good score, med risk grp\",\n        \"early death, good score, med risk grp\"\n      ))\n\n\n\n\n\n\n\n\n\n\n\n\ngtype\ndtype\ntime\ndelta\nscore\nwtime\ncomment\n\n\n\nAllogenic\nNon-Hodgkins\n28\ndead\n90\n24\nearly death, good score, low risk\n\n\nAllogenic\nHodgkins\n77\ndead\n60\n102\nhigh risk grp, long wait, poor score\n\n\nAllogenic\nHodgkins\n79\ndead\n70\n71\nhigh risk grp, short wait, poor score\n\n\nAutologous\nNon-Hodgkins\n53\ndead\n90\n17\nearly death, good score, med risk grp\n\n\nAutologous\nHodgkins\n30\ndead\n90\n73\nearly death, good score, med risk grp\n\n\n\n\n\nAction Items\n\nUnusual points may need checking, particularly if the data are not completely cleaned. In this case, observations 15 and 16 may show some trouble with the dichotomization of waiting time, but it still may be useful.\nThe two largest residuals seem to be due to unexpectedly early deaths, but unfortunately this can occur.\nIf hazards don’t look proportional, then we may need to use strata, between which the base hazards are permitted to be different. For this problem, the natural strata are the two diseases, because they could need to be managed differently anyway.\nA main point that we want to be sure of is the relative risk difference by disease type and graft type.\n\n\nShow R codehodg.cox2 |&gt; \n  predict(\n    reference = \"zero\",\n    newdata = means |&gt; \n      mutate(\n        wt2 = \"short\", \n        score = 0), \n    type = \"lp\") |&gt; \n  data.frame('linear predictor' = _) |&gt; \n  pander()\n\n\nLinear Risk Predictors for Lymphoma\n\n\n\n\n\n \nlinear.predictor\n\n\n\nNon-Hodgkins,Allogenic\n0\n\n\nNon-Hodgkins,Autologous\n0.6651\n\n\nHodgkins,Allogenic\n2.327\n\n\nHodgkins,Autologous\n0.9256\n\n\n\n\n\nFor Non-Hodgkin’s, the allogenic graft is better. For Hodgkin’s, the autologous graft is much better.\n\n7.0.7 Stratified survival models\nRevisiting the leukemia dataset (anderson)\nWe will analyze remission survival times on 42 leukemia patients, half on new treatment, half on standard treatment.\nThis is the same data as the drug6mp data from KMsurv, but with two other variables and without the pairing. This version comes from the Kleinbaum and Klein survival textbook (e.g., p281):\n\nShow R codeanderson = \n  paste0(\n    \"http://web1.sph.emory.edu/dkleinb/allDatasets/\",\n    \"surv2datasets/anderson.dta\") |&gt; \n  haven::read_dta() |&gt; \n  mutate(\n    status = status |&gt; \n      case_match(\n        1 ~ \"relapse\",\n        0 ~ \"censored\"\n      ),\n    \n    sex = sex |&gt; \n      case_match(\n        0 ~ \"female\",\n        1 ~ \"male\"\n      ) |&gt; \n      factor() |&gt; \n      relevel(ref = \"female\"),\n    \n    rx = rx |&gt; \n      case_match(\n        0 ~ \"new\",\n        1 ~ \"standard\"\n      ) |&gt; \n      factor() |&gt; relevel(ref = \"standard\"),\n    \n    surv = Surv(\n      time = survt, \n      event = (status == \"relapse\"))\n  )\n\nprint(anderson)\n\n\nCox semi-parametric proportional hazards model\n\nShow R codeanderson.cox1 = coxph(\n  formula = surv ~ rx + sex + logwbc,\n  data = anderson)\n\nsummary(anderson.cox1)\n\nCall:\ncoxph(formula = surv ~ rx + sex + logwbc, data = anderson)\n\n  n= 42, number of events= 30 \n\n            coef exp(coef) se(coef)       z Pr(&gt;|z|)    \nrxnew   -1.50359   0.22233  0.46151 -3.2580 0.001122 ** \nsexmale  0.31468   1.36982  0.45451  0.6923 0.488722    \nlogwbc   1.68194   5.37599  0.33658  4.9971 5.82e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        exp(coef) exp(-coef) lower .95 upper .95\nrxnew     0.22233    4.49781  0.089983   0.54934\nsexmale   1.36982    0.73002  0.562059   3.33844\nlogwbc    5.37599    0.18601  2.779444  10.39821\n\nConcordance= 0.851  (se = 0.041 )\nLikelihood ratio test= 47.19  on 3 df,   p=3e-10\nWald test            = 33.54  on 3 df,   p=2e-07\nScore (logrank) test = 48.01  on 3 df,   p=2e-10\n\n\nTest the proportional hazards assumption\n\nShow R codecox.zph(anderson.cox1)\n\n         chisq df      p\nrx     0.03603  1 0.8494\nsex    5.41975  1 0.0199\nlogwbc 0.14237  1 0.7059\nGLOBAL 5.87932  3 0.1176\n\n\nGraph the K-M survival curves\n\nShow R codeanderson_km_model = survfit(\n  formula = surv ~ sex,\n  data = anderson)\n\nanderson_km_model |&gt; \n  autoplot(conf.int = FALSE) +\n  theme_bw() +\n  theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\nThe survival curves cross, which indicates a problem in the proportionality assumption by sex.\nGraph the Nelson-Aalen cumulative hazard\nWe can also look at the log-hazard (“cloglog survival”) plots:\n\nShow R codeanderson_na_model = survfit(\n  formula = surv ~ sex,\n  data = anderson,\n  type = \"fleming\")\n\nanderson_na_model |&gt; \n  autoplot(\n    fun = \"cumhaz\",\n    conf.int = FALSE) +\n  theme_classic() +\n  theme(legend.position=\"bottom\") +\n  ylab(\"log(Cumulative Hazard)\") +\n  scale_y_continuous(\n    trans = \"log10\",\n    name = \"Cumulative hazard (H(t), log scale)\") +\n  scale_x_continuous(\n    breaks = c(1,2,5,10,20,50),\n    trans = \"log\"\n  )\n\n\nCumulative hazard (cloglog scale) for anderson data\n\n\n\n\nThis can be fixed by using strata or possibly by other model alterations.\nThe Stratified Cox Model\n\nIn a stratified Cox model, each stratum, defined by one or more factors, has its own base survival function \\(h_0(t)\\).\nBut the coefficients for each variable not used in the strata definitions are assumed to be the same across strata.\nTo check if this assumption is reasonable one can include interactions with strata and see if they are significant (this may generate a warning and NA lines but these can be ignored).\nSince the sex variable shows possible non-proportionality, we try stratifying on sex.\n\n\nShow R codeanderson.coxph.strat = \n  coxph(\n    formula = \n      surv ~ rx + logwbc + strata(sex),\n    data = anderson)\n\nsummary(anderson.coxph.strat)\n\nCall:\ncoxph(formula = surv ~ rx + logwbc + strata(sex), data = anderson)\n\n  n= 42, number of events= 30 \n\n           coef exp(coef) se(coef)       z  Pr(&gt;|z|)    \nrxnew  -0.99810   0.36858  0.47355 -2.1077   0.03506 *  \nlogwbc  1.45365   4.27872  0.34407  4.2249 2.391e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\nrxnew    0.36858    2.71313   0.14569   0.93244\nlogwbc   4.27872    0.23371   2.17993   8.39819\n\nConcordance= 0.812  (se = 0.059 )\nLikelihood ratio test= 32.06  on 2 df,   p=1e-07\nWald test            = 22.75  on 2 df,   p=1e-05\nScore (logrank) test = 30.8  on 2 df,   p=2e-07\n\n\nLet’s compare this to a model fit only on the subset of males:\n\nShow R codeanderson.coxph.male = \n  coxph(\n    formula = surv ~ rx + logwbc,\n    subset = sex == \"male\",\n    data = anderson)\n\nsummary(anderson.coxph.male)\n\nCall:\ncoxph(formula = surv ~ rx + logwbc, data = anderson, subset = sex == \n    \"male\")\n\n  n= 20, number of events= 14 \n\n           coef exp(coef) se(coef)       z Pr(&gt;|z|)   \nrxnew  -1.97789   0.13836  0.73920 -2.6757 0.007457 **\nlogwbc  1.74278   5.71319  0.53577  3.2528 0.001143 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\nrxnew    0.13836    7.22746  0.032494   0.58915\nlogwbc   5.71319    0.17503  1.999071  16.32783\n\nConcordance= 0.905  (se = 0.043 )\nLikelihood ratio test= 29.18  on 2 df,   p=5e-07\nWald test            = 15.33  on 2 df,   p=5e-04\nScore (logrank) test = 26.42  on 2 df,   p=2e-06\n\n\n\nShow R codeanderson.coxph.female = \n  coxph(\n    formula = \n      surv ~ rx + logwbc,\n    subset = sex == \"female\",\n    data = anderson)\n\nsummary(anderson.coxph.female)\n\nCall:\ncoxph(formula = surv ~ rx + logwbc, data = anderson, subset = sex == \n    \"female\")\n\n  n= 22, number of events= 16 \n\n           coef exp(coef) se(coef)       z Pr(&gt;|z|)  \nrxnew  -0.31127   0.73252  0.56355 -0.5523  0.58072  \nlogwbc  1.20615   3.34059  0.50349  2.3956  0.01659 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\nrxnew    0.73252    1.36516   0.24273    2.2106\nlogwbc   3.34059    0.29935   1.24524    8.9618\n\nConcordance= 0.692  (se = 0.085 )\nLikelihood ratio test= 6.65  on 2 df,   p=0.04\nWald test            = 6.36  on 2 df,   p=0.04\nScore (logrank) test = 6.74  on 2 df,   p=0.03\n\n\nThe coefficients of treatment look different. Are they statistically different?\n\nShow R codeanderson.coxph.strat.intxn = \n  coxph(\n    formula = surv ~ strata(sex) * (rx + logwbc),\n    data = anderson)\n\nanderson.coxph.strat.intxn |&gt; summary()\n\nCall:\ncoxph(formula = surv ~ strata(sex) * (rx + logwbc), data = anderson)\n\n  n= 42, number of events= 30 \n\n                           coef exp(coef) se(coef)       z Pr(&gt;|z|)  \nrxnew                  -0.31127   0.73252  0.56355 -0.5523  0.58072  \nlogwbc                  1.20615   3.34059  0.50349  2.3956  0.01659 *\nstrata(sex)male:rxnew  -1.66662   0.18888  0.92952 -1.7930  0.07298 .\nstrata(sex)male:logwbc  0.53663   1.71023  0.73522  0.7299  0.46546  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                       exp(coef) exp(-coef) lower .95 upper .95\nrxnew                    0.73252    1.36516  0.242728    2.2106\nlogwbc                   3.34059    0.29935  1.245237    8.9618\nstrata(sex)male:rxnew    0.18888    5.29423  0.030548    1.1679\nstrata(sex)male:logwbc   1.71023    0.58472  0.404792    7.2257\n\nConcordance= 0.797  (se = 0.058 )\nLikelihood ratio test= 35.83  on 4 df,   p=3e-07\nWald test            = 21.69  on 4 df,   p=2e-04\nScore (logrank) test = 33.15  on 4 df,   p=1e-06\n\n\n\nShow R codeanova(\n  anderson.coxph.strat.intxn,\n  anderson.coxph.strat)\n\n\n\nloglik\nChisq\nDf\nPr(&gt;|Chi|)\n\n\n\n-53.851887\nNA\nNA\nNA\n\n\n-55.734815\n3.7658564\n2\n0.15214394\n\n\n\n\n\nWe don’t have enough evidence to tell the difference between these two models.\nConclusions\n\nWe chose to use a stratified model because of the apparent non-proportionality of the hazard for the sex variable.\nWhen we fit interactions with the strata variable, we did not get an improved model (via the likelihood ratio test).\nSo we use the stratifed model with coefficients that are the same across strata.\nAnother Modeling Approach\n\nWe used an additive model without interactions and saw that we might need to stratify by sex.\nInstead, we could try to improve the model’s functional form - maybe the interaction of treatment and sex is real, and after fitting that we might not need separate hazard functions.\nEither approach may work.\n\n\nShow R codeanderson.coxph.intxn = \n  coxph(\n    formula = surv ~ (rx + logwbc) * sex,\n    data = anderson)\n\nanderson.coxph.intxn |&gt; summary()\n\nCall:\ncoxph(formula = surv ~ (rx + logwbc) * sex, data = anderson)\n\n  n= 42, number of events= 30 \n\n                    coef exp(coef)  se(coef)       z Pr(&gt;|z|)  \nrxnew          -0.374810  0.687420  0.554519 -0.6759  0.49909  \nlogwbc          1.063699  2.897068  0.472606  2.2507  0.02440 *\nsexmale        -2.805216  0.060494  2.032303 -1.3803  0.16749  \nrxnew:sexmale  -2.178164  0.113249  0.910947 -2.3911  0.01680 *\nlogwbc:sexmale  1.230312  3.422298  0.630078  1.9526  0.05086 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n               exp(coef) exp(-coef) lower .95 upper .95\nrxnew           0.687420    1.45471 0.2318542    2.0381\nlogwbc          2.897068    0.34518 1.1472974    7.3155\nsexmale         0.060494   16.53065 0.0011267    3.2480\nrxnew:sexmale   0.113249    8.83008 0.0189949    0.6752\nlogwbc:sexmale  3.422298    0.29220 0.9953926   11.7663\n\nConcordance= 0.861  (se = 0.036 )\nLikelihood ratio test= 56.95  on 5 df,   p=5e-11\nWald test            = 35.58  on 5 df,   p=1e-06\nScore (logrank) test = 57.07  on 5 df,   p=5e-11\n\n\n\nShow R codecox.zph(anderson.coxph.intxn)\n\n            chisq df     p\nrx         0.1361  1 0.712\nlogwbc     1.6521  1 0.199\nsex        1.2659  1 0.261\nrx:sex     0.1490  1 0.699\nlogwbc:sex 0.1019  1 0.750\nGLOBAL     3.7465  5 0.586\n\n\n\n7.0.8 Time-varying covariates\n(adapted from Klein, Moeschberger, et al. (2003), §9.2)\nMotivating example: back to the leukemia dataset\n\n# load the data:\ndata(bmt, package = 'KMsurv')\nbmt |&gt; as_tibble() |&gt; print(n = 5)\n\n# A tibble: 137 × 22\n  group    t1    t2    d1    d2    d3    ta    da    tc    dc    tp    dp    z1\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     1  2081  2081     0     0     0    67     1   121     1    13     1    26\n2     1  1602  1602     0     0     0  1602     0   139     1    18     1    21\n3     1  1496  1496     0     0     0  1496     0   307     1    12     1    26\n4     1  1462  1462     0     0     0    70     1    95     1    13     1    17\n5     1  1433  1433     0     0     0  1433     0   236     1    12     1    32\n# ℹ 132 more rows\n# ℹ 9 more variables: z2 &lt;int&gt;, z3 &lt;int&gt;, z4 &lt;int&gt;, z5 &lt;int&gt;, z6 &lt;int&gt;,\n#   z7 &lt;int&gt;, z8 &lt;int&gt;, z9 &lt;int&gt;, z10 &lt;int&gt;\n\n\nThis dataset comes from the Copelan et al. (1991) study of allogenic bone marrow transplant therapy for acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).\nOutcomes (endpoints)\n\nThe main endpoint is disease-free survival (t2 and d3) for the three risk groups, “ALL”, “AML Low Risk”, and “AML High Risk”.\nPossible intermediate events\n\ngraft vs. host disease (GVHD), an immunological rejection response to the transplant (bad)\nacute (AGVHD)\nchronic (CGVHD)\nplatelet recovery, a return of platelet count to normal levels (good)\n\nOne or the other, both in either order, or neither may occur.\nCovariates\n\nWe are interested in possibly using the covariates z1-z10 to adjust for other factors.\nIn addition, the time-varying covariates for acute GVHD, chronic GVHD, and platelet recovery may be useful.\nPreprocessing\nWe reformat the data before analysis:\n\nShow R code# reformat the data:\nbmt1 = \n  bmt |&gt; \n  as_tibble() |&gt; \n  mutate(\n    id = 1:n(), # will be used to connect multiple records for the same individual\n    \n    group =  group |&gt; \n      case_match(\n        1 ~ \"ALL\",\n        2 ~ \"Low Risk AML\",\n        3 ~ \"High Risk AML\") |&gt; \n      factor(levels = c(\"ALL\", \"Low Risk AML\", \"High Risk AML\")),\n    \n    `patient age` = z1,\n    \n    `donor age` = z2,\n    \n    `patient sex` = z3 |&gt; \n      case_match(\n        0 ~ \"Female\",\n        1 ~ \"Male\"),\n    \n    `donor sex` = z4 |&gt; \n      case_match(\n        0 ~ \"Female\",\n        1 ~ \"Male\"),\n    \n    `Patient CMV Status` = z5 |&gt; \n      case_match(\n        0 ~ \"CMV Negative\",\n        1 ~ \"CMV Positive\"),\n    \n    `Donor CMV Status` = z6 |&gt; \n      case_match(\n        0 ~ \"CMV Negative\",\n        1 ~ \"CMV Positive\"),\n    \n    `Waiting Time to Transplant` = z7,\n    \n    FAB = z8 |&gt; \n      case_match(\n        1 ~ \"Grade 4 Or 5 (AML only)\",\n        0 ~ \"Other\") |&gt; \n      factor() |&gt; \n      relevel(ref = \"Other\"),\n    \n    hospital = z9 |&gt;  # `z9` is hospital\n      case_match(\n        1 ~ \"Ohio State University\",\n        2 ~ \"Alferd\",\n        3 ~ \"St. Vincent\",\n        4 ~ \"Hahnemann\") |&gt; \n      factor() |&gt; \n      relevel(ref = \"Ohio State University\"),\n    \n    MTX = (z10 == 1) # a prophylatic treatment for GVHD\n    \n  ) |&gt; \n  select(-(z1:z10)) # don't need these anymore\n\nbmt1 |&gt; \n  select(group, id:MTX) |&gt; \n  print(n = 10)\n\n# A tibble: 137 × 12\n   group    id `patient age` `donor age` `patient sex` `donor sex`\n   &lt;fct&gt; &lt;int&gt;         &lt;int&gt;       &lt;int&gt; &lt;chr&gt;         &lt;chr&gt;      \n 1 ALL       1            26          33 Male          Female     \n 2 ALL       2            21          37 Male          Male       \n 3 ALL       3            26          35 Male          Male       \n 4 ALL       4            17          21 Female        Male       \n 5 ALL       5            32          36 Male          Male       \n 6 ALL       6            22          31 Male          Male       \n 7 ALL       7            20          17 Male          Female     \n 8 ALL       8            22          24 Male          Female     \n 9 ALL       9            18          21 Female        Male       \n10 ALL      10            24          40 Male          Male       \n# ℹ 127 more rows\n# ℹ 6 more variables: `Patient CMV Status` &lt;chr&gt;, `Donor CMV Status` &lt;chr&gt;,\n#   `Waiting Time to Transplant` &lt;int&gt;, FAB &lt;fct&gt;, hospital &lt;fct&gt;, MTX &lt;lgl&gt;\n\n\nTime-Dependent Covariates\n\nA time-dependent covariate (“TDC”) is a covariate whose value changes during the course of the study.\nFor variables like age that change in a linear manner with time, we can just use the value at the start.\nBut it may be plausible that when and if GVHD occurs, the risk of relapse or death increases, and when and if platelet recovery occurs, the risk decreases.\nAnalysis in R\n\nWe form a variable precovery which is = 0 before platelet recovery and is = 1 after platelet recovery, if it occurs.\nFor each subject where platelet recovery occurs, we set up multiple records (lines in the data frame); for example one from t = 0 to the time of platelet recovery, and one from that time to relapse, recovery, or death.\nWe do the same for acute GVHD and chronic GVHD.\nFor each record, the covariates are constant.\n\n\nShow R codebmt2 = bmt1 |&gt; \n  #set up new long-format data set:\n  tmerge(bmt1, id = id, tstop = t2) |&gt; \n  \n  # the following three steps can be in any order, \n  # and will still produce the same result:\n  #add aghvd as tdc:\n  tmerge(bmt1, id = id, agvhd = tdc(ta)) |&gt; \n  #add cghvd as tdc:\n  tmerge(bmt1, id = id, cgvhd = tdc(tc)) |&gt; \n  #add platelet recovery as tdc:\n  tmerge(bmt1, id = id, precovery = tdc(tp))   \n\nbmt2 = bmt2 |&gt; \n  as_tibble() |&gt; \n  mutate(status = as.numeric((tstop == t2) & d3))\n# status only = 1 if at end of t2 and not censored\n\n\nLet’s see how we’ve rearranged the first row of the data:\n\nShow R codebmt1 |&gt; \n  dplyr::filter(id == 1) |&gt; \n  dplyr::select(id, t1, d1, t2, d2, d3, ta, da, tc, dc, tp, dp)\n\n\n\nid\nt1\nd1\nt2\nd2\nd3\nta\nda\ntc\ndc\ntp\ndp\n\n\n1\n2081\n0\n2081\n0\n0\n67\n1\n121\n1\n13\n1\n\n\n\n\nThe event times for this individual are:\n\n\nt = 0 time of transplant\n\ntp = 13 platelet recovery\n\nta = 67 acute GVHD onset\n\ntc = 121 chronic GVHD onset\n\nt2 = 2081 end of study, patient not relapsed or dead\n\nAfter converting the data to long-format, we have:\n\nShow R codebmt2 |&gt; \n  select(\n    id,\n    tstart,\n    tstop,\n    agvhd,\n    cgvhd,\n    precovery,\n    status\n  ) |&gt; \n  dplyr::filter(id == 1)\n\n\n\nid\ntstart\ntstop\nagvhd\ncgvhd\nprecovery\nstatus\n\n\n\n1\n0\n13\n0\n0\n0\n0\n\n\n1\n13\n67\n0\n0\n1\n0\n\n\n1\n67\n121\n1\n0\n1\n0\n\n\n1\n121\n2081\n1\n1\n1\n0\n\n\n\n\n\nNote that status could have been 1 on the last row, indicating that relapse or death occurred; since it is false, the participant must have exited the study without experiencing relapse or death (i.e., they were censored).\nEvent sequences\nLet:\n\nA = acute GVHD\nC = chronic GVHD\nP = platelet recovery\n\nEach of the eight possible combinations of A or not-A, with C or not-C, with P or not-P occurs in this data set.\n\nA always occurs before C, and P always occurs before C, if both occur.\nThus there are ten event sequences in the data set: None, A, C, P, AC, AP, PA, PC, APC, and PAC.\nIn general, there could be as many as \\(1+3+(3)(2)+6=16\\) sequences, but our domain knowledge tells us that some are missing: CA, CP, CAP, CPA, PCA, PC, PAC\nDifferent subjects could have 1, 2, 3, or 4 intervals, depending on which of acute GVHD, chronic GVHD, and/or platelet recovery occurred.\nThe final interval for any subject has status = 1 if the subject relapsed or died at that time; otherwise status = 0.\nAny earlier intervals have status = 0.\nEven though there might be multiple lines per ID in the dataset, there is never more than one event, so no alterations need be made in the estimation procedures or in the interpretation of the output.\nThe function tmerge in the survival package eases the process of constructing the new long-format dataset.\nModel with Time-Fixed Covariates\n\nShow R codebmt1 = \n  bmt1 |&gt; \n  mutate(surv = Surv(t2,d3))\n\nbmt_coxph_TF = coxph(\n  formula = surv ~ group + `patient age`*`donor age` + FAB,\n  data = bmt1)\nsummary(bmt_coxph_TF)\n\nCall:\ncoxph(formula = surv ~ group + `patient age` * `donor age` + \n    FAB, data = bmt1)\n\n  n= 137, number of events= 83 \n\n                                  coef   exp(coef)    se(coef)       z Pr(&gt;|z|)\ngroupLow Risk AML          -1.09064760  0.33599883  0.35427897 -3.0785 0.002080\ngroupHigh Risk AML         -0.40390516  0.66770744  0.36277665 -1.1134 0.265549\n`patient age`              -0.08163876  0.92160482  0.03610660 -2.2610 0.023756\n`donor age`                -0.08458681  0.91889188  0.03009727 -2.8104 0.004947\nFABGrade 4 Or 5 (AML only)  0.83741559  2.31038828  0.27846445  3.0073 0.002636\n`patient age`:`donor age`   0.00315925  1.00316425  0.00095076  3.3229 0.000891\n                              \ngroupLow Risk AML          ** \ngroupHigh Risk AML            \n`patient age`              *  \n`donor age`                ** \nFABGrade 4 Or 5 (AML only) ** \n`patient age`:`donor age`  ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                           exp(coef) exp(-coef) lower .95 upper .95\ngroupLow Risk AML            0.33600    2.97620   0.16779   0.67282\ngroupHigh Risk AML           0.66771    1.49766   0.32794   1.35951\n`patient age`                0.92160    1.08506   0.85864   0.98919\n`donor age`                  0.91889    1.08827   0.86625   0.97473\nFABGrade 4 Or 5 (AML only)   2.31039    0.43283   1.33861   3.98763\n`patient age`:`donor age`    1.00316    0.99685   1.00130   1.00504\n\nConcordance= 0.665  (se = 0.033 )\nLikelihood ratio test= 32.8  on 6 df,   p=1e-05\nWald test            = 33.02  on 6 df,   p=1e-05\nScore (logrank) test = 35.75  on 6 df,   p=3e-06\n\nShow R codedrop1(bmt_coxph_TF, test = \"Chisq\")\n\n\n\n\nDf\nAIC\nLRT\nPr(&gt;Chi)\n\n\n\n\nNA\n725.78784\nNA\nNA\n\n\ngroup\n2\n734.29908\n12.5112414\n0.00191963\n\n\nFAB\n1\n733.00400\n9.2161618\n0.00239888\n\n\n\npatient age:donor age\n\n1\n733.30153\n9.5136879\n0.00203945\n\n\n\n\n\n\nShow R codebmt1$mres = \n  bmt_coxph_TF |&gt; \n  update(. ~ . - `donor age`) |&gt; \n  residuals(type=\"martingale\")\n\nbmt1 |&gt; \n  ggplot(aes(x = `donor age`, y = mres)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", aes(col = \"loess\")) +\n  geom_smooth(method = 'lm', aes(col = \"lm\")) +\n  theme_classic() +\n  xlab(\"Donor age\") +\n  ylab(\"Martingale Residuals\") +\n  guides(col=guide_legend(title = \"\"))\n\n\nMartingale residuals for Donor age\n\n\n\n\nA more complex functional form for donor age seems warranted; left as an exercise for the reader.\nNow we will add the time-varying covariates:\n\nShow R code# add counting process formulation of Surv():\nbmt2 = \n  bmt2 |&gt; \n  mutate(\n    surv = \n      Surv(\n        time = tstart,\n        time2 = tstop,\n        event = status,\n        type = \"counting\"))\n\n\nLet’s see how the data looks for patient 15:\n\nShow R codebmt1 |&gt; dplyr::filter(id == 15) |&gt; dplyr::select(tp, dp, tc,dc, ta, da, FAB, surv, t1, d1, t2, d2, d3)\n\n\n\ntp\ndp\ntc\ndc\nta\nda\nFAB\nsurv\nt1\nd1\nt2\nd2\nd3\n\n\n21\n1\n220\n1\n418\n0\nOther\n418\n418\n1\n418\n0\n1\n\n\n\nShow R codebmt2 |&gt; dplyr::filter(id == 15) |&gt; dplyr::select(id, agvhd, cgvhd, precovery, surv)\n\n\n\nid\nagvhd\ncgvhd\nprecovery\nsurv\n\n\n\n15\n0\n0\n0\n( 0, 21+]\n\n\n15\n0\n0\n1\n( 21,220+]\n\n\n15\n0\n1\n1\n(220,418]\n\n\n\n\n\nModel with Time-Dependent Covariates\n\nShow R codebmt_coxph_TV = coxph(\n  formula = \n    surv ~ \n    group + `patient age`*`donor age` + FAB + agvhd + cgvhd + precovery,\n  data = bmt2)\n\nsummary(bmt_coxph_TV)\n\nCall:\ncoxph(formula = surv ~ group + `patient age` * `donor age` + \n    FAB + agvhd + cgvhd + precovery, data = bmt2)\n\n  n= 341, number of events= 83 \n\n                                  coef   exp(coef)    se(coef)       z Pr(&gt;|z|)\ngroupLow Risk AML          -1.03851442  0.35398016  0.35822038 -2.8991 0.003742\ngroupHigh Risk AML         -0.38048094  0.68353259  0.37486705 -1.0150 0.310117\n`patient age`              -0.07335107  0.92927453  0.03595566 -2.0400 0.041346\n`donor age`                -0.07640620  0.92643981  0.03019649 -2.5303 0.011396\nFABGrade 4 Or 5 (AML only)  0.80570020  2.23826317  0.28427265  2.8343 0.004593\nagvhd                       0.15056493  1.16249078  0.30684843  0.4907 0.623652\ncgvhd                      -0.11613589  0.89035422  0.28904631 -0.4018 0.687839\nprecovery                  -0.94112267  0.39018954  0.34786110 -2.7055 0.006821\n`patient age`:`donor age`   0.00289459  1.00289878  0.00094354  3.0678 0.002156\n                             \ngroupLow Risk AML          **\ngroupHigh Risk AML           \n`patient age`              * \n`donor age`                * \nFABGrade 4 Or 5 (AML only) **\nagvhd                        \ncgvhd                        \nprecovery                  **\n`patient age`:`donor age`  **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                           exp(coef) exp(-coef) lower .95 upper .95\ngroupLow Risk AML            0.35398    2.82502   0.17541   0.71433\ngroupHigh Risk AML           0.68353    1.46299   0.32785   1.42510\n`patient age`                0.92927    1.07611   0.86604   0.99712\n`donor age`                  0.92644    1.07940   0.87320   0.98293\nFABGrade 4 Or 5 (AML only)   2.23826    0.44677   1.28215   3.90737\nagvhd                        1.16249    0.86022   0.63709   2.12119\ncgvhd                        0.89035    1.12315   0.50527   1.56892\nprecovery                    0.39019    2.56286   0.19732   0.77157\n`patient age`:`donor age`    1.00290    0.99711   1.00105   1.00476\n\nConcordance= 0.702  (se = 0.028 )\nLikelihood ratio test= 40.28  on 9 df,   p=7e-06\nWald test            = 42.35  on 9 df,   p=3e-06\nScore (logrank) test = 47.25  on 9 df,   p=4e-07\n\n\nPlatelet recovery is highly significant.\nNeither acute GVHD (agvhd) nor chronic GVHD (cgvhd) has a statistically significant effect here, nor are they significant in models with the other one removed.\n\nShow R codeupdate(bmt_coxph_TV, .~.-agvhd) |&gt; summary()\n\nCall:\ncoxph(formula = surv ~ group + `patient age` + `donor age` + \n    FAB + cgvhd + precovery + `patient age`:`donor age`, data = bmt2)\n\n  n= 341, number of events= 83 \n\n                                  coef   exp(coef)    se(coef)       z Pr(&gt;|z|)\ngroupLow Risk AML          -1.04987004  0.34998323  0.35672687 -2.9431 0.003250\ngroupHigh Risk AML         -0.41704937  0.65898839  0.36534770 -1.1415 0.253656\n`patient age`              -0.07074898  0.93169574  0.03547693 -1.9942 0.046127\n`donor age`                -0.07569294  0.92710083  0.03007505 -2.5168 0.011843\nFABGrade 4 Or 5 (AML only)  0.80703516  2.24125317  0.28343710  2.8473 0.004409\ncgvhd                      -0.09539332  0.90901533  0.28597858 -0.3336 0.738706\nprecovery                  -0.98365334  0.37394246  0.33817038 -2.9088 0.003629\n`patient age`:`donor age`   0.00285862  1.00286271  0.00093629  3.0531 0.002265\n                             \ngroupLow Risk AML          **\ngroupHigh Risk AML           \n`patient age`              * \n`donor age`                * \nFABGrade 4 Or 5 (AML only) **\ncgvhd                        \nprecovery                  **\n`patient age`:`donor age`  **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                           exp(coef) exp(-coef) lower .95 upper .95\ngroupLow Risk AML            0.34998    2.85728   0.17394   0.70420\ngroupHigh Risk AML           0.65899    1.51748   0.32203   1.34854\n`patient age`                0.93170    1.07331   0.86911   0.99879\n`donor age`                  0.92710    1.07863   0.87403   0.98339\nFABGrade 4 Or 5 (AML only)   2.24125    0.44618   1.28596   3.90619\ncgvhd                        0.90902    1.10009   0.51897   1.59220\nprecovery                    0.37394    2.67421   0.19273   0.72553\n`patient age`:`donor age`    1.00286    0.99715   1.00102   1.00470\n\nConcordance= 0.701  (se = 0.027 )\nLikelihood ratio test= 40.05  on 8 df,   p=3e-06\nWald test            = 42.37  on 8 df,   p=1e-06\nScore (logrank) test = 47.21  on 8 df,   p=1e-07\n\nShow R codeupdate(bmt_coxph_TV, .~.-cgvhd) |&gt; summary()\n\nCall:\ncoxph(formula = surv ~ group + `patient age` + `donor age` + \n    FAB + agvhd + precovery + `patient age`:`donor age`, data = bmt2)\n\n  n= 341, number of events= 83 \n\n                                  coef   exp(coef)    se(coef)       z Pr(&gt;|z|)\ngroupLow Risk AML          -1.01963833  0.36072538  0.35531132 -2.8697 0.004109\ngroupHigh Risk AML         -0.38135586  0.68293482  0.37456820 -1.0181 0.308620\n`patient age`              -0.07318862  0.92942550  0.03588981 -2.0393 0.041424\n`donor age`                -0.07675347  0.92611814  0.03012065 -2.5482 0.010828\nFABGrade 4 Or 5 (AML only)  0.81171596  2.25176863  0.28401185  2.8580 0.004263\nagvhd                       0.13162135  1.14067632  0.30262299  0.4349 0.663610\nprecovery                  -0.94669705  0.38802052  0.34726498 -2.7262 0.006408\n`patient age`:`donor age`   0.00290421  1.00290843  0.00094252  3.0813 0.002061\n                             \ngroupLow Risk AML          **\ngroupHigh Risk AML           \n`patient age`              * \n`donor age`                * \nFABGrade 4 Or 5 (AML only) **\nagvhd                        \nprecovery                  **\n`patient age`:`donor age`  **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                           exp(coef) exp(-coef) lower .95 upper .95\ngroupLow Risk AML            0.36073    2.77219   0.17978   0.72380\ngroupHigh Risk AML           0.68293    1.46427   0.32775   1.42302\n`patient age`                0.92943    1.07593   0.86629   0.99716\n`donor age`                  0.92612    1.07978   0.87303   0.98244\nFABGrade 4 Or 5 (AML only)   2.25177    0.44410   1.29054   3.92894\nagvhd                        1.14068    0.87667   0.63033   2.06422\nprecovery                    0.38802    2.57718   0.19645   0.76639\n`patient age`:`donor age`    1.00291    0.99710   1.00106   1.00476\n\nConcordance= 0.701  (se = 0.027 )\nLikelihood ratio test= 40.12  on 8 df,   p=3e-06\nWald test            = 42.11  on 8 df,   p=1e-06\nScore (logrank) test = 47.14  on 8 df,   p=1e-07\n\n\nLet’s drop them both:\n\nShow R codebmt_coxph_TV2 = update(bmt_coxph_TV, . ~ . - agvhd -cgvhd)\nbmt_coxph_TV2 |&gt; summary()\n\nCall:\ncoxph(formula = surv ~ group + `patient age` + `donor age` + \n    FAB + precovery + `patient age`:`donor age`, data = bmt2)\n\n  n= 341, number of events= 83 \n\n                                  coef   exp(coef)    se(coef)       z Pr(&gt;|z|)\ngroupLow Risk AML          -1.03251998  0.35610844  0.35320191 -2.9233 0.003463\ngroupHigh Risk AML         -0.41388806  0.66107495  0.36520948 -1.1333 0.257093\n`patient age`              -0.07096467  0.93149480  0.03545326 -2.0016 0.045323\n`donor age`                -0.07605237  0.92676767  0.03000709 -2.5345 0.011261\nFABGrade 4 Or 5 (AML only)  0.81192620  2.25224207  0.28323097  2.8667 0.004148\nprecovery                  -0.98350529  0.37399783  0.33799702 -2.9098 0.003617\n`patient age`:`donor age`   0.00287164  1.00287576  0.00093553  3.0695 0.002144\n                             \ngroupLow Risk AML          **\ngroupHigh Risk AML           \n`patient age`              * \n`donor age`                * \nFABGrade 4 Or 5 (AML only) **\nprecovery                  **\n`patient age`:`donor age`  **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                           exp(coef) exp(-coef) lower .95 upper .95\ngroupLow Risk AML            0.35611    2.80813   0.17821   0.71159\ngroupHigh Risk AML           0.66107    1.51269   0.32313   1.35244\n`patient age`                0.93149    1.07354   0.86897   0.99852\n`donor age`                  0.92677    1.07902   0.87383   0.98291\nFABGrade 4 Or 5 (AML only)   2.25224    0.44400   1.29279   3.92375\nprecovery                    0.37400    2.67381   0.19283   0.72539\n`patient age`:`donor age`    1.00288    0.99713   1.00104   1.00472\n\nConcordance= 0.7  (se = 0.027 )\nLikelihood ratio test= 39.93  on 7 df,   p=1e-06\nWald test            = 42.18  on 7 df,   p=5e-07\nScore (logrank) test = 47.11  on 7 df,   p=5e-08\n\n\n\n7.0.9 Recurrent Events\n(Adapted from Kleinbaum and Klein, Ch 8)\n\nSometimes an appropriate analysis requires consideration of recurrent events.\nA patient with arthritis may have more than one flareup. The same is true of many recurring-remitting diseases.\nIn this case, we have more than one line in the data frame, but each line may have an event.\nWe have to use a “robust” variance estimator to account for correlation of time-to-events within a patient.\n\nBladder Cancer Data Set\nThe bladder cancer dataset from Kleinbaum and Klein contains recurrent event outcome information for eighty-six cancer patients followed for the recurrence of bladder cancer tumor after transurethral surgical excision (Byar and Green 1980). The exposure of interest is the effect of the drug treatment of thiotepa. Control variables are the initial number and initial size of tumors. The data layout is suitable for a counting processes approach.\nThis drug is still a possible choice for some patients. Another therapeutic choice is Bacillus Calmette-Guerin (BCG), a live bacterium related to cow tuberculosis.\nData dictionary\n\nVariables in the bladder dataset\n\nVariable\nDefinition\n\n\n\nid\nPatient unique ID\n\n\nstatus\nfor each time interval: 1 = recurred, 0 = censored\n\n\ninterval\n1 = first recurrence, etc.\n\n\nintime\n`tstop - tstart (all times in months)\n\n\ntstart\nstart of interval\n\n\ntstop\nend of interval\n\n\ntx\ntreatment code, 1 = thiotepa\n\n\nnum\nnumber of initial tumors\n\n\nsize\nsize of initial tumors (cm)\n\n\n\n\nThere are 85 patients and 190 lines in the dataset, meaning that many patients have more than one line.\nPatient 1 with 0 observation time was removed.\nOf the 85 patients, 47 had at least one recurrence and 38 had none.\n18 patients had exactly one recurrence.\nThere were up to 4 recurrences in a patient.\nOf the 190 intervals, 112 terminated with a recurrence and 78 were censored.\nDifferent intervals for the same patient are correlated.\n\nIs the effective sample size 47 or 112? This might narrow confidence intervals by as much as a factor of \\(\\sqrt{112/47}=1.54\\)\nWhat happens if I have 5 treatment and 5 control values and want to do a t-test and I then duplicate the 10 values as if the sample size was 20? This falsely narrows confidence intervals by a factor of \\(\\sqrt{2}=1.41\\).\n\n\nShow R codebladder = \n  paste0(\n    \"http://web1.sph.emory.edu/dkleinb/allDatasets\",\n    \"/surv2datasets/bladder.dta\") |&gt; \n  read_dta() |&gt; \n  as_tibble()\n\nbladder = bladder[-1,]  #remove subject with 0 observation time\nprint(bladder)\n\n\n\nShow R codebladder = \n  bladder |&gt; \n  mutate(\n    surv = \n      Surv(\n        time = start,\n        time2 = stop,\n        event = event,\n        type = \"counting\"))\n\nbladder.cox1 = coxph(\n  formula = surv~tx+num+size,\n  data = bladder)\n\n#results with biased variance-covariance matrix:\nsummary(bladder.cox1)\n\nCall:\ncoxph(formula = surv ~ tx + num + size, data = bladder)\n\n  n= 190, number of events= 112 \n\n          coef exp(coef)  se(coef)       z  Pr(&gt;|z|)    \ntx   -0.411638  0.662564  0.199892 -2.0593 0.0394657 *  \nnum   0.163669  1.177825  0.047766  3.4265 0.0006115 ***\nsize -0.041079  0.959754  0.070295 -0.5844 0.5589668    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     exp(coef) exp(-coef) lower .95 upper .95\ntx     0.66256    1.50929   0.44779   0.98034\nnum    1.17782    0.84902   1.07256   1.29342\nsize   0.95975    1.04193   0.83623   1.10153\n\nConcordance= 0.624  (se = 0.032 )\nLikelihood ratio test= 14.66  on 3 df,   p=0.002\nWald test            = 15.9  on 3 df,   p=0.001\nScore (logrank) test = 16.18  on 3 df,   p=0.001\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe likelihood ratio and score tests assume independence of observations within a cluster. The Wald and robust score tests do not.\n\n\nadding cluster = id\n\nIf we add cluster= id to the call to coxph, the coefficient estimates don’t change, but we get an additional column in the summary() output: robust se:\n\nShow R codebladder.cox2 = coxph(\n  formula = surv ~ tx + num + size,\n  cluster = id,\n  data = bladder)\n\n#unbiased though this reduces power:\nsummary(bladder.cox2)\n\nCall:\ncoxph(formula = surv ~ tx + num + size, data = bladder, cluster = id)\n\n  n= 190, number of events= 112 \n\n          coef exp(coef)  se(coef) robust se       z Pr(&gt;|z|)   \ntx   -0.411638  0.662564  0.199892  0.248764 -1.6547 0.097979 . \nnum   0.163669  1.177825  0.047766  0.058422  2.8015 0.005087 **\nsize -0.041079  0.959754  0.070295  0.074214 -0.5535 0.579912   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     exp(coef) exp(-coef) lower .95 upper .95\ntx     0.66256    1.50929   0.40689    1.0789\nnum    1.17782    0.84902   1.05039    1.3207\nsize   0.95975    1.04193   0.82983    1.1100\n\nConcordance= 0.624  (se = 0.031 )\nLikelihood ratio test= 14.66  on 3 df,   p=0.002\nWald test            = 11.19  on 3 df,   p=0.01\nScore (logrank) test = 16.18  on 3 df,   p=0.001,   Robust = 10.84  p=0.01\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not).\n\n\nrobust se is larger than se, and accounts for the repeated observations from the same individuals:\n\nShow R coderound(bladder.cox2$naive.var, 4)\n\n\n\n0.0400\n-0.0014\n0.0000\n\n\n-0.0014\n0.0023\n0.0007\n\n\n0.0000\n0.0007\n0.0049\n\n\n\nShow R coderound(bladder.cox2$var, 4)\n\n\n\n0.0619\n-0.0026\n-0.0004\n\n\n-0.0026\n0.0034\n0.0013\n\n\n-0.0004\n0.0013\n0.0055\n\n\n\n\nThese are the ratios of correct confidence intervals to naive ones:\n\nShow R codewith(bladder.cox2, diag(var)/diag(naive.var)) |&gt; sqrt()\n\n[1] 1.2444917 1.2230924 1.0557607\n\n\nWe might try dropping the non-significant size variable:\n\nShow R code#remove non-significant size variable:\nbladder.cox3 = bladder.cox2 |&gt; update(. ~ . - size)\nsummary(bladder.cox3)\n\nCall:\ncoxph(formula = surv ~ tx + num, data = bladder, cluster = id)\n\n  n= 190, number of events= 112 \n\n         coef exp(coef)  se(coef) robust se       z Pr(&gt;|z|)   \ntx  -0.411723  0.662508  0.200287  0.251532 -1.6369 0.101659   \nnum  0.170006  1.185312  0.046457  0.056363  3.0162 0.002559 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    exp(coef) exp(-coef) lower .95 upper .95\ntx    0.66251    1.50942   0.40466    1.0847\nnum   1.18531    0.84366   1.06134    1.3238\n\nConcordance= 0.623  (se = 0.031 )\nLikelihood ratio test= 14.31  on 2 df,   p=8e-04\nWald test            = 10.24  on 2 df,   p=0.006\nScore (logrank) test = 15.81  on 2 df,   p=4e-04,   Robust = 10.6  p=0.005\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not).\n\n\n\nWays to check PH assumption:\n\ncloglog\nschoenfeld residuals\ninteraction with time\n\n\n\n7.0.10 Age as the time scale\nSee Canchola et al. (2003).\n\n\n\n\n\n\nCanchola, Alison J, Susan L Stewart, Leslie Bernstein, Dee W West, Ronald K Ross, Dennis Deapen, Richard Pinder, et al. 2003. “Cox Regression Using Different Time-Scales.” Western Users of SAS Software. https://www.lexjansen.com/wuss/2003/DataAnalysis/i-cox_time_scales.pdf.\n\n\nGrambsch, Patricia M, and Terry M Therneau. 1994. “Proportional Hazards Tests and Diagnostics Based on Weighted Residuals.” Biometrika 81 (3): 515–26. https://doi.org/10.1093/biomet/81.3.515.\n\n\nKlein, John P, Melvin L Moeschberger, et al. 2003. Survival Analysis: Techniques for Censored and Truncated Data. Vol. 1230. Springer. https://link.springer.com/book/10.1007/b97377.",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Building Cox Proportional Hazards models</span>"
    ]
  },
  {
    "objectID": "parametric-survival-models.html",
    "href": "parametric-survival-models.html",
    "title": "\n8  Parametric survival models\n",
    "section": "",
    "text": "Configuring R\nFunctions from these packages will be used throughout this document:\n\nShow R codelibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(conflicted) # check for conflicting function definitions\nconflicts_prefer(dplyr::filter)\n\n\nHere are some R settings I use in this document:\n\nShow R coderm(list = ls()) # delete any data that's already loaded into R\nknitr::opts_chunk$set(message = FALSE)\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\noptions('digits' = 4)\n\n\n\n8.0.1 Parametric Survival Models\nExponential Distribution\n\nThe exponential distribution is the basic distribution for survival analysis.\n\n\\[\n\\begin{aligned}\nf(t) &= \\lambda e^{-\\lambda t}\\\\\n\\text{log}\\left\\{f(t)\\right\\} &= \\text{log}\\left\\{\\lambda\\right\\}-\\lambda t\\\\\nF(t) &= 1-e^{-\\lambda t}\\\\\nS(t)&= e^{-\\lambda t}\\\\\nH(t)&=\\text{log}\\left\\{S(t)\\right\\} = -\\lambda t\\\\\nh(t) &= \\lambda\\\\\n\\text{E}(T) &= \\lambda^{-1}\n\\end{aligned}\n\\]\nWeibull Distribution\nUsing the Kalbfleisch and Prentice (2002) notation:\n\\[\n\\begin{aligned}\nf(t)&= \\lambda p (\\lambda t)^{p-1}e^{-(\\lambda t)^p}\\\\\nF(t)&=1 - e^{-(\\lambda t)^p}\\\\\nS(t)&=e^{-(\\lambda t)^p}\\\\\nh(t)&=\\lambda p (\\lambda t)^{p-1}\\\\\nH(t)&=(\\lambda t)^p\\\\\n\\text{log}\\left\\{H(t)\\right\\} &= p \\text{log}\\left\\{\\lambda t\\right\\} = p \\text{log}\\left\\{\\lambda\\right\\} + p \\text{log}\\left\\{t\\right\\}\\\\\n\\text{E}(T) &= \\lambda^{-1} \\cdot \\Gamma\\left(1 + \\frac{1}{p}\\right)\n\\end{aligned}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nRecall from calculus:\n\n\\(\\Gamma(t) \\stackrel{\\text{def}}{=}\\int_{u=0}^{\\infty}u^{t-1}e^{-u}du\\)\n\\(\\Gamma(t) = (t-1)!\\) for integers \\(t \\in \\mathbb Z\\)\nIt is implemented by the gamma() function in R.\n\n\n\n\n\n\n\n\n\n\n\nHere are some Weibull density functions, with \\(\\lambda = 1\\) and \\(p\\) varying:\n\nShow R codelibrary(ggplot2)\nlambda = 1\nggplot() +\n  geom_function(\n    aes(col = \"0.25\"),\n    fun = \\(x) dweibull(x, shape = 0.25, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"0.5\"),\n    fun = \\(x) dweibull(x, shape = 0.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1\"),\n    fun = \\(x) dweibull(x, shape = 1, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1.5\"),\n    fun = \\(x) dweibull(x, shape = 1.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"2\"),\n    fun = \\(x) dweibull(x, shape = 2, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"5\"),\n    fun = \\(x) dweibull(x, shape = 5, scale = 1/lambda)) +\n  theme_bw() + \n  xlim(0, 2.5) +\n  ylab(\"f(t)\") +\n  theme(axis.title.y = element_text(angle=0)) +\n  theme(legend.position=\"bottom\") +\n  guides(\n    col = \n      guide_legend(\n        title = \"p\",\n        label.theme = \n          element_text(\n            size = 12)))\n\n\nDensity functions for Weibull distribution\n\n\n\n\nProperties of Weibull hazard functions\n\nWhen \\(p=1\\), the Weibull distribution simplifies to the exponential distribution\nWhen \\(p &gt; 1\\), the hazard is increasing\nWhen \\(p &lt; 1\\), the hazard is decreasing\n\nIn HW: prove these properties\nThis distribution provides more flexibility than the exponential.\nHere are some Weibull hazard functions, with \\(\\lambda = 1\\) and \\(p\\) varying:\n\nShow R codelibrary(ggplot2)\nlibrary(eha)\nlambda = 1\n\nggplot() +\n  geom_function(\n    aes(col = \"0.25\"),\n    fun = \\(x) hweibull(x, shape = 0.25, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"0.5\"),\n    fun = \\(x) hweibull(x, shape = 0.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1\"),\n    fun = \\(x) hweibull(x, shape = 1, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1.5\"),\n    fun = \\(x) hweibull(x, shape = 1.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"2\"),\n    fun = \\(x) hweibull(x, shape = 2, scale = 1/lambda)) +\n  theme_bw() + \n  xlim(0, 2.5) + \n  ylab(\"h(t)\") +\n  theme(axis.title.y = element_text(angle=0)) +\n  theme(legend.position=\"bottom\") +\n  guides(\n    col = \n      guide_legend(\n        title = \"p\",\n        label.theme = \n          element_text(\n            size = 12)))\n\n\nHazard functions for Weibull distribution\n\n\n\n\n\nShow R codelibrary(ggplot2)\nlambda = 1\n\nggplot() +\n  geom_function(\n    aes(col = \"0.25\"),\n    fun = \\(x) pweibull(lower = FALSE, x, shape = 0.25, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"0.5\"),\n    fun = \\(x) pweibull(lower = FALSE, x, shape = 0.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1\"),\n    fun = \\(x) pweibull(lower = FALSE, x, shape = 1, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1.5\"),\n    fun = \\(x) pweibull(lower = FALSE, x, shape = 1.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"2\"),\n    fun = \\(x) pweibull(lower = FALSE, x, shape = 2, scale = 1/lambda)) +\n  theme_bw() + \n  xlim(0, 2.5) + \n  ylab(\"S(t)\") +\n  theme(axis.title.y = element_text(angle=0)) +\n  theme(legend.position=\"bottom\") +\n  guides(\n    col = \n      guide_legend(\n        title = \"p\",\n        label.theme = \n          element_text(\n            size = 12)))\n\n\nSurvival functions for Weibull distribution\n\n\n\n\nExponential Regression\nFor each subject \\(i\\), define a linear predictor:\n\\[\n\\begin{aligned}\n\\eta(\\boldsymbol x) &= \\beta_0 + (\\beta_1x_1 + \\dots + \\beta_p x_p)\\\\\nh(t|\\boldsymbol x) &= \\text{exp}\\left\\{\\eta(\\boldsymbol x)\\right\\}\\\\\nh_0 &\\stackrel{\\text{def}}{=} h(t|\\boldsymbol 0)\\\\\n&= \\text{exp}\\left\\{\\eta(\\boldsymbol 0)\\right\\}\\\\\n&= \\text{exp}\\left\\{\\beta_0 + (\\beta_1 \\cdot 0 + \\dots + \\beta_p \\cdot 0)\\right\\}\\\\\n&= \\text{exp}\\left\\{\\beta_0 + 0\\right\\}\\\\\n&= \\text{exp}\\left\\{\\beta_0\\right\\}\\\\\n\\end{aligned}\n\\]\nWe let the linear predictor have a constant term, and when there are no additional predictors the hazard is \\(\\lambda = \\text{exp}\\left\\{\\beta_0\\right\\}\\). This has a log link as in a generalized linear model. Since the hazard does not depend on \\(t\\), the hazards are (trivially) proportional.\nAccelerated Failure Time\nPreviously, we assumed the hazards were proportional; that is, the covariates multiplied the baseline hazard function:\n\\[\n\\begin{aligned}\nh(T=t|X=x)\n&\\stackrel{\\text{def}}{=} p(T=t|X=x,T \\ge t)\\\\\n&= h(t|X=0)\\cdot \\text{exp}\\left\\{\\eta(x)\\right\\}\\\\\n&= h(t|X=0)\\cdot \\theta(x)\\\\\n&= h_0(t)\\cdot \\theta(x)\n\\end{aligned}\n\\]\nand correspondingly,\n\\[\n\\begin{aligned}\nH(t|x)\n&= \\theta(x)H_0(t)\\\\\nS(t|x)\n&= \\text{exp}\\left\\{-H(t|x)\\right\\}\\\\\n&= \\text{exp}\\left\\{-\\theta(x)\\cdot H_0(t)\\right\\}\\\\\n&= \\left(\\text{exp}\\left\\{- H_0(t)\\right\\}\\right)^{\\theta(x)}\\\\\n&= \\left(S_0(t)\\right)^{\\theta(x)}\\\\\n\\end{aligned}\n\\]\nAn alternative modeling assumption would be \\[S(t|X=x)=S_0(t\\cdot \\theta(x))\\] where \\(\\theta(x)=\\text{exp}\\left\\{\\eta(x)\\right\\}\\), \\(\\eta(x) =\\beta_1x_1+\\cdots+\\beta_px_p\\), and \\(S_0(t)=P(T\\ge t|X=0)\\) is the base survival function.\nThen\n\\[\n\\begin{aligned}\nE(T|X=x)\n&= \\int_{t=0}^{\\infty} S(t|x)dt\\\\\n&= \\int_{t=0}^{\\infty} S_0(t\\cdot \\theta(x))dt\\\\\n&= \\int_{u=0}^{\\infty} S_0(u)du \\cdot \\theta(x)^{-1}\\\\\n&= \\theta(x)^{-1} \\cdot \\int_{u=0}^{\\infty} S_0(u)du\\\\\n&= \\theta(x)^{-1} \\cdot \\text{E}(T|X=0)\\\\\n\\end{aligned}\n\\] So the mean of \\(T\\) given \\(X=x\\) is the baseline mean divided by \\(\\theta(x) = \\text{exp}\\left\\{\\eta(x)\\right\\}\\).\nThis modeling strategy is called an accelerated failure time model, because covariates cause uniform acceleration (or slowing) of failure times.\nAdditionally:\n\\[\n\\begin{aligned}\nH(t|x) &= H_0(\\theta(x)\\cdot t)\\\\\nh(t|x) &= \\theta(x) \\cdot h_0(\\theta(x)\\cdot t)\n\\end{aligned}\n\\]\nIf the base distribution is exponential with parameter \\(\\lambda\\) then\n\\[\n\\begin{aligned}\nS(t|x)\n&= \\text{exp}\\left\\{-\\lambda \\cdot t \\theta(x)\\right\\}\\\\\n&= [\\text{exp}\\left\\{-\\lambda t\\right\\}]^{\\theta(x)}\\\\\n\\end{aligned}\n\\]\nwhich is an exponential model with base hazard multiplied by \\(\\theta(x)\\), which is also the proportional hazards model.\n\nIn terms of the log survival time \\(Y=\\text{log}\\left\\{T\\right\\}\\) the model can be written as\n\\[\n\\begin{aligned}\nY&=\\alpha-\\eta+W\\\\\n\\alpha&= -\\text{log}\\left\\{\\lambda\\right\\}\n\\end{aligned}\n\\]\nwhere \\(W\\) has the extreme value distribution. The estimated parameter \\(\\lambda\\) is the intercept and the other coefficients are those of \\(\\eta\\), which will be the opposite sign of those for coxph.\n\nFor a Weibull distribution, the hazard function and the survival function are\n\\[\n\\begin{aligned}\nh(t)&=\\lambda p (\\lambda t)^{p-1}\\\\\nS(t)&=e^{-(\\lambda t)^p}\n\\end{aligned}\n\\]\nWe can construct a proportional hazards model by using a linear predictor \\(\\eta_i\\) without constant term and letting \\(\\theta_i=e^{\\eta_i}\\) we have\n\\[\n\\begin{aligned}\nh(t)&=\\lambda p (\\lambda t)^{p-1}\\theta_i\n\\end{aligned}\n\\]\nA distribution with \\(h(t)=\\lambda p (\\lambda t)^{p-1}\\theta_i\\) is a Weibull distribution with parameters \\(\\lambda^*=\\lambda \\theta_i^{1/p}\\) and \\(p\\) so the survival function is\n\\[\n\\begin{aligned}\nS^*(t)&=e^{-(\\lambda^* t)^p}\\\\\n&=e^{-(\\lambda \\theta^{1/p} t)^p}\\\\\n&= S(t\\theta^{1/p})\n\\end{aligned}\n\\]\nso this is also an accelerated failure time model.\n\nIn terms of the log survival time \\(Y=\\text{log}\\left\\{T\\right\\}\\) the model can be written as\n\\[\n\\begin{aligned}\nY&=\\alpha-\\sigma\\eta+\\sigma W\\\\\n\\alpha&= -\\text{log}\\left\\{\\lambda\\right\\}\\\\\n\\sigma &= 1/p\n\\end{aligned}\n\\]\nwhere \\(W\\) has the extreme value distribution. The estimated parameter \\(\\lambda\\) is the intercept and the other coefficients are those of \\(\\eta\\), which will be the opposite sign of those for coxph.\n\nThese AFT models are log-linear, meaning that the linear predictor has a log link. The exponential and the Weibull are the only log-linear models that are simultaneously proportional hazards models. Other parametric distributions can be used for survival regression either as a proportional hazards model or as an accelerated failure time model.\nDataset: Leukemia treatments\nRemission survival times on 42 leukemia patients, half on new treatment, half on standard treatment.\nThis is the same data as the drug6mp data from KMsurv, but with two other variables and without the pairing.\n\nShow R codelibrary(haven)\nlibrary(survival)\nanderson = \n  paste0(\n    \"http://web1.sph.emory.edu/dkleinb/allDatasets\",\n    \"/surv2datasets/anderson.dta\") |&gt; \n  read_dta() |&gt; \n  mutate(\n    status = status |&gt; \n      case_match(\n        1 ~ \"relapse\",\n        0 ~ \"censored\"\n      ),\n    sex = sex |&gt; \n      case_match(\n        0 ~ \"female\",\n        1 ~ \"male\"\n      ),\n    \n    rx = rx |&gt; \n      case_match(\n        0 ~ \"new\",\n        1 ~ \"standard\"\n      ),\n    \n    surv = Surv(time = survt,event = (status == \"relapse\"))\n  ) \n\nprint(anderson)\n\n\nCox semi-parametric model\n\nShow R codeanderson.cox0 = coxph(\n  formula = surv ~ rx,\n  data = anderson)\nsummary(anderson.cox0)\n\nCall:\ncoxph(formula = surv ~ rx, data = anderson)\n\n  n= 42, number of events= 30 \n\n            coef exp(coef) se(coef)    z Pr(&gt;|z|)    \nrxstandard 1.572     4.817    0.412 3.81  0.00014 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n           exp(coef) exp(-coef) lower .95 upper .95\nrxstandard      4.82      0.208      2.15      10.8\n\nConcordance= 0.69  (se = 0.041 )\nLikelihood ratio test= 16.4  on 1 df,   p=5e-05\nWald test            = 14.5  on 1 df,   p=1e-04\nScore (logrank) test = 17.2  on 1 df,   p=3e-05\n\n\nWeibull parametric model\n\nShow R codeanderson.weib &lt;- survreg(\n  formula = surv ~ rx,\n  data = anderson,\n  dist = \"weibull\")\nsummary(anderson.weib)\n\n\nCall:\nsurvreg(formula = surv ~ rx, data = anderson, dist = \"weibull\")\n             Value Std. Error     z       p\n(Intercept)  3.516      0.252 13.96 &lt; 2e-16\nrxstandard  -1.267      0.311 -4.08 4.5e-05\nLog(scale)  -0.312      0.147 -2.12   0.034\n\nScale= 0.732 \n\nWeibull distribution\nLoglik(model)= -106.6   Loglik(intercept only)= -116.4\n    Chisq= 19.65 on 1 degrees of freedom, p= 9.3e-06 \nNumber of Newton-Raphson Iterations: 5 \nn= 42 \n\n\nExponential parametric model\n\nShow R codeanderson.exp &lt;- survreg(\n  formula = surv ~ rx,\n  data = anderson,\n  dist = \"exp\")\nsummary(anderson.exp)\n\n\nCall:\nsurvreg(formula = surv ~ rx, data = anderson, dist = \"exp\")\n             Value Std. Error     z       p\n(Intercept)  3.686      0.333 11.06 &lt; 2e-16\nrxstandard  -1.527      0.398 -3.83 0.00013\n\nScale fixed at 1 \n\nExponential distribution\nLoglik(model)= -108.5   Loglik(intercept only)= -116.8\n    Chisq= 16.49 on 1 degrees of freedom, p= 4.9e-05 \nNumber of Newton-Raphson Iterations: 4 \nn= 42 \n\n\nDiagnostic - complementary log-log survival plot\n\nShow R codelibrary(survminer)\nsurvfit(\n  formula = surv ~ rx,\n  data = anderson) |&gt; \n  ggsurvplot(fun = \"cloglog\")\n\n\n\n\n\n\n\nIf the cloglog plot is linear, then a Weibull model may be ok.\n\n8.0.2 Combining left-truncation and interval-censoring\nFrom [https://stat.ethz.ch/pipermail/r-help/2015-August/431733.html]:\n\ncoxph does left truncation but not left (or interval) censoring survreg does interval censoring but not left truncation (or time dependent covariates).\n\n\nTerry Therneau, August 31, 2015",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parametric survival models</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Canchola, Alison J, Susan L Stewart, Leslie Bernstein, Dee W West,\nRonald K Ross, Dennis Deapen, Richard Pinder, et al. 2003. “Cox\nRegression Using Different Time-Scales.” Western Users of SAS\nSoftware. https://www.lexjansen.com/wuss/2003/DataAnalysis/i-cox_time_scales.pdf.\n\n\nCasella, George, and Roger Berger. 2002. Statistical Inference.\n2nd ed. Cengage Learning. https://www.cengage.com/c/statistical-inference-2e-casella-berger/9780534243128/.\n\n\nDobson, Annette J, and Adrian G Barnett. 2018. An Introduction to\nGeneralized Linear Models. 4th ed. CRC press. https://doi.org/10.1201/9781315182780.\n\n\nFieller, Nick. 2016. Basics of Matrix Algebra for Statistics with\nr. Chapman; Hall/CRC. https://doi.org/10.1201/9781315370200.\n\n\nGrambsch, Patricia M, and Terry M Therneau. 1994. “Proportional\nHazards Tests and Diagnostics Based on Weighted Residuals.”\nBiometrika 81 (3): 515–26. https://doi.org/10.1093/biomet/81.3.515.\n\n\nKlein, John P, Melvin L Moeschberger, et al. 2003. Survival\nAnalysis: Techniques for Censored and Truncated Data. Vol. 1230.\nSpringer. https://link.springer.com/book/10.1007/b97377.\n\n\nLawrance, Rachael, Evgeny Degtyarev, Philip Griffiths, Peter Trask,\nHelen Lau, Denise D’Alessio, Ingolf Griebsch, Gudrun Wallenstein, Kim\nCocks, and Kaspar Rufibach. 2020. “What Is an Estimand, and How\nDoes It Relate to Quantifying the Effect of Treatment on\nPatient-Reported Quality of Life Outcomes in Clinical Trials?”\nJournal of Patient-Reported Outcomes 4 (1): 1–8.\n\n\nMcLachlan, Geoffrey J, and Thriyambakam Krishnan. 2007. The EM\nAlgorithm and Extensions. 2nd ed. John Wiley & Sons. https://doi.org/10.1002/9780470191613.\n\n\nPohl, Moritz, Lukas Baumann, Rouven Behnisch, Marietta Kirchner,\nJohannes Krisam, and Anja Sander. 2021. “Estimands—A Basic Element for Clinical\nTrials.” Deutsches Ärzteblatt\nInternational 118 (51-52): 883–88. https://doi.org/10.3238/arztebl.m2021.0373.\n\n\nVan Buuren, Stef. 2018. Flexible Imputation of Missing Data.\nCRC press. https://stefvanbuuren.name/fimd/.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, and Charles E\nMcCulloch. 2012. Regression Methods in Biostatistics: Linear,\nLogistic, Survival, and Repeated Measures Models. 2nd ed. Springer.\nhttps://doi.org/10.1007/978-1-4614-1353-0.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "Appendix A — Probability",
    "section": "",
    "text": "A.0.1 Random variables\n\nTypes of random variables\n\nDefinition A.1 (binary variable) A binary variable is a random variable which has only two possible values in its range.\n\nExamples of binary outcomes include:\n\nexposure (exposed vs unexposed)\ndisease (diseased vs healthy)\nrecovery (recovered vs unrecovered)\nrelapse (relapse vs remission)\nreturn to hospital (returned vs not)\nvital status (dead vs alive)\n\n\n\n\nA.0.2 Characteristics of probability distributions\n\nDefinition A.2 (Density function) The density function \\(f(t)\\) for a random variable \\(T\\) at value \\(t\\) can be defined as the derivative of the cumulative probability function \\(P(T\\le t)\\); that is:\n\\[f(t) \\stackrel{\\text{def}}{=}\\frac{d}{dt} \\Pr(T\\le t)\\]\n\n\nDefinition A.3 (Hazard function) The hazard function for a random variable \\(T\\) at value \\(t\\) is the conditional density of \\(T\\) at \\(t\\), given \\(T\\ge t\\); that is:\n\\[h(t) \\stackrel{\\text{def}}{=}p(T=t|T\\ge t)\\]\nIf \\(T\\) represents the time at which an event occurs, then \\(h(t)\\) is the probability that the event occurs at time \\(t\\), given that it has not occurred prior to time \\(t\\).\n\n\nDefinition A.4 (Variance) The variance of a random variable \\(X\\) is the expectation of the squared difference between \\(X\\) and \\(\\mathbb{E}\\left[X\\right]\\); that is:\n\\[\n\\text{Var}\\left(X\\right) \\stackrel{\\text{def}}{=}\\mathbb{E}\\left[(X-\\mathbb{E}\\left[X\\right])^2\\right]\n\\]\n\n\nTheorem A.1 (Alternative expression for variance) \\[\\text{Var}\\left(X\\right)=\\mathbb{E}\\left[X^2\\right] - \\left(\\mathbb{E}\\left[X\\right]\\right)^2\\]\n\nProof. By linearity of expectation, we have:\n\\[\n\\begin{aligned}\n\\text{Var}\\left(X\\right)\n&\\stackrel{\\text{def}}{=}\\mathbb{E}\\left[(X-\\mathbb{E}\\left[X\\right])^2\\right]\\\\\n&=\\mathbb{E}\\left[X^2 - 2X\\mathbb{E}\\left[X\\right] + \\left(\\mathbb{E}\\left[X\\right]\\right)^2\\right]\\\\\n&=\\mathbb{E}\\left[X^2\\right] - \\mathbb{E}\\left[2X\\mathbb{E}\\left[X\\right]\\right] + \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[X\\right]\\right)^2\\right]\\\\\n&=\\mathbb{E}\\left[X^2\\right] - 2\\mathbb{E}\\left[X\\right]\\mathbb{E}\\left[X\\right] + \\left(\\mathbb{E}\\left[X\\right]\\right)^2\\\\\n&=\\mathbb{E}\\left[X^2\\right] - \\left(\\mathbb{E}\\left[X\\right]\\right)^2\\\\\n\\end{aligned}\n\\]\n\n\n\nDefinition A.5 (Precision) The precision of a random variable \\(X\\), often denoted \\(\\tau(X)\\), \\(\\tau_X\\), or shorthanded as \\(\\tau\\), is the inverse of that random variable’s variance; that is:\n\\[\\tau(X) \\stackrel{\\text{def}}{=}\\left(\\text{Var}\\left(X\\right)\\right)^{-1}\\]\n\n\n\nA.0.3 Key probability distributions\n\nDefinition A.6 (Bernoulli distribution) The Bernoulli distribution family for a random variable \\(X\\) is defined as:\n\\[\\Pr(X=x) = \\left\\{{\\pi, x=1}\\atop{1-\\pi, x=0}\\right.\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "estimation.html",
    "href": "estimation.html",
    "title": "Appendix B — Estimation",
    "section": "",
    "text": "B.0.1 Probabilitistic models\nWhen we perform statistical analyses, we use data to help us choose between models - specifically, to determine which models best explain that data.\nHowever, physical processes do not produce data on their own. Data is only produced when scientists implement an observation process (i.e., a scientific study), which is distinct from the underlying physical process1.\nIn order to learn about the physical processes we are ultimately interested in, we often need to make special considerations for the observation process that produced the data which we are analyzing. In particular, if some of the planned observations in the study design were not completed, we will likely need to account for the incompleteness of the resulting data set in our analysis. If we are not sure why some observations are incomplete, we may need to model the observation process in addition to the physical process we were originally interested in. For example, if some participants in a study dropped out part-way through the study, we may need investigate why those participants dropped out, as opposed to other participants who completed the study.\nThese kinds of missing data issues are outside of the scope of this course; see Van Buuren (2018) for more details.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "estimation.html#footnotes",
    "href": "estimation.html#footnotes",
    "title": "Appendix B — Estimation",
    "section": "",
    "text": "in many cases, the observation process and the physical process interact with each other.↩︎\nc.f., Pohl et al. (2021), Lawrance et al. (2020)↩︎\nhttps://en.wikipedia.org/wiki/Does_exactly_what_it_says_on_the_tin↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "intro-MLEs.html",
    "href": "intro-MLEs.html",
    "title": "Appendix C — Introduction to Maximum Likelihood Inference",
    "section": "",
    "text": "C.0.1 Maximum likelihood inference for univariate Gaussian models\nSuppose \\(X_{1},\\ldots,X_{n} \\sim_{iid}N\\left( \\mu,\\ \\sigma^{2} \\right)\\). Let \\(X = \\left( X_{1},\\ldots,X_{n} \\right)^{\\top}\\) be these random variables in vector format. Let \\(x_{i}\\) and \\(x\\) denote the corresponding observed data. Let \\(\\theta = \\left( \\mu,\\sigma^{2} \\right)^{\\top}\\) be the vector of parameters. Let \\(\\Theta\\) denote the parameters as a random vector.\nThen the log-likelihood \\(\\ell \\stackrel{\\text{def}}{=}\\ell(X;\\theta) \\stackrel{\\text{def}}{=}p\\left( X = x \\mid \\Theta = \\theta \\right)\\) is:\n\\[\n\\begin{aligned}\n\\ell\n&\\propto - \\frac{n}{2}\\text{log}\\left\\{\\sigma^{2}\\right\\} - \\frac{1}{2}\\sum_{i = 1}^{n}\\frac{\\left( x_{i} - \\mu \\right)^{2}}{\\sigma^{2}}\\\\\n&= - \\frac{n}{2}\\text{log}\\left\\{\\sigma^{2}\\right\\} - \\frac{1}{2\\sigma^{2}}\\sum_{i = 1}^{n}{x_{i}^{2} - 2x_{i}\\mu + \\mu^{2}}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Introduction to Maximum Likelihood Inference</span>"
    ]
  },
  {
    "objectID": "intro-MLEs.html#footnotes",
    "href": "intro-MLEs.html#footnotes",
    "title": "Appendix C — Introduction to Maximum Likelihood Inference",
    "section": "",
    "text": "I might sometimes switch the order of \\(x,\\) \\(\\theta\\); this is unintentional and not meaningful.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Introduction to Maximum Likelihood Inference</span>"
    ]
  },
  {
    "objectID": "common-mistakes.html",
    "href": "common-mistakes.html",
    "title": "Appendix D — Common Mistakes",
    "section": "",
    "text": "D.0.1 Parameters versus random variables\nThe parameters of a probability distribution shouldn’t involve the random variables being modeled:\n\n\n\n\n\n\nThis is wrong\n\n\n\n\\[X \\sim Pois(\\lambda)\\] \\[\\hat{\\lambda}_{ML} \\rightarrow_D N(\\bar{X}, \\lambda/n)\\]\n\n\n\nSolution. \\[\\hat{\\lambda}_{ML} \\rightarrow_D N(\\lambda, \\lambda/n)\\]\n\nExpectations are means, not sums, despite the similarity of \\(\\Sigma\\) and \\(\\text{E}\\). Really, we should use \\(\\mu\\) instead of \\(\\text{E}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Common Mistakes</span>"
    ]
  }
]