[
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Regression Models for Epidemiology",
    "section": "Preface",
    "text": "Preface\nThis web-book is derived from my lecture slides for the Spring 2023 session of Epidemiology 204: “Quantitative Epidemiology III: Statistical Models”, at UC Davis.\nI have drawn these materials from many sources, including but not limited to:\n\nDavid Rocke’s materials from the 2021 edition of Epi 204\nRegression methods in biostatistics: linear, logistic, survival, and repeated measures models, 2nd edition (Vittinghoff et al. 2012)\nAn Introduction to Generalized Linear Models, 4th edition (Dobson and Barnett 2018)\n\n\nLicense\nThis book is licensed to you under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.\nThe code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e. public domain.\n\n\n\n\n\n\nDobson, Annette J, and Adrian G Barnett. 2018. An Introduction to Generalized Linear Models. 4th ed. CRC press. https://doi.org/10.1201/9781315182780.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. 2nd ed. Springer. https://doi.org/10.1007/978-1-4614-1353-0."
  },
  {
    "objectID": "intro-to-GLMs.html#introduction-to-epi-204",
    "href": "intro-to-GLMs.html#introduction-to-epi-204",
    "title": "\n1  Introduction\n",
    "section": "\n1.1 Introduction to Epi 204",
    "text": "1.1 Introduction to Epi 204\nWelcome to Epidemiology 204: Quantitative Epidemiology III (Statistical Models).\nIn this course, we will start where Epi 203 left off: with linear regression models.\nIf you haven’t taken Epi 203 or a similar introduction to mathematical statistical inference, please talk to me after class.\n\n1.1.1 What you should know\nEpi 202: probability models for different data types\n\nbinomial\nPoisson\nGaussian\nexponential\n\nEpi 203: inference for one or several homogenous populations\n\nthe maximum likelihood inference framework:\n\nlikelihood functions\nlog-likelihood functions\nscore functions\nestimating equations\ninformation matrices\npoint estimates\nstandard errors\nconfidence intervals\nhypothesis tests\np-values\n\n\nHypothesis tests for one, two, and &gt;2 groups:\n\nt-tests/ANOVA for Gaussian models\nchi-square tests for binomial and Poisson models\n\n\nSome linear regression\n\nStat 108: linear regression models\n\nbuilding models for Gaussian outcomes\n\nmultiple predictors\ninteractions\n\n\nregression diagnostics\nfundamentals of R programming; e.g.:\n\nR for Data Science (Wickham, Cetinkaya-Rundel, Grolemund 2023)\nIntroductory Statistics with R (Dalgaard 2008)\n\n\nRMarkdown or Quarto for formatting homework\nLaTeX for writing math in RMarkdown/Quarto\n\n1.1.2 What we will cover in this course\n\nLinear (Gaussian) regression models (review and more details)\n\nRegression models for non-Gaussian outcomes\n\nbinary\ncount\ntime to event\n\n\nStatistical analysis using R"
  },
  {
    "objectID": "intro-to-GLMs.html#regression-models",
    "href": "intro-to-GLMs.html#regression-models",
    "title": "\n1  Introduction\n",
    "section": "\n1.2 Regression models",
    "text": "1.2 Regression models\nWhy do we need them?\n\ncontinuous predictors\nnot enough data to analyze some subgroups individually\n\n\n1.2.1 Example: Adelie penguins\n\nCodelibrary(ggplot2)\nlibrary(plotly)\nlibrary(dplyr)\nggpenguins &lt;- \n  palmerpenguins::penguins |&gt; \n  dplyr::filter(species == \"Adelie\") |&gt; \n  ggplot(\n    aes(x = bill_length_mm , y = body_mass_g)) +\n  geom_point() + \n  xlab(\"Bill length (mm)\") + \n  ylab(\"Body mass (g)\")\n\n\n\n\n\nPalmer penguins\n\n\n\n1.2.2 Linear regression\n\nCodeggpenguins2 = \n  ggpenguins +\n  stat_smooth(method = \"lm\",\n              formula = y ~ x,\n              geom = \"smooth\")\n\n\n\n\n\nPalmer penguins with linear regression fit\n\n\n\n1.2.3 Curved regression lines\n\nCodeggpenguins2 = ggpenguins +\n  stat_smooth(\n    method = \"lm\",\n    formula = y ~ log(x),\n    geom = \"smooth\") +\n  xlab(\"Bill length (mm)\") + \n  ylab(\"Body mass (g)\")\n\n\n\n\n\n\n\n\n\n1.2.4 Multiple regression\n\nCodeggpenguins =\n  palmerpenguins::penguins |&gt; \n  ggplot(\n    aes(x = bill_length_mm , \n        y = body_mass_g,\n        color = species\n    )\n  ) +\n  geom_point() +\n  stat_smooth(\n    method = \"lm\",\n    formula = y ~ x,\n    geom = \"smooth\") +\n  xlab(\"Bill length (mm)\") + \n  ylab(\"Body mass (g)\")\n\n\n\n\n\n\n\n\n\n1.2.5 Modeling non-Gaussian outcomes\n\nCodelibrary(glmx)\ndata(BeetleMortality)\nbeetles = BeetleMortality |&gt;\n  mutate(\n    pct = died/n,\n    survived = n - died\n  )\n\nplot1 = \n  beetles |&gt; \n  ggplot(aes(x = dose, y = pct)) +\n  geom_point(aes(size = n)) +\n  xlab(\"Dose (log mg/L)\") +\n  ylab(\"Mortality rate (%)\") +\n  scale_y_continuous(labels = scales::percent) +\n  # xlab(bquote(log[10]), bquote(CS[2])) +\n  scale_size(range = c(1,2))\n\n\n\n\n\nMortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\n\n1.2.6 Why don’t we use linear regression?\n\nCodebeetles_long = \n  beetles  |&gt; \n  reframe(.by = everything(),\n          outcome = c(\n            rep(1, times = died), \n            rep(0, times = survived))\n  )\n\nlm1 = \n  beetles_long |&gt; \n  lm(\n    formula = outcome ~ dose, \n    data = _)\n\n\nrange1 = range(beetles$dose) + c(-.2, .2)\n\nf.linear = function(x) predict(lm1, newdata = data.frame(dose = x))\n\nplot2 = \n  plot1 + \n  geom_function(fun = f.linear, aes(col = \"Straight line\")) +\n  labs(colour=\"Model\", size = \"\")\n\n\n\n\n\nMortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\n\n1.2.7 Zoom out\n\n\n\nMortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\n\n1.2.8 log transformation of dose?\n\nCodelm2 = \n  beetles_long |&gt; \n  lm(formula = outcome ~ log(dose), data = _)\n\nf.linearlog = function(x) predict(lm2, newdata = data.frame(dose = x))\n\nplot3 = plot2 + \n  expand_limits(x = c(1.6, 2)) +\n  geom_function(fun = f.linearlog, aes(col = \"Log-transform dose\"))\n\n\n\n\n\nMortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\n\n1.2.9 Logistic regression\n\nCodeglm1 = beetles |&gt; \n  glm(formula = cbind(died, survived) ~ dose, family = \"binomial\")\n\nf = function(x) predict(glm1, newdata = data.frame(dose = x), type = \"response\")\n\nplot4 = plot3 + geom_function(fun = f, aes(col = \"Logistic regression\"))\n\n\n\n\n\nMortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\n\n1.2.10 Three parts to regression models\n\nWhat distribution does the outcome have for a specific subpopulation defined by covariates? (outcome model)\nHow does the combination of covariates relate to the mean? (link function)\nHow do the covariates combine? (linear predictor, interactions)"
  },
  {
    "objectID": "glms.html",
    "href": "glms.html",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "This section is primarily adapted starting from the textbook “An Introduction to Generalized Linear Models” (4th edition, 2018) by Annette J. Dobson and Adrian G. Barnett:\nhttps://doi.org/10.1201/9781315182780\nThe type of predictive model one uses depends on several issues; one is the type of response.\n\nMeasured values such as quantity of a protein, age, weight usually can be handled in an ordinary linear regression model, possibly after a log transformation.\nPatient survival, which may be censored, calls for a different method (survival analysis, Cox regression).\nIf the response is binary, then can we use logistic regression models\nIf the response is a count, we can use Poisson regression\nIf the count has a higher variance than is consistent with the Poisson, we can use a negative binomial or over-dispersed Poisson\nOther forms of response can generate other types of generalized linear models\n\nWe need a linear predictor of the same form as in linear regression βx. In theory, such a linear predictor can generate any type of number as a prediction, positive, negative, or zero\nWe choose a suitable distribution for the type of data we are predicting (normal for any number, gamma for positive numbers, binomial for binary responses, Poisson for counts)\nWe create a link function which maps the mean of the distribution onto the set of all possible linear prediction results, which is the whole real line (-∞, ∞). The inverse of the link function takes the linear predictor to the actual prediction.\n\nOrdinary linear regression has identity link (no transformation by the link function) and uses the normal distribution\nIf one is predicting an inherently positive quantity, one may want to use the log link since ex is always positive.\nAn alternative to using a generalized linear model with a log link, is to transform the data using the log. This is a device that works well with measurement data and may be usable in other cases, but it cannot be used for 0/1 data or for count data that may be 0.\n\n\nR glm() Families\n\n\n\n\n\n\nFamily\nLinks\n\n\n\n\ngaussian\nidentity, log, inverse\n\n\nbinomial\nlogit, probit, cauchit, log, cloglog\n\n\ngamma\ninverse, identity, log\n\n\ninverse.gaussian\n1/mu^2, inverse, identity, log\n\n\nPoisson\nlog, identity, sqrt\n\n\nquasi\nidentity, logit, probit, cloglog, inverse, log, 1/mu^2 and sqrt\n\n\nquasibinomial\nlogit, probit, identity, cloglog, inverse, log, 1/mu^2 and sqrt\n\n\nquasipoisson\nlog, identity, logit, probit, cloglog, inverse, 1/mu^2 and sqrt\n\n\n\n\nR glm() Link Functions; \\(\\eta = X\\beta = g(\\mu)\\)\n\n\n\n\n\n\n\n\n\nName\nDomain\nRange\nLink Function\nInverse Link Function\n\n\n\n\nidentity\n\\((-\\infty, \\infty)\\)\n\\((-\\infty, \\infty)\\)\n\\(\\eta = \\mu\\)\n\\(\\mu = \\eta\\)\n\n\nlog\n\\((0,\\infty)\\)\n\\((-\\infty, \\infty)\\)\n\\(\\eta = \\log{\\mu}\\)\n\\(\\mu = \\exp{\\eta}\\)\n\n\ninverse\n\\((0, \\infty)\\)\n\\((0,\\infty)\\)\n\\(\\eta = 1/\\mu\\)\n\\(\\mu = 1/\\eta\\)\n\n\nlogit\n\\((0,1)\\)\n\\((-\\infty, \\infty)\\)\n\\(\\eta = \\log{\\mu/(1-\\mu)}\\)\n\\(\\mu = exp{\\eta}/(1+exp{\\eta})\\)\n\n\nprobit\n\\((0,1)\\)\n\\((-\\infty, \\infty)\\)\n\\(\\eta = \\Phi^-1{\\mu}\\)\n\\(\\mu = \\Phi(\\eta)\\)\n\n\ncloglog\n\\((0,1)\\)\n\\((-\\infty, \\infty)\\)\n\\(\\eta = \\log{-\\log{1-\\mu}}\\)\n\\(\\mu = {1-\\exp{-\\exp{\\eta}}}\\)\n\n\n1/mu^2\n\\((0,\\infty)\\)\n\\((0, \\infty)\\)\n\\(\\eta = 1/\\mu^2\\)\n\\(\\mu = 1/\\sqrt(\\eta)\\)\n\n\nsqrt\n\\((0,\\infty)\\)\n\\((0,\\infty)\\)\n\\(\\eta = \\sqrt{\\mu}\\)\n\\(\\mu = \\eta^2\\)"
  },
  {
    "objectID": "Linear-models-overview.html#understanding-gaussian-linear-regression-models",
    "href": "Linear-models-overview.html#understanding-gaussian-linear-regression-models",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.1 Understanding Gaussian Linear Regression Models",
    "text": "2.1 Understanding Gaussian Linear Regression Models\n\n2.1.1 Motivating example: birthweights and gestational age\nSuppose we want to learn about the distributions of birthweights for (human) babies born at different gestational ages and with different chromosomal sexes (Dobson and Barnett, Example 2.2.2):\n\nCodedata(\"birthweight\", package = \"dobson\")\n\nbw = \n  birthweight |&gt; \n  pivot_longer(\n    cols = everything(),\n    names_to = c(\"sex\", \".value\"),\n    names_sep = \"s \"\n  ) |&gt; \n  mutate(\n    sex = ifelse(sex == \"boy\", \"male\", \"female\"),\n    male = (sex == \"male\") |&gt; as.integer())  |&gt; \n  rename(age = `gestational age`)\n\n\n\nCodeplot1 = bw |&gt; \n  ggplot(aes(\n    x = age, \n    y = weight,\n    linetype = sex,\n    shape = sex,\n    col = sex))  +\n  theme_bw() +\n  xlab(\"Gestational age (weeks)\") +\n  ylab(\"Birthweight (grams)\") +\n  # expand_limits(y = 0, x = 0) +\n  geom_point(alpha = .7)\n\n\n\nCodeggplotly(plot1 + facet_wrap(~ sex))\n\n\n\n\n\n\n2.1.2 Parallel lines regression\nWe don’t have enough data to model the distribution of birth weight separately for each combination of gestational age and sex, so let’s instead consider a (relatively) simple model for how that distribution varies with gestational age and sex.\nNotation\nLet:\n\n\n\\(Y\\) represent birthweight (measured in grams)\n\n\\(X_1\\) represent chromosomal sex:\n\n\n\\(X_1 = 0\\) if female (XX)\n\n\\(X_1 = 1\\) if male (XY)\n\n\n\n\\(X_2\\) represent gestational age at birth (measured in weeks).\n\n\n\n\n\n\n\nNote\n\n\n\nFemale is the reference level for the categorical variable \\(X_1\\) (chromosomal sex). The choice of a reference level is arbitrary and does not limit what we can do with the resulting model; it only makes it more computationally convenient to make inferences about comparisons involving that reference group.\n\n\nNow, consider the following model:\n\\[Y \\sim N(\\mu(X_1,X_2), \\sigma^2)\\]\n\\[\\mu(X_1,X_2)\\stackrel{\\text{def}}{=}E[Y|X_1, X_2] = \\beta_0 + \\beta_1 X_1+ \\beta_2 X_2\\]\nImplementing the Model in R\nHere’s how we can implement this model in R:\n\nCodebw_lm1 = lm(\n  formula = weight ~ sex + age, \n  data = bw)\n\nbw_lm1 |&gt; \n  parameters(show_sigma = TRUE) |&gt; \n  print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(21)\np\n\n\n\n(Intercept)\n-1773.32\n794.59\n(-3425.75, -120.89)\n-2.23\n0.037\n\n\nsex (male)\n163.04\n72.81\n(11.63, 314.45)\n2.24\n0.036\n\n\nage\n120.89\n20.46\n(78.34, 163.45)\n5.91\n&lt; .001\n\n\n\n\n\nHere’s how this model looks, superimposed on the data:\n\nCodebw = \n  bw |&gt; \n  mutate(`E[Y|X=x]` = fitted(bw_lm1)) |&gt; \n  arrange(sex, age)\n\nplot2 = \n  plot1 %+% bw +\n  geom_line(aes(y = `E[Y|X=x]`))\n\n\n\nCodeggplotly(plot2)\n\n\nParallel-slopes model of birthweight\n\n\nModel assumptions and predictions\nTo learn what this model is assuming, let’s plug in a few values.\nAccording to this model, what’s the mean birthweight for a female born at 36 weeks?\n\nCodepred_female = coef(bw_lm1)[\"(Intercept)\"] + coef(bw_lm1)[\"age\"]*36\n\n# print(pred_female)\n## built-in prediction: \n# predict(bw_lm1, newdata = tibble(sex = \"female\", age = 36))\n\n\n\\[E[Y|X_1 = 0, X_2 = 36] = \\beta_0 + \\beta_1 \\cdot 0+ \\beta_2 \\cdot 36 =  2578.873934\\]\nWhat’s the mean birthweight for a male born at 36 weeks?\n\nCodepred_male = \n  coef(bw_lm1)[\"(Intercept)\"] + \n  coef(bw_lm1)[\"sexmale\"] + \n  coef(bw_lm1)[\"age\"]*36\n\n\n\\[\nE[Y|X_1 = 1, X_2 = 36] = \\beta_0 + \\beta_1 \\cdot 1+ \\beta_2 \\cdot 36 =  2741.91323693\n\\]\nWhat’s the difference in mean birthweights between males born at 36 weeks and females born at 36 weeks?\n\\[\n\\begin{aligned}\n& E[Y|X_1 = 1, X_2 = 36] - E[Y|X_1 = 0, X_2 = 36]\\\\\n&=\n2741.91323693 - 2578.873934\\\\\n&=\n163.03930293\n\\end{aligned}\n\\]\nShortcut:\n\\[\n\\begin{aligned}\n& E[Y|X_1 = 1, X_2 = 36] -\nE[Y|X_1 = 0, X_2 = 36]\\\\\n&= (\\beta_0 + \\beta_1 \\cdot 1+ \\beta_2 \\cdot 36) - (\\beta_0 + \\beta_1 \\cdot 0+ \\beta_2 \\cdot 36) \\\\\n&= \\beta_1 \\\\\n&=  163.03930293\n\\end{aligned}\n\\]\nNote that age doesn’t show up in this difference: in other words, according to this model, the difference between females and males with the same gestational age is the same for every age.\nThat’s an assumption of the model; it’s built-in to the parametric structure, even before we plug in the estimated values of those parameters.\nThat’s why the lines are parallel.\n\n2.1.3 Interactions\nWhat if we don’t like that parallel lines assumption?\nThen we need to allow an “interaction” between age and sex:\n\\[\nE[Y|X_1, X_2] = \\beta_0 + \\beta_1 X_1+ \\beta_2 X_2 + \\beta_3 (X_1 \\cdot X_2)\n\\]\n\nCodebw_lm2 = lm(weight ~ sex + age + sex:age, data = bw)\n\n\nHere are the estimated parameters (\\(\\beta\\)s):\n\nCodebw_lm2 |&gt; \n  parameters() |&gt; \n  print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\n\n(Intercept)\n-2141.67\n1163.60\n(-4568.90, 285.56)\n-1.84\n0.081\n\n\nsex (male)\n872.99\n1611.33\n(-2488.18, 4234.17)\n0.54\n0.594\n\n\nage\n130.40\n30.00\n(67.82, 192.98)\n4.35\n&lt; .001\n\n\nsex (male) × age\n-18.42\n41.76\n(-105.52, 68.68)\n-0.44\n0.664\n\n\n\n\n\nHere’s another way we could rewrite this model (by collecting terms involving \\(X_2\\)):\n\\[\nE[Y|X_1, X_2] = \\beta_0 + \\beta_1 X_1+ (\\beta_2 + \\beta_3 X_1) X_2\n\\]\n\n\n\n\n\n\nNote\n\n\n\nIf you want to understand a coefficient in a model with interactions, collect terms for the corresponding variable, and you will see what other variables are interacting with the variable you are interested in.\n\n\nIn this case, the coefficient \\(X_2\\) is interacting with \\(X_1\\). So the slope of \\(Y\\) with respect to \\(X_2\\) depends on the value of \\(X_2\\).\nAccording to this model, there is no such thing as “the slope of birthweight with respect to age”. There are two slopes, one for each sex.1 We can only talk about “the slope of birthweight with respect to age among males” and “the slope of birthweight with respect to age among females”.\nThen: that coefficient is the difference in means per unit change in its corresponding coefficient, when the other collected variables are set to 0.\nHere’s how this model looks, superimposed on the data:\n\nCodebw = \n  bw |&gt; \n  mutate(\n    predlm2 = predict(bw_lm2)\n  ) |&gt; \n  arrange(sex, age)\n\nplot1_interact = \n  plot1 %+% bw +\n  geom_line(aes(y = predlm2))\n\n\n\nCodeggplotly(plot1_interact)\n\n\n\n\n\nNow we can see that the lines aren’t parallel.\nTo learn what this model is assuming, let’s plug in a few values.\nAccording to this model, what’s the mean birthweight for a female born at 36 weeks?\n\nCodepred_female = coef(bw_lm2)[\"(Intercept)\"] + coef(bw_lm2)[\"age\"]*36\n\n\n\\[E[Y|X_1 = 0, X_2 = 36] = \\beta_0 + \\beta_1 \\cdot 0+ \\beta_2 \\cdot 36 + \\beta_3 \\cdot (0 * 36) =  2552.73333333\\] What’s the mean birthweight for a male born at 36 weeks?\n\nCodepred_male = \n  coef(bw_lm2)[\"(Intercept)\"] + \n  coef(bw_lm2)[\"sexmale\"] + \n  coef(bw_lm2)[\"age\"]*36 + \n  coef(bw_lm2)[\"sexmale:age\"] * 36\n\n\n\\[E[Y|X_1 = 0, X_2 = 36] = \\beta_0 + \\beta_1 \\cdot 1+ \\beta_2 \\cdot 36 + \\beta_3 \\cdot 1 \\cdot 36 =  2762.70689655\\]\nWhat’s the difference in mean birthweights between males born at 36 weeks and females born at 36 weeks?\n\\[\n\\begin{aligned}\n& E[Y|X_1 = 1, X_2 = 36] - E[Y|X_1 = 0, X_2 = 36]\\\\\n&= (\\beta_0 + \\beta_1 \\cdot 1+ \\beta_2 \\cdot 36 + \\beta_3 \\cdot 1 \\cdot 36)\\\\\n&\\ \\ \\ \\ \\  -(\\beta_0 + \\beta_1 \\cdot 0+ \\beta_2 \\cdot 36 + \\beta_3 \\cdot 0 \\cdot 36) \\\\\n&= \\beta_2 + \\beta_3\\cdot 36\\\\\n&=  209.97356322\n\\end{aligned}\n\\]\nNote that age now does show up in the difference: in other words, according to this model, the difference in mean birthweights between females and males with the same gestational age can vary by gestational age.\nThat’s how the lines in the graph ended up non-parallel.\n\n2.1.4 Stratified regression\nWe could re-write the interaction model as a stratified model, with a slope and intercept for each sex:\n\nCodebw_lm_strat = \n  bw |&gt; \n  lm(\n    formula = weight ~ sex + sex:age - 1, \n    data = _)\n\nbw_lm_strat |&gt; \n  parameters() |&gt; \n  print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\n\nsex (female)\n-2141.67\n1163.60\n(-4568.90, 285.56)\n-1.84\n0.081\n\n\nsex (male)\n-1268.67\n1114.64\n(-3593.77, 1056.42)\n-1.14\n0.268\n\n\nsex (female) × age\n130.40\n30.00\n(67.82, 192.98)\n4.35\n&lt; .001\n\n\nsex (male) × age\n111.98\n29.05\n(51.39, 172.57)\n3.86\n&lt; .001\n\n\n\n\n\n\n2.1.5 Curved-line regression\nIf we transform some of our covariates (\\(X\\)s) and plot the resulting model on the original covariate scale, we end up with curved regression lines:\n\nCodebw_lm3 = lm(weight ~ sex:log(age) - 1, data = bw)\nlibrary(palmerpenguins)\n\nggpenguins &lt;- \n  palmerpenguins::penguins |&gt; \n  dplyr::filter(species == \"Adelie\") |&gt; \n  ggplot(\n    aes(x = bill_length_mm , y = body_mass_g)) +\n  geom_point() + \n  xlab(\"Bill length (mm)\") + \n  ylab(\"Body mass (g)\")\n\nggpenguins2 = ggpenguins +\n  stat_smooth(\n    method = \"lm\",\n              formula = y ~ log(x),\n              geom = \"smooth\") +\n  xlab(\"Bill length (mm)\") + \n  ylab(\"Body mass (g)\")\n\n\n\nCodeggpenguins2 |&gt;  ggplotly()"
  },
  {
    "objectID": "Linear-models-overview.html#estimating-linear-models-via-maximum-likelihood",
    "href": "Linear-models-overview.html#estimating-linear-models-via-maximum-likelihood",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.2 Estimating Linear Models via Maximum Likelihood",
    "text": "2.2 Estimating Linear Models via Maximum Likelihood\n\n2.2.1 Review of one-sample inference\nPreviously, we learned how to fit outcome-only models of the form \\(p(X=x|\\theta)\\) to iid data \\(\\mathbf x = (x_1,…,x_n)\\) using maximum likelihood estimation:\n\\[\\mathcal L(\\mathbf x|\\theta) = p(X_1 = x_1, …,X_n =x_n|\\theta) = \\prod_{i=1}^n p(X=x_i|\\theta)\\]\n\\[\\ell(x|\\theta) = \\text{log}\\left\\{\\mathcal L(x|\\theta)\\right\\}\\]\n\\[\\hat \\theta_{ML} = \\arg \\max_\\theta \\ell(x|\\theta)\\]\nWe learned how to quantify our uncertainty about these maximum likelihood estimates; with sufficient sample size, \\(\\hat \\theta_{ML}\\) has the approximate distribution:\n\\[\n\\hat\\theta_{ML} \\dot \\sim N(\\theta,\\mathcal I(\\theta)^{-1})\n\\]\nFor models in the “exponential family” of distributions, which includes the Gaussian, Poisson, Bernoulli, Binomial, Exponential, and Gamma distributions, \\(\\mathcal I(\\theta) = \\text -E[\\mathcal{l}''(X|\\theta)]\\), so we estimated \\(\\mathcal I(\\theta)\\) using either \\(\\mathcal I(\\theta)|_{\\theta = \\hat \\theta_{ML}}\\) or \\(\\mathcal{l}''(\\mathbf x |\\theta)|_{\\theta = \\hat \\theta_{ML}}\\).\nThen an asymptotic approximation of a 95% confidence interval for \\(\\theta_k\\) is\n\\[\\hat \\theta_{ML} \\pm z_{0.975} \\times \\left[\\left(\\hat{\\mathcal I}(\\hat \\theta_{ML})\\right)^{-1}\\right]_{kk}\\]\nwhere \\(z_\\beta\\) the \\(\\beta\\) quantile of the standard Gaussian distribution.\n\n2.2.2 MLEs for Linear Regression\nLet’s use maximum likelihood again:\n\\[\n\\mathcal L(\\mathbf y|\\mathbf x,\\beta, \\sigma^2) =\n\\prod_{i=1}^n (2\\pi\\sigma^2)^{-1/2}\n\\text{exp}\\left\\{-\\frac{1}{2\\sigma^2}(y_i - x_i'\\beta)^2\\right\\}\n\\]\n\\[\n\\ell(\\mathbf y|\\mathbf x,\\beta, \\sigma^2) \\propto -\\frac{n}{2}\\text{log}\\left\\{\\sigma^2\\right\\} - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - x_i' \\beta)^2\n\\]\n\\[\n\\ell'(\\mathbf y|\\mathbf x,\\beta, \\sigma^2) \\propto -\\frac{n}{2}\\text{log}\\left\\{\\sigma^2\\right\\} - \\frac{1}{2\\sigma^2}\\frac{d}{d\\beta}\\left(\\sum_{i=1}^n (y_i - x_i' \\beta)^2\\right)\n\\]\nA few tools from linear algebra will make this analysis go easier (see Fieller, Section 7.2 for details).\n\\[\nf_{\\beta}(\\mathbf x) = (f_{\\beta}(x_1), f_{\\beta}(x_2), ..., f_{\\beta}(x_n))^\\top\n\\]\nLet \\(\\mathbf x\\) and \\(\\beta\\) be vectors of length \\(p\\), or in other words, matrices of length \\(p\\times 1\\):\n\\[\nx = \\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\n\\vdots \\\\\nx_{p}\n\\end{bmatrix} \\\\\n\\beta = \\begin{bmatrix}\n\\beta_{1} \\\\\n\\beta_{2} \\\\\n\\vdots \\\\\n\\beta_{p}\n\\end{bmatrix}\n\\]\nThen\n\\[\nx' \\equiv x^\\top \\equiv [x_1, x_2, ..., x_p]\n\\]\nand\n\\[\nx'\\beta = [x_1, x_2, ..., x_p]\n\\begin{bmatrix}\n\\beta_{1} \\\\\n\\beta_{2} \\\\\n\\vdots \\\\\n\\beta_{p}\n\\end{bmatrix} =\nx_1\\beta_1+x_2\\beta_2 +...+x_p \\beta_p\n\\]\nIf \\(f(\\beta)\\) is a function that takes \\(\\beta\\) as input and outputs a scalar, such as \\(f(\\beta) = x'\\beta\\), then:\n\\[\n\\frac{d}{d \\beta} f(\\beta)=\n\\begin{bmatrix}\n\\frac{d}{d\\beta_1}f(\\beta) \\\\\n\\frac{d}{d\\beta_2}f(\\beta) \\\\\n\\vdots \\\\\n\\frac{d}{d\\beta_p}f(\\beta)\n\\end{bmatrix}\n\\]\nIn particular, if \\(f(\\beta) = x'\\beta\\), then:\n\\[\n\\frac{d}{d \\beta} x'\\beta=\n\\begin{bmatrix}\n\\frac{d}{d\\beta_1}(x_1\\beta_1+x_2\\beta_2 +...+x_p \\beta_p ) \\\\\n\\frac{d}{d\\beta_2}(x_1\\beta_1+x_2\\beta_2 +...+x_p \\beta_p ) \\\\\n\\vdots \\\\\n\\frac{d}{d\\beta_p}(x_1\\beta_1+x_2\\beta_2 +...+x_p \\beta_p )\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\n\\vdots \\\\\nx_{p}\n\\end{bmatrix}\n= \\mathbf x\n\\]\nIn general:\n\\[\n\\frac{d}{d\\beta} x'\\beta = x\n\\]\nThis looks a lot like non-vector calculus, except that you have to transpose the coefficient.\nSimilarly,\n\\[\n\\frac{d}{d\\beta} \\beta'\\beta = 2\\beta\n\\]\nThis is like taking the derivative of \\(x^2\\).\nAnd finally, if \\(S\\) is a \\(p\\times p\\) matrix, then:\n\\[\n\\frac{d}{d\\beta} \\beta'S\\beta = 2S\\beta\n\\]\nAgain, this is like taking the derivative of \\(cx^2\\) with respect to \\(c\\) in non-vector calculus.\nThus:\n\\[\n\\sum_{i=1}^n (y_i - f_\\beta(x_i))^2 = (\\mathbf y - X\\beta)'(\\mathbf y - X\\beta)\n\\]\n\\[\n(\\mathbf y - X\\beta)' = (\\mathbf y' - (X\\beta)') = (\\mathbf y' - \\beta'X')\n\\]\nSo\n\\[\n\\begin{aligned}\n(\\mathbf y - X\\beta)'(\\mathbf y - X\\beta) &= (\\mathbf y' - \\beta'X')(\\mathbf y - X\\beta)\\\\\n&= y'y - \\beta'X'y - y'X\\beta +\\beta'X'X\\beta\\\\\n&= y'y - 2y'X\\beta +\\beta'X'X\\beta\n\\end{aligned}\n\\]\nSo\n\\[\n\\begin{aligned}\n\\frac{d}{d\\beta}\\left(\\sum_{i=1}^n (y_i - x_i' \\beta)^2\\right) &=\n\\frac{d}{d\\beta}(\\mathbf y - X\\beta)'(\\mathbf y - X\\beta)\\\\\n&= \\frac{d}{d\\beta} (y'y - 2y'X\\beta +\\beta'X'X\\beta)\\\\\n&= (- 2X'y +2X'X\\beta)\n\\end{aligned}\n\\]\nSo if \\(\\ell(\\beta,\\sigma^2) =0\\), then\n\\[\n\\begin{aligned}\n0 &= (- 2X'y +2X'X\\beta)\\\\\n2X'y &= 2X'X\\beta\\\\\nX'y &= X'X\\beta\\\\\n(X'X)^{-1}X'y &= \\beta\n\\end{aligned}\n\\]\nThe second derivative matrix \\(\\ell_{\\beta, \\beta'} ''(\\beta, \\sigma^2;\\mathbf X,\\mathbf y)\\) is negative definite at \\(\\beta = (X'X)^{-1}X'y\\), so \\(\\hat \\beta_{ML} = (X'X)^{-1}X'y\\) is the MLE for \\(\\beta\\).\nSimilarly (not shown):\n\\[\n\\hat\\sigma^2_{ML} = \\frac{1}{n} (Y-X\\hat\\beta)'(Y-X\\hat\\beta)\n\\]\nAnd\n\\[\n\\begin{aligned}\n\\mathcal I_{\\beta} &= E[-\\ell_{\\beta, \\beta'} ''(Y|X,\\beta, \\sigma^2)]\\\\\n&= \\frac{1}{\\sigma^2}X'X\n\\end{aligned}\n\\]\nSo:\n\\[\nVar(\\hat \\beta) \\approx (\\mathcal I_{\\beta})^{-1} = \\sigma^2 (X'X)^{-1}\n\\]\nand\n\\[\n\\hat\\beta \\dot \\sim N(\\beta, \\mathcal I_{\\beta}^{-1})\n\\] These are all results you have hopefully seen before, and in the Gaussian linear regression case they are exact, not just approximate.\nIn our model 2 above, this matrix is:\n\nCodebw_lm2 |&gt; vcov()\n\n\n\n\n(Intercept)\nsexmale\nage\nsexmale:age\n\n\n\n(Intercept)\n1353968\n-1353968\n-34871.0\n34871.0\n\n\nsexmale\n-1353968\n2596387\n34871.0\n-67211.0\n\n\nage\n-34871\n34871\n899.9\n-899.9\n\n\nsexmale:age\n34871\n-67211\n-899.9\n1743.5\n\n\n\n\n\nNote that if we take the square roots of the diagonals, we get the standard errors listed in the model output:\n\nCodebw_lm2 |&gt; vcov() |&gt; diag() |&gt; sqrt()\n\n(Intercept)     sexmale         age sexmale:age \n    1163.60     1611.33       30.00       41.76 \n\n\n\nCodebw_lm2 |&gt; summary() |&gt; coef() |&gt; pander()\n\n\n\n\n\n\n\n\n\n\n \nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n(Intercept)\n-2,142\n1,164\n-1.841\n0.08057\n\n\nsexmale\n873\n1,611\n0.5418\n0.594\n\n\nage\n130.4\n30\n4.347\n0.0003127\n\n\nsexmale:age\n-18.42\n41.76\n-0.4411\n0.6639\n\n\n\n\n\nSo we can do confidence intervals, hypothesis tests, and p-values exactly as in the one-variable case we looked at previously."
  },
  {
    "objectID": "Linear-models-overview.html#inference-about-gaussian-linear-regression-models",
    "href": "Linear-models-overview.html#inference-about-gaussian-linear-regression-models",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.3 Inference about Gaussian Linear Regression Models",
    "text": "2.3 Inference about Gaussian Linear Regression Models\nResearch question: is there really an interaction between sex and age?\n\\(H_0: \\beta_3 = 0\\)\n\\(H_A: \\beta_3 \\neq 0\\)\n\\(P(|\\hat\\beta_3| &gt; |-18.41724138| \\mid H_0)\\) = ?\n\n2.3.1 Wald tests and CIs\nR can give you Wald tests for single coefficients and corresponding CIs:\n\nCodebw_lm2 |&gt; \n  parameters(ci_method = \"wald\") |&gt; \n  print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\n\n(Intercept)\n-2141.67\n1163.60\n(-4568.90, 285.56)\n-1.84\n0.081\n\n\nsex (male)\n872.99\n1611.33\n(-2488.18, 4234.17)\n0.54\n0.594\n\n\nage\n130.40\n30.00\n(67.82, 192.98)\n4.35\n&lt; .001\n\n\nsex (male) × age\n-18.42\n41.76\n(-105.52, 68.68)\n-0.44\n0.664\n\n\n\n\n\n\n2.3.2 One-parameter inference by hand\nTo understand what’s happening, let’s replicate these results by hand for the interaction term.\nP-value\n\nCodebeta_hat = coef(summary(bw_lm2))[\"sexmale:age\", \"Estimate\"]\nse_hat = coef(summary(bw_lm2))[\"sexmale:age\", \"Std. Error\"]\ndfresid = bw_lm2$df.residual\nt_stat = abs(beta_hat)/se_hat\npval_t = pt(abs(t_stat), df = dfresid, lower = FALSE) * 2\n\n\n\\[\n\\begin{aligned}\n&P\\left(\n| \\hat \\beta_3  | &gt;\n| -18.41724138| \\middle| H_0\n\\right)\n&= P\\left(\n\\left| \\frac{\\hat\\beta_3}{\\hat{SE}(\\hat\\beta_3)} \\right| &gt;\n\\left| \\frac{-18.41724138}{41.75581745} \\right| \\middle| H_0\n\\right)\\\\\n&= P\\left(\n| T_{20} | &gt;  0.44107007 \\middle| H_0\n\\right)\\\\\n&= 0.66389342\n\\end{aligned}\n\\] This matches the result in the table above.\nConfidence interval\n\nCodeconfint_radius_t = se_hat * qt(p = 0.975, df = dfresid, lower = TRUE)\nconfint_t = beta_hat + c(-1,1) * confint_radius_t\nprint(confint_t)\n\n[1] -105.52   68.68\n\n\nThis also matches.\n\n2.3.3 Gaussian approximations\nHere are the asymptotic (Gaussian approximation) equivalents:\nP-value\n\nCodepval_z = pnorm(abs(t_stat), lower = FALSE) * 2\n\nprint(pval_z)\n\n[1] 0.6592\n\n\nConfidence interval\n\nCodeconfint_radius_z = se_hat * qnorm(0.975, lower = TRUE)\nconfint_z = \n  beta_hat + c(-1,1) * confint_radius_z\nprint(confint_z)\n\n[1] -100.26   63.42\n\n\n\n2.3.4 Likelihood ratio statistics\n\nCodelogLik(bw_lm2)\n\n'log Lik.' -156.6 (df=5)\n\nCodelogLik(bw_lm1)\n\n'log Lik.' -156.7 (df=4)\n\nCodelLR = (logLik(bw_lm2) - logLik(bw_lm1)) |&gt; as.numeric()\ndelta_df = (bw_lm1$df.residual - df.residual(bw_lm2))\n\nd_lLR = function(x, df = delta_df) dchisq(x, df = df)\n\nx_max = 1\n\nchisq_plot = \n  ggplot() + \n  geom_function(fun = d_lLR) +\n  stat_function( fun = d_lLR, xlim = c(lLR, x_max), geom = \"area\", fill = \"gray\") +\n  geom_segment(aes(x = lLR, xend = lLR, y = 0, yend = d_lLR(lLR)), col = \"red\") + \n  xlim(0.0001,x_max) + \n  ylim(0,4) + \n  ylab(\"p(X=x)\") + \n  xlab(\"log(likelihood ratio) statistic [x]\") +\n  theme_classic()\n\n\n\nCodepchisq(\n  q = 2*lLR, \n  df = delta_df, \n  lower = FALSE)\n\n[1] 0.6298\n\n\n\nCodechisq_plot |&gt;  ggplotly()\n\n\n\n\n\nNow we can get the p-value:\n\nCodepchisq(2*lLR, df = delta_df, lower = FALSE)\n\n[1] 0.6298\n\n\n\n2.3.5 \nIn practice you don’t have to do this by hand; there are functions to do it for you:\n\nCode# built in\nlibrary(lmtest)\nlrtest(bw_lm2, bw_lm1)\n\n\n\n#Df\nLogLik\nDf\nChisq\nPr(&gt;Chisq)\n\n\n\n5\n-156.6\nNA\nNA\nNA\n\n\n4\n-156.7\n-1\n0.2323\n0.6298"
  },
  {
    "objectID": "Linear-models-overview.html#goodness-of-fit",
    "href": "Linear-models-overview.html#goodness-of-fit",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.4 Goodness of fit",
    "text": "2.4 Goodness of fit\n\n2.4.1 AIC and BIC\nWhen we use likelihood ratio tests, we are comparing how well different models fit the data.\nLikelihood ratio tests require “nested” models: one must be a special case of the other.\nIf we have non-nested models, we can instead use the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC):\n\nAIC = \\(-2 * \\ell(\\hat\\theta) + 2 * p\\)\nBIC = \\(-2 * \\ell(\\hat\\theta) + p * \\text{log}(n)\\)\n\nwhere \\(\\ell\\) is the log-likelihood of the data evaluated using the parameter estimates \\(\\hat\\theta\\), \\(p\\) is the number of estimated parameters in the model (including \\(\\hat\\sigma^2\\)), and \\(n\\) is the number of observations.\nYou can calculate these criteria using the logLik() function, or use the built-in R functions:\nAIC in R\n\nCode-2 * logLik(bw_lm2) |&gt; as.numeric() + \n  2*(length(coef(bw_lm2))+1) # sigma counts as a parameter here\n\n[1] 323.2\n\nCodeAIC(bw_lm2)\n\n[1] 323.2\n\n\nBIC in R\n\nCode-2 * logLik(bw_lm2) |&gt; as.numeric() + \n  (length(coef(bw_lm2))+1) * log(nobs(bw_lm2))\n\n[1] 329\n\nCodeBIC(bw_lm2)\n\n[1] 329\n\n\nLarge values of AIC and BIC are worse than small values. There are no hypothesis tests or p-values associated with these criteria.\n\n2.4.2 (Residual) Deviance\nLet \\(q\\) be the number of distinct covariate combinations in a data set.\n\nCodebw.X.unique = \n  bw |&gt; \n  count(sex, age)\n\nn_unique.bw  = nrow(bw.X.unique)\n\n\nFor example, in the birthweight data, there are \\(q = 12\\) unique patterns:\n\nCodebw.X.unique |&gt; \n  pander(\n    row.names = rownames(bw.X.unique))\n\n\n\n\n\n\n\n\n\n \nsex\nage\nn\n\n\n\n1\nfemale\n36\n2\n\n\n2\nfemale\n37\n1\n\n\n3\nfemale\n38\n2\n\n\n4\nfemale\n39\n2\n\n\n5\nfemale\n40\n4\n\n\n6\nfemale\n42\n1\n\n\n7\nmale\n35\n1\n\n\n8\nmale\n36\n1\n\n\n9\nmale\n37\n2\n\n\n10\nmale\n38\n3\n\n\n11\nmale\n40\n4\n\n\n12\nmale\n41\n1\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf a given covariate pattern has more than one observation in a dataset, those observations are called replicates.\n\n\nThen the most complicated model we could fit would have one parameter (a mean) for each of these combinations, plus a variance parameter:\n\nCodelm_max = \n  bw |&gt; \n  mutate(age = factor(age)) |&gt; \n  lm(\n    formula = weight ~ sex:age - 1, \n    data = _)\n\nlm_max |&gt; \n  parameters() |&gt; \n  print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(12)\np\n\n\n\nsex (male) × age35\n2925.00\n187.92\n(2515.55, 3334.45)\n15.56\n&lt; .001\n\n\nsex (female) × age36\n2570.50\n132.88\n(2280.98, 2860.02)\n19.34\n&lt; .001\n\n\nsex (male) × age36\n2625.00\n187.92\n(2215.55, 3034.45)\n13.97\n&lt; .001\n\n\nsex (female) × age37\n2539.00\n187.92\n(2129.55, 2948.45)\n13.51\n&lt; .001\n\n\nsex (male) × age37\n2737.50\n132.88\n(2447.98, 3027.02)\n20.60\n&lt; .001\n\n\nsex (female) × age38\n2872.50\n132.88\n(2582.98, 3162.02)\n21.62\n&lt; .001\n\n\nsex (male) × age38\n2982.00\n108.50\n(2745.60, 3218.40)\n27.48\n&lt; .001\n\n\nsex (female) × age39\n2846.00\n132.88\n(2556.48, 3135.52)\n21.42\n&lt; .001\n\n\nsex (female) × age40\n3152.25\n93.96\n(2947.52, 3356.98)\n33.55\n&lt; .001\n\n\nsex (male) × age40\n3256.25\n93.96\n(3051.52, 3460.98)\n34.66\n&lt; .001\n\n\nsex (male) × age41\n3292.00\n187.92\n(2882.55, 3701.45)\n17.52\n&lt; .001\n\n\nsex (female) × age42\n3210.00\n187.92\n(2800.55, 3619.45)\n17.08\n&lt; .001\n\n\n\n\n\nWe call this model the full, maximal, or saturated model for this dataset.\nWe can calculate the log-likelihood of this model as usual:\n\nCodelogLik(lm_max)\n\n'log Lik.' -151.4 (df=13)\n\n\nWe can compare this model to our other models using chi-square tests, as usual:\n\nCodelrtest(lm_max, bw_lm2)\n\n\n\n#Df\nLogLik\nDf\nChisq\nPr(&gt;Chisq)\n\n\n\n13\n-151.4\nNA\nNA\nNA\n\n\n5\n-156.6\n-8\n10.36\n0.241\n\n\n\n\n\nThe likelihood ratio statistic for this test is \\[\\lambda = 2 * (\\ell_{\\text{full}} - \\ell) = 10.35537421\\] where:\n\n\n\\(\\ell_{\\text{max}}\\) is the log-likelihood of the full model: -151.40160056\n\n\\(\\ell\\) is the log-likelihood of our comparison model (two slopes, two intercepts): -156.57928767\n\nThis statistic is called the deviance or residual deviance for our two-slopes and two-intercepts model; it tells us how much the likelihood of that model deviates from the likelihood of the maximal model.\nThe corresponding p-value tells us whether there we have enough evidence to detect that our two-slopes, two-intercepts model is a worse fit for the data than the maximal model; in other words, it tells us if there’s evidence that we missed any important patterns. (Remember, a nonsignificant p-value could mean that we didn’t miss anything and a more complicated model is unnecessary, or it could mean we just don’t have enough data to tell the difference between these models.)\n\n2.4.3 Null Deviance\nSimilarly, the least complicated model we could fit would have only one mean parameter, an intercept:\n\\[\\text E[Y|X=x] = \\beta_0\\] We can fit this model in R like so:\n\nCodelm0 = lm(weight ~ 1, data = bw)\n\nlm0 |&gt; parameters() |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(23)\np\n\n\n(Intercept)\n2967.67\n57.58\n(2848.56, 3086.77)\n51.54\n&lt; .001\n\n\n\n\nThis model also has a likelihood:\n\nCodelogLik(lm0)\n\n'log Lik.' -169 (df=2)\n\n\nAnd we can compare it to more complicated models using a likelihood ratio test:\n\nCodelrtest(bw_lm2, lm0)\n\n\n\n#Df\nLogLik\nDf\nChisq\nPr(&gt;Chisq)\n\n\n\n5\n-156.6\nNA\nNA\nNA\n\n\n2\n-169.0\n-3\n24.75\n0\n\n\n\n\n\nThe likelihood ratio statistic for the test comparing the null model to the maximal model is \\[\\lambda = 2 * (\\ell_{\\text{full}} - \\ell_{0}) = 35.10673188\\] where:\n\n\n\\(\\ell_{\\text{0}}\\) is the log-likelihood of the null model: -168.9549665\n\n\\(\\ell_{\\text{full}}\\) is the log-likelihood of the maximal model: -151.40160056\n\nIn R, this test is:\n\nCodelrtest(lm_max, lm0)\n\n\n\n#Df\nLogLik\nDf\nChisq\nPr(&gt;Chisq)\n\n\n\n13\n-151.4\nNA\nNA\nNA\n\n\n2\n-169.0\n-11\n35.11\n2e-04\n\n\n\n\n\nThis log-likelihood ratio statistic is called the null deviance. It tells us whether we have enough data to detect a difference between the null and full models."
  },
  {
    "objectID": "Linear-models-overview.html#rescaling",
    "href": "Linear-models-overview.html#rescaling",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.5 Rescaling",
    "text": "2.5 Rescaling\n\n2.5.1 Rescale age\n\nCodebw = \n  bw |&gt;\n  mutate(\n    `age - mean` = age - mean(age),\n    `age - 36wks` = age - 36\n  )\n\nlm1c = lm(weight ~ sex + `age - 36wks`, data = bw)\n\nlm2c = lm(weight ~ sex + `age - 36wks` + sex:`age - 36wks`, data = bw)\n\nparameters(lm2c, ci_method = \"wald\") |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\n\n(Intercept)\n2552.73\n97.59\n(2349.16, 2756.30)\n26.16\n&lt; .001\n\n\nsex (male)\n209.97\n129.75\n(-60.68, 480.63)\n1.62\n0.121\n\n\nage - 36wks\n130.40\n30.00\n(67.82, 192.98)\n4.35\n&lt; .001\n\n\nsex (male) × age - 36wks\n-18.42\n41.76\n(-105.52, 68.68)\n-0.44\n0.664\n\n\n\n\n\nCompare with what we got without rescaling:\n\nCodeparameters(bw_lm2, ci_method = \"wald\") |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\n\n(Intercept)\n-2141.67\n1163.60\n(-4568.90, 285.56)\n-1.84\n0.081\n\n\nsex (male)\n872.99\n1611.33\n(-2488.18, 4234.17)\n0.54\n0.594\n\n\nage\n130.40\n30.00\n(67.82, 192.98)\n4.35\n&lt; .001\n\n\nsex (male) × age\n-18.42\n41.76\n(-105.52, 68.68)\n-0.44\n0.664"
  },
  {
    "objectID": "Linear-models-overview.html#prediction",
    "href": "Linear-models-overview.html#prediction",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.6 Prediction",
    "text": "2.6 Prediction\n\n2.6.1 Prediction for linear models\n\\[\n\\begin{aligned}\n\\hat Y &= \\hat E[Y|X=x] \\\\\n&= x'\\hat\\beta \\\\\n&= \\hat\\beta_0\\cdot 1 + \\hat\\beta_1 x_1 + ... + \\hat\\beta_p x_p\n\\end{aligned}\n\\]\n\n2.6.2 prediction in R\n\nCodeX = model.matrix(bw_lm1)\nX |&gt; as_tibble() |&gt; print()\n\n# A tibble: 24 × 3\n   `(Intercept)` sexmale   age\n           &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1             1       1    40\n 2             1       0    40\n 3             1       1    38\n 4             1       0    36\n 5             1       1    40\n 6             1       0    40\n 7             1       1    35\n 8             1       0    38\n 9             1       1    36\n10             1       0    42\n# ℹ 14 more rows\n\nCodeprint(X[1,])\n\n(Intercept)     sexmale         age \n          1           1          40 \n\nCodeprint(coef(bw_lm1))\n\n(Intercept)     sexmale         age \n    -1773.3       163.0       120.9 \n\nCodeprint(X[1,] * coef(bw_lm1))\n\n(Intercept)     sexmale         age \n      -1773         163        4836 \n\nCode{X[1,] * coef(bw_lm1)} |&gt; sum() |&gt; print()\n\n[1] 3225\n\nCodeX %*% coef(bw_lm1) |&gt; as.vector()\n\n [1] 3225 3062 2984 2579 3225 3062 2621 2821 2742 3304 2863 2942 3346 3062 3225\n[16] 2700 2863 2579 2984 2821 3225 2942 2984 3062\n\nCodepredict(bw_lm1)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n3225 3062 2984 2579 3225 3062 2621 2821 2742 3304 2863 2942 3346 3062 3225 2700 \n  17   18   19   20   21   22   23   24 \n2863 2579 2984 2821 3225 2942 2984 3062 \n\nCodepredict(bw_lm1, se.fit = TRUE)\n\n$fit\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n3225 3062 2984 2579 3225 3062 2621 2821 2742 3304 2863 2942 3346 3062 3225 2700 \n  17   18   19   20   21   22   23   24 \n2863 2579 2984 2821 3225 2942 2984 3062 \n\n$se.fit\n [1] 61.46 57.17 51.58 76.03 61.46 57.17 85.25 53.38 69.96 83.89 57.95 51.38\n[13] 74.78 57.17 61.46 62.42 57.95 76.03 51.58 53.38 61.46 51.38 51.58 57.17\n\n$df\n[1] 21\n\n$residual.scale\n[1] 177.1\n\nCodepredict(bw_lm1, se.fit = TRUE)$fit\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n3225 3062 2984 2579 3225 3062 2621 2821 2742 3304 2863 2942 3346 3062 3225 2700 \n  17   18   19   20   21   22   23   24 \n2863 2579 2984 2821 3225 2942 2984 3062 \n\nCodepredict(bw_lm1, se.fit = TRUE)$se.fit\n\n [1] 61.46 57.17 51.58 76.03 61.46 57.17 85.25 53.38 69.96 83.89 57.95 51.38\n[13] 74.78 57.17 61.46 62.42 57.95 76.03 51.58 53.38 61.46 51.38 51.58 57.17\n\nCodepredict(bw_lm1, se.fit = TRUE, interval = \"confidence\")$fit |&gt; as_tibble()\n\n\n\nfit\nlwr\nupr\n\n\n\n3225\n3098\n3353\n\n\n3062\n2944\n3181\n\n\n2984\n2876\n3091\n\n\n2579\n2421\n2737\n\n\n3225\n3098\n3353\n\n\n3062\n2944\n3181\n\n\n2621\n2444\n2798\n\n\n2821\n2710\n2932\n\n\n2742\n2596\n2887\n\n\n3304\n3130\n3479\n\n\n2863\n2742\n2983\n\n\n2942\n2835\n3048\n\n\n3346\n3191\n3502\n\n\n3062\n2944\n3181\n\n\n3225\n3098\n3353\n\n\n2700\n2570\n2830\n\n\n2863\n2742\n2983\n\n\n2579\n2421\n2737\n\n\n2984\n2876\n3091\n\n\n2821\n2710\n2932\n\n\n3225\n3098\n3353\n\n\n2942\n2835\n3048\n\n\n2984\n2876\n3091\n\n\n3062\n2944\n3181\n\n\n\n\nCodepredict(bw_lm1, se.fit = TRUE, interval = \"predict\")$fit |&gt; as_tibble()\n\n\n\nfit\nlwr\nupr\n\n\n\n3225\n2836\n3615\n\n\n3062\n2675\n3449\n\n\n2984\n2600\n3367\n\n\n2579\n2178\n2980\n\n\n3225\n2836\n3615\n\n\n3062\n2675\n3449\n\n\n2621\n2212\n3030\n\n\n2821\n2436\n3205\n\n\n2742\n2346\n3138\n\n\n3304\n2897\n3712\n\n\n2863\n2475\n3250\n\n\n2942\n2558\n3325\n\n\n3346\n2947\n3746\n\n\n3062\n2675\n3449\n\n\n3225\n2836\n3615\n\n\n2700\n2309\n3090\n\n\n2863\n2475\n3250\n\n\n2579\n2178\n2980\n\n\n2984\n2600\n3367\n\n\n2821\n2436\n3205\n\n\n3225\n2836\n3615\n\n\n2942\n2558\n3325\n\n\n2984\n2600\n3367\n\n\n3062\n2675\n3449\n\n\n\n\n\nThe warning from the last command is: “predictions on current data refer to future responses” (since you already know what happened to the current data, and thus don’t need to predict it). You could also supply newdata to get predictions for new combinations of predictors that you didn’t see in your original dataset. See ?predict.lm for more."
  },
  {
    "objectID": "Linear-models-overview.html#diagnostics",
    "href": "Linear-models-overview.html#diagnostics",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.7 Diagnostics",
    "text": "2.7 Diagnostics\n\n2.7.1 Residuals\nDefinitions of residuals\n\nResiduals: \\[e_i = y_i - \\hat E[Y|X=x]\\]\nFor Gaussian models, \\(e_i\\) can be seen as an estimate of \\[\\epsilon_i = Y_i - \\text{E}[Y|X=x_i]\\]\nStandardized residuals: \\[r_i = \\frac{e_i}{\\hat{SD}(e_i)}\\]\nFor Gaussian models: \\[\\hat{SD}(e_i) \\approx \\frac{e_i}{\\hat{\\sigma}}\\]\nCharacteristics of residuals\nWith enough data and a correct model, the residuals will be approximately Guassian distributed, with variance \\(\\sigma^2\\), which we can estimate using \\(\\hat\\sigma^2\\): that is:\n\\[\ne_i\\ \\dot \\sim_{iid}\\ N(0, \\hat\\sigma^2)\n\\]\nHence, with enough data and a correct model, the standardized residuals will be approximately standard Gaussian; that is,\n\\[\nr_i\\ \\dot \\sim_{iid}\\ N(0,1)\n\\]\n\n2.7.2 Marginal distributions of residuals\nTo look for problems with our model, we can check whether the residuals \\(e_i\\) and standardized residuals \\(r_i\\) look like they have the distributions that they are supposed to have, according to the model.\nFirst, we need to compute the residuals. R makes this easy:\n(non-standardized) residuals in R\n\nCoderesid(bw_lm2)\n\n      1       2       3       4       5       6       7       8       9      10 \n 176.27 -140.73 -144.13  -59.53  177.47 -126.93  -68.93  242.67 -139.33   51.67 \n     11      12      13      14      15      16      17      18      19      20 \n 156.67 -125.13  274.28 -137.71  -27.69 -246.69 -191.67  189.33  -11.67 -242.64 \n     21      22      23      24 \n -47.64  262.36  210.36  -30.62 \n\nCode# check by hand\nbw$weight - fitted(bw_lm2)\n\n      1       2       3       4       5       6       7       8       9      10 \n 176.27 -140.73 -144.13  -59.53  177.47 -126.93  -68.93  242.67 -139.33   51.67 \n     11      12      13      14      15      16      17      18      19      20 \n 156.67 -125.13  274.28 -137.71  -27.69 -246.69 -191.67  189.33  -11.67 -242.64 \n     21      22      23      24 \n -47.64  262.36  210.36  -30.62 \n\n\nSuccess!\nStandardized residuals in R\n\nCoderstandard(bw_lm2)\n\n       1        2        3        4        5        6        7        8 \n 1.15982 -0.92601 -0.87479 -0.34723  1.03507 -0.73473 -0.39901  1.43752 \n       9       10       11       12       13       14       15       16 \n-0.82539  0.30606  0.92807 -0.87616  1.91428 -0.86559 -0.16430 -1.46376 \n      17       18       19       20       21       22       23       24 \n-1.11016  1.09658 -0.06761 -1.46159 -0.28696  1.58040  1.26717 -0.19805 \n\nCoderesid(bw_lm2)/sigma(bw_lm2)\n\n       1        2        3        4        5        6        7        8 \n 0.97593 -0.77920 -0.79802 -0.32962  0.98258 -0.70279 -0.38166  1.34357 \n       9       10       11       12       13       14       15       16 \n-0.77144  0.28606  0.86741 -0.69282  1.51858 -0.76244 -0.15331 -1.36584 \n      17       18       19       20       21       22       23       24 \n-1.06123  1.04825 -0.06463 -1.34341 -0.26376  1.45262  1.16471 -0.16954 \n\n\nThese are not quite the same, because R is doing something more complicated and precise to get the standard errors. Let’s not worry about those details for now; the difference is pretty small in this case:\n\nCoderstandard_compare_plot = \n  tibble(\n    x = resid(bw_lm2)/sigma(bw_lm2), \n    y = rstandard(bw_lm2)) |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_point() + \n  theme_bw() +\n  coord_equal() + \n  xlab(\"resid(bw_lm2)/sigma(bw_lm2)\") +\n  ylab(\"rstandard(bw_lm2)\") +\n  geom_abline(\n    aes(\n    intercept = 0,\n    slope = 1, \n    col = \"x=y\")) +\n  labs(colour=\"\") +\n  scale_colour_manual(values=\"red\")\n\n\n\nCodeggplotly(rstandard_compare_plot)\n\n\n\n\n\nLet’s add these residuals to the tibble of our dataset:\n\nCodebw = \n  bw |&gt; \n  mutate(\n    fitted_lm2 = fitted(bw_lm2),\n    \n    resid_lm2 = resid(bw_lm2),\n    # resid_lm2 = weight - fitted_lm2,\n    \n    std_resid_lm2 = rstandard(bw_lm2),\n    # std_resid_lm2 = resid_lm2 / sigma(bw_lm2)\n  )\n\nbw |&gt; \n  select(\n    sex,\n    age,\n    weight,\n    fitted_lm2,\n    resid_lm2,\n    std_resid_lm2\n  )\n\n\n\nsex\nage\nweight\nfitted_lm2\nresid_lm2\nstd_resid_lm2\n\n\n\nfemale\n36\n2729\n2553\n176.27\n1.1598\n\n\nfemale\n36\n2412\n2553\n-140.73\n-0.9260\n\n\nfemale\n37\n2539\n2683\n-144.13\n-0.8748\n\n\nfemale\n38\n2754\n2814\n-59.53\n-0.3472\n\n\nfemale\n38\n2991\n2814\n177.47\n1.0351\n\n\nfemale\n39\n2817\n2944\n-126.93\n-0.7347\n\n\nfemale\n39\n2875\n2944\n-68.93\n-0.3990\n\n\nfemale\n40\n3317\n3074\n242.67\n1.4375\n\n\nfemale\n40\n2935\n3074\n-139.33\n-0.8254\n\n\nfemale\n40\n3126\n3074\n51.67\n0.3061\n\n\nfemale\n40\n3231\n3074\n156.67\n0.9281\n\n\nfemale\n42\n3210\n3335\n-125.13\n-0.8762\n\n\nmale\n35\n2925\n2651\n274.28\n1.9143\n\n\nmale\n36\n2625\n2763\n-137.71\n-0.8656\n\n\nmale\n37\n2847\n2875\n-27.69\n-0.1643\n\n\nmale\n37\n2628\n2875\n-246.69\n-1.4638\n\n\nmale\n38\n2795\n2987\n-191.67\n-1.1102\n\n\nmale\n38\n3176\n2987\n189.33\n1.0966\n\n\nmale\n38\n2975\n2987\n-11.67\n-0.0676\n\n\nmale\n40\n2968\n3211\n-242.64\n-1.4616\n\n\nmale\n40\n3163\n3211\n-47.64\n-0.2870\n\n\nmale\n40\n3473\n3211\n262.36\n1.5804\n\n\nmale\n40\n3421\n3211\n210.36\n1.2672\n\n\nmale\n41\n3292\n3323\n-30.62\n-0.1981\n\n\n\n\n\nNow let’s build histograms:\nMarginal distribution of (nonstandardized) residuals\n\nCoderesid_marginal_hist = \n  bw |&gt; \n  ggplot(aes(x = resid_lm2)) +\n  geom_histogram()\n\n\n\nCodeggplotly(resid_marginal_hist)\n\n\n\n\n\nHard to tell with this small amount of data, but I’m a bit concerned that the histogram doesn’t show a bell-curve shape.\nMarginal distribution of standardized residuals\n\nCodestd_resid_marginal_hist = \n  bw |&gt; \n  ggplot(aes(x = std_resid_lm2)) +\n  geom_histogram()\n\n\n\nCodeggplotly(std_resid_marginal_hist)\n\n\n\n\n\nThis looks similar, although the scale of the x-axis got narrower, because we divided by \\(\\hat\\sigma\\) (roughly speaking).\nStill hard to tell if the distribution is Gaussian.\n\n2.7.3 QQ plot of standardized residuals\nAnother way to assess normality is the QQ plot of the standardized residuals versus normal quantiles:\n\nCodelibrary(ggfortify) \n# needed to make ggplot2::autoplot() work for `lm` objects\n\nqqplot_lm2_auto = \n  bw_lm2 |&gt; \n  autoplot(\n    which = 2, # options are 1:6; can do multiple at once\n    ncol = 1) +\n  theme_classic()\n\n\n\nCodeprint(qqplot_lm2_auto)\n\n\n\n\nIf the Gaussian model were correct, these points should follow the dotted line.\n\n\n\n\n\n\nNote\n\n\n\nFig 2.4 panel (c) in Dobson is a little different; they didn’t specify how they produced it, but other statistical analysis systems do things differently from R.\n\n\nQQ plot - how it’s built\nLet’s construct it by hand:\n\nCodebw = bw |&gt; \n  mutate(\n    p = (rank(std_resid_lm2) - 1/2)/n(), # \"Blom's method\"\n    expected_quantiles_lm2 = qnorm(p)\n  )\n\nqqplot_lm2 = \n  bw |&gt; \n  ggplot(\n    aes(\n      x = expected_quantiles_lm2, \n      y = std_resid_lm2, \n      col = sex, \n      shape = sex)\n  ) + \n  geom_point() +\n  theme_classic() +\n  theme(legend.position='none') + # removing the plot legend\n  ggtitle(\"Normal Q-Q\") +\n  xlab(\"Theoretical Quantiles\") + \n  ylab(\"Standardized residuals\")\n\n\nWe find the expected line like so:\n\nCodeps &lt;- c(.25, .75)                  # reference probabilities\na &lt;- quantile(rstandard(bw_lm2), ps)  # empirical quantiles\nb &lt;- qnorm(ps)                     # theoretical quantiles\n\nqq_slope = diff(a)/diff(b)\nqq_intcpt = a[1] - b[1] * qq_slope\n\nqqplot_lm2 = \n  qqplot_lm2 +\n  geom_abline(slope = qq_slope, intercept = qq_intcpt)\n\n\n\nCodeggplotly(qqplot_lm2)\n\n\n\n\n\n\n2.7.4 Conditional distributions of residuals\nIf our Gaussian linear regression model is correct, the residuals \\(e_i\\) and standardized residuals \\(r_i\\) should have:\n\nan approximately Gaussian distribution, with:\na mean of 0\na constant variance\n\nThis should be true regardless of the value of \\(X\\).\nBut if we didn’t correctly guess the functional form of linear component of the mean, \\[\\text{E}[Y|X=x] = \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p\\]\nThen the the residuals might have nonzero mean or nonconstant variance for some values of \\(x\\).\nResiduals versus fitted values\nTo look for these issues, we can plot the residuals \\(e_i\\) against the fitted values \\(\\hat y_i\\):\n\nCodeautoplot(bw_lm2, which = 1, ncol = 1) |&gt; print()\n\n\n\n\nIf the model is correct, the blue line should stay flat and close to 0, and the cloud of dots should have the same vertical spread regardless of the fitted value.\nIf not, we probably need to change the functional form of linear component of the mean, \\[\\text{E}[Y|X=x] = \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p\\]\nScale-location plot\nWe can also plot the square roots of the absolute values of the standardized residuals against the fitted values:\n\nCodeautoplot(bw_lm2, which = 3, ncol = 1) |&gt; print()\n\n\n\n\nHere, the blue line doesn’t need to be near 0, but it should be flat. If not, the residual variance \\(\\sigma^2\\) might not be constant, and we might need to transform our outcome \\(Y\\) (or use a model that allows non-constant variance).\nResiduals versus leverage\nWe can also plot our standardized residuals against “leverage”, which roughly speaking is a measure of how unusual each \\(x_i\\) value is. Very unusual \\(x_i\\) values can have extreme effects on the model fit, so we might want ot remove those observations as outliers, particularly if they have large residuals.\n\nCodeautoplot(bw_lm2, which = 5, ncol = 1) |&gt; print()\n\n\n\n\nThe blue line should be relatively flat and close to 0 here.\n\n2.7.5 Diagnostics constructed by hand\n\nCodebw = \n  bw |&gt; \n  mutate(\n    predlm2 = predict(bw_lm2),\n    residlm2 = weight - predlm2,\n    std_resid = residlm2 / sigma(bw_lm2),\n    # std_resid_builtin = rstandard(bw_lm2), # uses leverage\n    sqrt_abs_std_resid = std_resid |&gt; abs() |&gt; sqrt()\n    \n  )\n\n\nResiduals vs fitted\n\nCoderesid_vs_fit = bw |&gt; \n  ggplot(\n    aes(x = predlm2, y = residlm2, col = sex, shape = sex)\n  ) + \n  geom_point() +\n  theme_classic() +\n  geom_hline(yintercept = 0)\n\n\n\nCodeprint(resid_vs_fit)\n\n\n\n\nStandardized residuals vs fitted\n\nCodebw |&gt; \n  ggplot(\n    aes(x = predlm2, y = std_resid, col = sex, shape = sex)\n  ) + \n  geom_point() +\n  theme_classic() +\n  geom_hline(yintercept = 0)\n\n\n\n\nStandardized residuals vs gestational age\n\nCodebw |&gt; \n  ggplot(\n    aes(x = age, y = std_resid, col = sex, shape = sex)\n  ) + \n  geom_point() +\n  theme_classic() +\n  geom_hline(yintercept = 0)\n\n\n\n\n\nsqrt(abs(rstandard())) vs fitted\nCompare with autoplot(bw_lm2, 3)\n\nCodebw |&gt; \n  ggplot(\n    aes(x = predlm2, y = sqrt_abs_std_resid, col = sex, shape = sex)\n  ) + \n  geom_point() +\n  theme_classic() +\n  geom_hline(yintercept = 0)"
  },
  {
    "objectID": "Linear-models-overview.html#model-selection",
    "href": "Linear-models-overview.html#model-selection",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.8 Model selection",
    "text": "2.8 Model selection\nIf we have a lot of covariates in our dataset, we might want to choose a small subset to use in our model.\nThere are a few possible metrics to consider for choosing a “best” model.\n\n2.8.1 Mean squared error\nWe might want to minimize the mean squared error, \\(\\text E[(y-\\hat y)^2]\\), for new observations that weren’t in our data set when we fit the model.\nUnfortunately, \\[\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat y_i)^2\\] gives a biased estimate of \\(\\text E[(y-\\hat y)^2]\\) for new data. If we want an unbiased estimate, we will have to be clever.\n\n2.8.2 Cross-validation\n\nCodedata(\"carbohydrate\", package = \"dobson\")\nlibrary(cvTools)\nfull.model &lt;- lm(carbohydrate ~ ., data = carbohydrate)\ntemp = \n  cvFit(full.model, data = carbohydrate, K = 5, R = 10,\ny = carbohydrate$carbohydrate)"
  },
  {
    "objectID": "Linear-models-overview.html#categorical-covariates-with-more-than-two-levels",
    "href": "Linear-models-overview.html#categorical-covariates-with-more-than-two-levels",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.9 Categorical covariates with more than two levels",
    "text": "2.9 Categorical covariates with more than two levels\n\n2.9.1 \nIn the birthweight example, the variable sex had only two observed values:\n\nCodeunique(bw$sex)\n\n[1] \"female\" \"male\"  \n\n\nIf there are more than two observed values, we can’t just use a single variable with 0s and 1s.\n\n2.9.2 \nFor example, here’s the (in)famous iris data:\n\nCodeiris |&gt; tibble()\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n4.8\n3.1\n1.6\n0.2\nsetosa\n\n\n5.4\n3.4\n1.5\n0.4\nsetosa\n\n\n5.2\n4.1\n1.5\n0.1\nsetosa\n\n\n5.5\n4.2\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.2\n1.2\n0.2\nsetosa\n\n\n5.5\n3.5\n1.3\n0.2\nsetosa\n\n\n4.9\n3.6\n1.4\n0.1\nsetosa\n\n\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n5.1\n3.4\n1.5\n0.2\nsetosa\n\n\n5.0\n3.5\n1.3\n0.3\nsetosa\n\n\n4.5\n2.3\n1.3\n0.3\nsetosa\n\n\n4.4\n3.2\n1.3\n0.2\nsetosa\n\n\n5.0\n3.5\n1.6\n0.6\nsetosa\n\n\n5.1\n3.8\n1.9\n0.4\nsetosa\n\n\n4.8\n3.0\n1.4\n0.3\nsetosa\n\n\n5.1\n3.8\n1.6\n0.2\nsetosa\n\n\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n6.9\n3.1\n4.9\n1.5\nversicolor\n\n\n5.5\n2.3\n4.0\n1.3\nversicolor\n\n\n6.5\n2.8\n4.6\n1.5\nversicolor\n\n\n5.7\n2.8\n4.5\n1.3\nversicolor\n\n\n6.3\n3.3\n4.7\n1.6\nversicolor\n\n\n4.9\n2.4\n3.3\n1.0\nversicolor\n\n\n6.6\n2.9\n4.6\n1.3\nversicolor\n\n\n5.2\n2.7\n3.9\n1.4\nversicolor\n\n\n5.0\n2.0\n3.5\n1.0\nversicolor\n\n\n5.9\n3.0\n4.2\n1.5\nversicolor\n\n\n6.0\n2.2\n4.0\n1.0\nversicolor\n\n\n6.1\n2.9\n4.7\n1.4\nversicolor\n\n\n5.6\n2.9\n3.6\n1.3\nversicolor\n\n\n6.7\n3.1\n4.4\n1.4\nversicolor\n\n\n5.6\n3.0\n4.5\n1.5\nversicolor\n\n\n5.8\n2.7\n4.1\n1.0\nversicolor\n\n\n6.2\n2.2\n4.5\n1.5\nversicolor\n\n\n5.6\n2.5\n3.9\n1.1\nversicolor\n\n\n5.9\n3.2\n4.8\n1.8\nversicolor\n\n\n6.1\n2.8\n4.0\n1.3\nversicolor\n\n\n6.3\n2.5\n4.9\n1.5\nversicolor\n\n\n6.1\n2.8\n4.7\n1.2\nversicolor\n\n\n6.4\n2.9\n4.3\n1.3\nversicolor\n\n\n6.6\n3.0\n4.4\n1.4\nversicolor\n\n\n6.8\n2.8\n4.8\n1.4\nversicolor\n\n\n6.7\n3.0\n5.0\n1.7\nversicolor\n\n\n6.0\n2.9\n4.5\n1.5\nversicolor\n\n\n5.7\n2.6\n3.5\n1.0\nversicolor\n\n\n5.5\n2.4\n3.8\n1.1\nversicolor\n\n\n5.5\n2.4\n3.7\n1.0\nversicolor\n\n\n5.8\n2.7\n3.9\n1.2\nversicolor\n\n\n6.0\n2.7\n5.1\n1.6\nversicolor\n\n\n5.4\n3.0\n4.5\n1.5\nversicolor\n\n\n6.0\n3.4\n4.5\n1.6\nversicolor\n\n\n6.7\n3.1\n4.7\n1.5\nversicolor\n\n\n6.3\n2.3\n4.4\n1.3\nversicolor\n\n\n5.6\n3.0\n4.1\n1.3\nversicolor\n\n\n5.5\n2.5\n4.0\n1.3\nversicolor\n\n\n5.5\n2.6\n4.4\n1.2\nversicolor\n\n\n6.1\n3.0\n4.6\n1.4\nversicolor\n\n\n5.8\n2.6\n4.0\n1.2\nversicolor\n\n\n5.0\n2.3\n3.3\n1.0\nversicolor\n\n\n5.6\n2.7\n4.2\n1.3\nversicolor\n\n\n5.7\n3.0\n4.2\n1.2\nversicolor\n\n\n5.7\n2.9\n4.2\n1.3\nversicolor\n\n\n6.2\n2.9\n4.3\n1.3\nversicolor\n\n\n5.1\n2.5\n3.0\n1.1\nversicolor\n\n\n5.7\n2.8\n4.1\n1.3\nversicolor\n\n\n6.3\n3.3\n6.0\n2.5\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n7.1\n3.0\n5.9\n2.1\nvirginica\n\n\n6.3\n2.9\n5.6\n1.8\nvirginica\n\n\n6.5\n3.0\n5.8\n2.2\nvirginica\n\n\n7.6\n3.0\n6.6\n2.1\nvirginica\n\n\n4.9\n2.5\n4.5\n1.7\nvirginica\n\n\n7.3\n2.9\n6.3\n1.8\nvirginica\n\n\n6.7\n2.5\n5.8\n1.8\nvirginica\n\n\n7.2\n3.6\n6.1\n2.5\nvirginica\n\n\n6.5\n3.2\n5.1\n2.0\nvirginica\n\n\n6.4\n2.7\n5.3\n1.9\nvirginica\n\n\n6.8\n3.0\n5.5\n2.1\nvirginica\n\n\n5.7\n2.5\n5.0\n2.0\nvirginica\n\n\n5.8\n2.8\n5.1\n2.4\nvirginica\n\n\n6.4\n3.2\n5.3\n2.3\nvirginica\n\n\n6.5\n3.0\n5.5\n1.8\nvirginica\n\n\n7.7\n3.8\n6.7\n2.2\nvirginica\n\n\n7.7\n2.6\n6.9\n2.3\nvirginica\n\n\n6.0\n2.2\n5.0\n1.5\nvirginica\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\nCodesummary(iris)\n\n\n\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\nMin. :4.30\nMin. :2.00\nMin. :1.00\nMin. :0.1\nsetosa :50\n\n\n\n1st Qu.:5.10\n1st Qu.:2.80\n1st Qu.:1.60\n1st Qu.:0.3\nversicolor:50\n\n\n\nMedian :5.80\nMedian :3.00\nMedian :4.35\nMedian :1.3\nvirginica :50\n\n\n\nMean :5.84\nMean :3.06\nMean :3.76\nMean :1.2\nNA\n\n\n\n3rd Qu.:6.40\n3rd Qu.:3.30\n3rd Qu.:5.10\n3rd Qu.:1.8\nNA\n\n\n\nMax. :7.90\nMax. :4.40\nMax. :6.90\nMax. :2.5\nNA\n\n\n\n\n\n\n2.9.3 \nThere are three species:\n\nCodeiris$Species |&gt; unique()\n\n[1] setosa     versicolor virginica \nLevels: setosa versicolor virginica\n\n\n\n2.9.4 \nIf we want to model Sepal.Length by species, we could create a variable \\(X\\) that represents “setosa” as \\(X=1\\), “virginica” as \\(X=2\\), and “versicolor” as \\(X=3\\).\n\nCodedata(iris) # this step is not always necessary, but ensures you're starting  \n# from the original version of a dataset stored in a loaded package\n\niris = \n  iris |&gt; \n  tibble() |&gt;\n  mutate(\n    X = case_when(\n      Species == \"setosa\" ~ 1,\n      Species == \"virginica\" ~ 2,\n      Species == \"versicolor\" ~ 3\n    )\n  )\n\niris |&gt; \n  distinct(Species, X) |&gt; \n  print()\n\n# A tibble: 3 × 2\n  Species        X\n  &lt;fct&gt;      &lt;dbl&gt;\n1 setosa         1\n2 versicolor     3\n3 virginica      2\n\n\nThen we could fit a model like:\n\nCodeiris_lm1 = lm(Sepal.Length ~ X, data = iris)\niris_lm1 |&gt; parameters() |&gt; print_md()\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(148)\np\n\n\n\n(Intercept)\n4.91\n0.16\n(4.60, 5.23)\n30.83\n&lt; .001\n\n\nX\n0.47\n0.07\n(0.32, 0.61)\n6.30\n&lt; .001\n\n\n\n\n\n\n2.9.5 Let’s see how that model looks:\n\nCodeiris_plot1 = iris |&gt; \n  ggplot(\n    aes(\n      x = X, \n      y = Sepal.Length)\n  ) +\n  geom_point(alpha = .1) +\n  geom_abline(\n    intercept = coef(iris_lm1)[1], \n    slope = coef(iris_lm1)[2]) +\n  theme_bw(base_size = 18)\n\n\n\nCodeggplotly(iris_plot1)\n\n\n\n\n\nWe have forced the model to use a straight line for the three estimated means. Maybe not a good idea?\n\n2.9.6 Let’s see what R does with categorical variables by default:\n\nCodeiris_lm2 = lm(Sepal.Length ~ Species, data = iris)\niris_lm2 |&gt; parameters() |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(147)\np\n\n\n\n(Intercept)\n5.01\n0.07\n(4.86, 5.15)\n68.76\n&lt; .001\n\n\nSpecies (versicolor)\n0.93\n0.10\n(0.73, 1.13)\n9.03\n&lt; .001\n\n\nSpecies (virginica)\n1.58\n0.10\n(1.38, 1.79)\n15.37\n&lt; .001\n\n\n\n\n\n\n2.9.7 Re-parametrize with no intercept\nIf you don’t want the default and offset option, you can use “-1” like we’ve seen previously:\n\nCodeiris.lm2b = lm(Sepal.Length ~ Species - 1, data = iris)\niris.lm2b |&gt; parameters() |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(147)\np\n\n\n\nSpecies (setosa)\n5.01\n0.07\n(4.86, 5.15)\n68.76\n&lt; .001\n\n\nSpecies (versicolor)\n5.94\n0.07\n(5.79, 6.08)\n81.54\n&lt; .001\n\n\nSpecies (virginica)\n6.59\n0.07\n(6.44, 6.73)\n90.49\n&lt; .001\n\n\n\n\n\n\n2.9.8 Let’s see what these new models look like:\n\nCodeiris_plot2 = \n  iris |&gt; \n  mutate(\n    predlm2 = predict(iris_lm2)) |&gt; \n  arrange(X) |&gt; \n  ggplot(aes(x = X, y = Sepal.Length)) +\n  geom_point(alpha = .1) +\n  geom_line(aes(y = predlm2), col = \"red\") +\n  geom_abline(\n    intercept = coef(iris_lm1)[1], \n    slope = coef(iris_lm1)[2]) + \n  theme_bw(base_size = 18)\n\n\n\nCodeggplotly(iris_plot2)\n\n\n\n\n\n\n2.9.9 Let’s see how R did that:\n\nCodeformula(iris_lm2)\n\nSepal.Length ~ Species\n\nCodemodel.matrix(iris_lm2) |&gt; as_tibble() |&gt; unique()\n\n\n\n(Intercept)\nSpeciesversicolor\nSpeciesvirginica\n\n\n\n1\n0\n0\n\n\n1\n1\n0\n\n\n1\n0\n1\n\n\n\n\n\nThis is called a “corner point parametrization”.\n\nCodeformula(iris.lm2b)\n\nSepal.Length ~ Species - 1\n\nCodemodel.matrix(iris.lm2b) |&gt; as_tibble() |&gt; unique()\n\n\n\nSpeciessetosa\nSpeciesversicolor\nSpeciesvirginica\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1\n\n\n\n\n\nThis can be called a “group point parametrization”.\nThere are more options; see Dobson & Barnett §6.4.1."
  },
  {
    "objectID": "Linear-models-overview.html#footnotes",
    "href": "Linear-models-overview.html#footnotes",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "",
    "text": "using the definite article “the” would mean there is only one slope.↩︎"
  },
  {
    "objectID": "logistic-regression.html#introduction",
    "href": "logistic-regression.html#introduction",
    "title": "3  Logistic Regression",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\n\n3.1.1 What is logistic regression?\nLogistic regression is a framework for modeling binary outcomes, conditional on one or more predictors (a.k.a. covariates).\nExamples of binary variables include:\n\nexposure (exposed vs unexposed)\ndisease (diseased vs healthy)\nrecovery (recovered vs unrecovered)\nrelapse (relapse vs remission)\nreturn to hospital (returned vs not)\nvital status (dead vs alive)\n\nLogistic regression uses the Bernoulli distribution to model the outcome variable, conditional on one or more covariates.\n\nExercise 3.1 Write down a mathematical definition of the Bernoulli distribution.\n\nSolution. See Definition A.6.\n\n\n\n\n3.1.2 Logistic regression versus linear regression\nLogistic regression differs from linear regression, which uses the Gaussian (“normal”) distribution to model the outcome variable, conditional on the covariates.\n\nExercise 3.2 Recall: what kinds of outcomes is linear regression used for?\n\nSolution. Linear regression is typically used for numerical outcomes that aren’t event counts or waiting times for an event. Examples of outcomes that are often analyzed using linear regression include include weight, height, and income."
  },
  {
    "objectID": "logistic-regression.html#risk-estimation-and-prediction",
    "href": "logistic-regression.html#risk-estimation-and-prediction",
    "title": "3  Logistic Regression",
    "section": "3.2 Risk Estimation and Prediction",
    "text": "3.2 Risk Estimation and Prediction\nIn Epi 203, you have already seen methods for modeling binary outcomes using one covariate that is also binary (such as exposure/non-exposure). In this section, we review one-covariate analyses, with a special focus on risk ratios and odds ratios, which are important concepts for interpreting logistic regression.\n\nExample 3.1 (Oral Contraceptive Use and Heart Attack)  \n\nResearch question: how does oral contraceptive (OC) use affect the risk of myocardial infarction (MI; a.k.a. heart attack)?\n\n\nThis was an issue when oral contraceptives were first developed, because the original formulations used higher concentrations of hormones. Modern OCs don’t have this issue.\n\nTable 3.1 contains simulated data for an imaginary follow-up (a.k.a. prospective) study in which two groups are identified, one using OCs and another not using OCs, and both groups are tracked for three years to determine how many in each groups have MIs.\n\n\nCode\nlibrary(dplyr)\noc_mi = \n  tribble(\n    ~OC, ~MI, ~Total,\n    \"OC use\", 13, 5000,\n    \"No OC use\", 7, 10000\n  ) |&gt; \n  mutate(`No MI` = Total - MI) |&gt; \n  relocate(`No MI`, .after = MI)\n\ntotals = \n  oc_mi |&gt; \n  summarize(across(c(MI, `No MI`, Total), sum)) |&gt; \n  mutate(OC = \"Total\")\n\noc_mi = bind_rows(oc_mi, totals)\n\npander(oc_mi)\n\n\n\n\nTable 3.1: Simulated data from study of oral contraceptive use and heart attack risk\n\n\n\n\n\n\n\n\nOC\nMI\nNo MI\nTotal\n\n\n\n\nOC use\n13\n4,987\n5,000\n\n\nNo OC use\n7\n9,993\n10,000\n\n\nTotal\n20\n14,980\n15,000\n\n\n\n\n\n\n\n\nExercise 3.3 Review: estimate the probabilities of MI for OC users and non-OC users in Example 3.1.\n\nSolution. \\[\\hat{p}(MI|OC) = \\frac{13}{5000} = 0.0026\\]\n\\[\\hat{p}(MI|\\neg OC) = \\frac{7}{10000} = 7\\times 10^{-4}\\]\n\n\n\n\n\n\n\n\nTwo meanings of “controls”\n\n\n\nDepending on context, “controls” can mean either individuals who don’t experience an exposure of interest, or individuals who don’t experience an outcome of interest.\n\nDefinition 3.1 (cases and controls in retrospective studies) In retrospective studies, participants who experience the outcome of interest are called cases, while participants who don’t experience that outcome are called controls.\n\n\nDefinition 3.2 (treatment groups and control groups in prospective studies) In prospective studies, the group of participants who experience the treatment or exposure of interest is called the treatment group, while the participants who receive the baseline or comparison treatment (for example, clinical trial participants who receive a placebo or a standard-of-care treatment rather than an experimental treatment) are called controls.\n\n\n\n\n3.2.1 Comparing probabilities\n\nRisk differences\nThe simplest comparison of two probabilities, \\(\\pi_1\\), and \\(\\pi_2\\), is the difference of their values:\n\n\nDefinition 3.3 (Risk difference) The risk difference of two probabilities, \\(\\pi_1\\), and \\(\\pi_2\\), is the difference of their values: \\[\\delta(\\pi_1,\\pi_2) \\stackrel{\\text{def}}{=}\\pi_1 - \\pi_2\\]\n\nExample 3.2 In Example 3.1, the estimated risk difference in MI risk between OC users and OC non-users is:\n\\[\n\\begin{aligned}\n\\hat\\delta(\\pi(OC), \\pi(\\neg OC))\n&= \\delta(\\hat\\pi(OC), \\hat\\pi(\\neg OC))\\\\\n&= \\hat\\pi(OC) - \\hat\\pi(\\neg OC)\\\\\n&= 0.0026 - 7\\times 10^{-4}\\\\\n&= 0.0019\n\\end{aligned}\n\\]\n\n\n\n\nRisk ratios\n\n\nDefinition 3.4 (Relative risk ratios) The relative risk of two probabilities \\(\\pi_1\\) and \\(\\pi_2\\), also called the risk ratio, is the ratio of those probabilities:\n\\[\\rho(\\pi_1,\\pi_2) = \\frac{\\pi_1}{\\pi_2}\\]\n\nExample 3.3 Above, we estimated that: \\[\\hat{p}(MI|OC) = 0.0026\\]\n\\[\\hat{p}(MI|\\neg OC) = 7\\times 10^{-4}\\]\nSo we might estimate that the relative risk of MI for OC versus non-OC is:\n\\[\n\\begin{aligned}\n\\hat\\rho(OC, \\neg OC)\n&=\\frac{\\hat{p}(MI|OC)}{\\hat{p}(MI|\\neg OC)}\\\\\n&= \\frac{0.0026}{7\\times 10^{-4}}\\\\\n&= 3.71428571\n\\end{aligned}\n\\]\n\n\nBoth risk differences and risk ratios are defined by two probabilities, plus a choice of which probability is the baseline or reference probability (i.e., which probability is the subtrahend of the risk difference or the denominator of the risk ratio). To switch which one is the reference probability, invert the ratio and multiply the difference by -1.\n\nExample 3.4 Above, we estimated that the risk ratio of OC versus non-OC is:\n\\[\n\\begin{aligned}\n\\rho(OC, \\neg OC)\n&= 3.71428571\n\\end{aligned}\n\\]\nIn comparison, the risk ratio for non-OC versus OC is:\n\\[\n\\begin{aligned}\n\\rho(\\neg OC, OC)\n&=\\frac{\\hat{p}(MI|\\neg OC)}{\\hat{p}(MI|OC)}\\\\\n&= \\frac{7\\times 10^{-4}}{0.0026}\\\\\n&= 0.26923077\\\\\n&= \\frac{1}{\\rho(OC, \\neg OC)}\n\\end{aligned}\n\\]\n\n\n\n\n3.2.2 Odds and probabilities\nIn logistic regression, we will make use of a transformation (rescaling) of probability, called odds.\n\nDefinition 3.5 (Odds) The odds of an outcome, denoted \\(\\omega\\) (“omega”), is the probability that the outcome occurs, divided by the probability that it doesn’t occur.\nThat is, if the probability of an outcome is \\(\\pi\\), then the corresponding odds of that outcome is\n\\[\\omega(\\pi) \\stackrel{\\text{def}}{=}\\frac{\\pi}{1-\\pi}\\]\nThis function, which transforms probabilities into odds, is called the odds function (see Figure 3.1).\n\n\n\nCode\nodds = function(pi) pi/(1-pi)\nlibrary(ggplot2)\nodds_plot = ggplot() + \n  geom_function(fun = odds, aes(col = \"odds function\")) +\n  xlim(0, .5) +\n  xlab(\"Probability\") +\n  ylab(\"Odds\") +\n  geom_abline(aes(intercept = 0, slope = 1, col = \"y=x\")) + \n  theme_bw()\nprint(odds_plot)\n\n\n\n\nFigure 3.1: Odds versus probability\n\n\n\n\n\n\nExample 3.5 (Calculating odds) In Exercise 3.3, we estimated that the probability of MI, given OC use, is \\(\\pi(OC) \\stackrel{\\text{def}}{=}\\Pr(MI|OC) = 0.0026\\). If this estimate is correct, then the odds of MI, given OC use, is:\n\\[\n\\begin{aligned}\n\\omega(OC)\n&\\stackrel{\\text{def}}{=}\\frac{\\Pr(MI|OC)}{\\Pr(\\neg MI|OC)}\\\\\n&=\\frac{\\Pr(MI|OC)}{1-\\Pr(MI|OC)}\\\\\n&=\\frac{\\pi(OC)}{1-\\pi(OC)}\\\\\n&=\\frac{0.0026}{1-0.0026}\\\\\n&=0.00260678\n\\end{aligned}\n\\]\n\n\nExercise 3.4 (Calculating odds) Estimate the odds of MI, for non-OC users.\n\nSolution. \\[\n\\omega_(\\neg OC) = 7.00490343\\times 10^{-4}\n\\]\n\n\n\nA shortcut for calculating odds\nThe usual estimate for a probability of an event is “# events/# observations”. We often denote # events as \\(x\\) and # observations as \\(n\\). So: \\[\\hat\\pi = \\frac{x}{n}\\] Thus, the usual estimate for the probability of a nonevent is:\n\\[\n\\begin{aligned}\n1-\\hat\\pi\n&= 1-\\frac{x}{n}\\\\\n&= \\frac{n}{n} - \\frac{x}{n}\\\\\n&= \\frac{n - x}{n}\n\\end{aligned}\n\\]\nThus, the estimated odds is: \\[\n\\begin{aligned}\n\\frac{\\hat\\pi}{1-\\hat\\pi}\n&= \\frac{\\left(\\frac{x}{n}\\right)}{\\left(\\frac{n-x}{n}\\right)}\\\\\n&= \\frac{x}{n-x}\n\\end{aligned}\n\\] That is, odds can be calculated directly as “# events” divided by “# nonevents” (without needing to calculate \\(\\hat\\pi\\) and \\(1-\\hat\\pi\\) first).\n\nExample 3.6 (calculating odds using the shortcut) In Example 3.5, we calculated \\[\n\\begin{aligned}\n\\omega(OC)\n&=0.00260678\n\\end{aligned}\n\\] Let’s recalculate this result using our shortcut:\n\\[\n\\begin{aligned}\n\\omega(OC)\n&=\\frac{13}{5000-13}\\\\\n&=0.00260678\n\\end{aligned}\n\\]\nSame answer!\n\n\n\nOdds of rare events\nFor rare events (small \\(\\pi\\)), odds and probabilities are nearly equal, because \\(1-\\pi \\approx 1\\) (see Figure 3.1).\nFor example, in Example 3.5, the probability and odds differ by \\(6.77762182\\times 10^{-6}\\).\n\nExercise 3.5 What odds value corresponds to the probability \\(\\pi = 0.2\\), and what is the numerical difference between these two values?\n\nSolution. \\[\n\\omega = \\frac{\\pi}{1-\\pi}\n=\\frac{.2}{.8}\n= .25\n\\]\n\n\n\n\n\n3.2.3 The inverse odds function\n\nDefinition 3.6 (inverse odds function) The inverse odds function, \\[\\pi(\\omega)\\stackrel{\\text{def}}{=}\\frac{\\omega}{1+\\omega}\\] converts odds into their corresponding probabilities (Figure 3.2).\n\nThe inverse-odds function takes an odds as input and produces a probability as output. Its domain of inputs is \\([0,\\infty)\\) and its range of outputs is \\([0,1]\\).\n\n\nCode\nodds_inv = function(omega) omega / (1 + omega)\nggplot() +\n  geom_function(fun = odds_inv, aes(col = \"inverse-odds\")) +\n  xlab(\"Odds\") +\n  ylab(\"Probability\") +\n  xlim(0,5) +\n  ylim(0,1) +\n  geom_abline(aes(intercept = 0, slope = 1, col = \"x=y\"))\n\n\n\n\nFigure 3.2: The inverse odds function, \\(\\pi(\\omega)\\)\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAn equivalent expression for the inverse odds function is \\[\\pi(\\omega) = (1-\\omega^{-1})^{-1} \\tag{3.1}\\]\n\n\n\nExercise 3.6 Prove that Equation 3.1 is equivalent to Definition 3.6.\n\n\nExercise 3.7 What probability corresponds to an odds of \\(\\omega = 1\\), and what is the numerical difference between these two values?\n\nSolution. \\[\n\\pi(1) = \\frac{1}{1+1}\n=\\frac{1}{2}\n= .5\n\\] \\[\n1 - \\pi(1) = 1 - .5 = .5\n\\]\n\n\n\n\n3.2.4 Odds ratios\nNow that we have defined odds, we can introduce another way of comparing event probabilities: odds ratios.\n\nDefinition 3.7 (Odds ratio) The odds ratio for two odds \\(\\omega_1\\), \\(\\omega_2\\) is their ratio:\n\\[\\theta(\\omega_1, \\omega_2) = \\frac{\\omega_1}{\\omega_2}\\]\n\n\nExample 3.7 (Calculating odds ratios) In Example 3.1, the odds ratio for OC users versus OC-non-users is:\n\\[\n\\begin{aligned}\n\\theta(\\omega(OC), \\omega(\\neg OC))\n&= \\frac{\\omega(OC)}{\\omega(\\neg OC)}\\\\\n&= \\frac{0.0026}{7\\times 10^{-4}}\\\\\n&= 3.71428571\\\\\n\\end{aligned}\n\\]\n\nWhen the outcome is rare (i.e., its probability is small) for both groups being compared in an odds ratio, the odds of the outcome will be similar to the probability of the outcome, and thus the risk ratio will be similar to the odds ratio.\nFor example, in Example 3.1, the outcome is rare for both OC and non-OC participants, so the odds for both groups are similar to the corresponding probabilities, and the odds ratio is similar the risk ratio.\n\nA shortcut for calculating odds ratio estimates\nThe general form of a two-by-two table is shown in Table 3.2.\n\n\nTable 3.2: A generic 2x2 table\n\n\n\nEvent\nNon-Event\nTotal\n\n\n\n\nExposed\na\nb\na+b\n\n\nNon-exposed\nc\nd\nc+d\n\n\nTotal\na+c\nb+d\na+b+c+d\n\n\n\n\nFrom this table, we have:\n\n\\(\\hat\\pi(Event|Exposed) = a/(a+b)\\)\n\\(\\hat\\pi(\\neg Event|Exposed) = b/(a+b)\\)\n\\(\\hat\\omega(Event|Exposed) = \\frac{\\left(\\frac{a}{a+b}\\right)}{\\left(\\frac{b}{a+b}\\right)}=\\frac{a}{b}\\)\n\\(\\hat\\omega(Event|\\neg Exposed) = \\frac{c}{d}\\) (see Exercise 3.8)\n\\(\\theta(Exposed,\\neg Exposed) = \\frac{\\frac{a}{b}}{\\frac{c}{d}} = \\frac{ad}{bc}\\)\n\n\nExercise 3.8 Given Table 3.2, show that \\(\\hat\\omega(Event|\\neg Exposed) = \\frac{c}{d}\\).\n\n\n\nProperties of odds ratios\nOdds ratios have a special property: we can swap a covariate with the outcome, and the odds ratio remains the same.\n\nExample 3.8 In Example 3.1, we have:\n\\[\n\\begin{aligned}\n\\theta(MI; OC)\n&\\stackrel{\\text{def}}{=}\n\\frac{\\omega(MI|OC)}{\\omega(MI|\\neg OC)}\\\\\n&\\stackrel{\\text{def}}{=}\\frac\n{\\left(\\frac{\\Pr(MI|OC)}{\\Pr(\\neg MI|OC)}\\right)}\n{\\left(\\frac{\\Pr(MI|\\neg OC)}{\\Pr(\\neg MI|\\neg OC)}\\right)}\\\\\n&= \\frac\n{\\left(\\frac{\\Pr(MI,OC)}{\\Pr(\\neg MI,OC)}\\right)}\n{\\left(\\frac{\\Pr(MI,\\neg OC)}{\\Pr(\\neg MI,\\neg OC)}\\right)}\\\\\n&= \\left(\\frac{\\Pr(MI,OC)}{\\Pr(\\neg MI,OC)}\\right)\n\\left(\\frac{\\Pr(\\neg MI,\\neg OC)}{\\Pr(MI,\\neg OC)}\\right)\\\\\n&= \\left(\\frac{\\Pr(MI,OC)}{\\Pr(MI,\\neg OC)}\\right)\n\\left(\\frac{\\Pr(\\neg MI,\\neg OC)}{\\Pr(\\neg MI,OC)}\\right)\\\\\n&= \\left(\\frac{\\Pr(OC,MI)}{\\Pr(\\neg OC,MI)}\\right)\n\\left(\\frac{\\Pr(\\neg OC,\\neg MI)}{\\Pr(OC,\\neg MI)}\\right)\\\\\n&= \\left(\\frac{\\Pr(OC|MI)}{\\Pr(\\neg OC|MI)}\\right)\n\\left(\\frac{\\Pr(\\neg OC|\\neg MI)}{\\Pr(OC|\\neg MI)}\\right)\\\\\n&= \\frac{\\left(\\frac{\\Pr(OC|MI)}{\\Pr(\\neg OC|MI)}\\right)}\n{\\left(\\frac{\\Pr(OC|\\neg MI)}{\\Pr(\\neg OC|\\neg MI)}\\right)}\\\\\n&\\stackrel{\\text{def}}{=}\\frac{\\omega(OC|MI)}\n{\\omega(OC|\\neg MI)}\\\\\n&\\stackrel{\\text{def}}{=}\\theta(OC; MI)\n\\end{aligned}\n\\]\n\n\nExercise 3.9 For Table 3.2, show that \\(\\hat\\theta(Exposed, Unexposed) = \\hat\\theta(Event, \\neg Event)\\).\n\n\n\n\n3.2.5 Effect of study design\nTable 3.1 simulates a follow-up study in which two populations were followed and the number of MI’s was observed. The risks are \\(P(MI|OC)\\) and \\(P(MI|\\neg OC)\\) and we can estimate these risks from the data.\nBut suppose we had a case-control study in which we had 100 women with MI and selected a comparison group of 100 women without MI (matched as groups on age, etc.). Then MI is not random, and we cannot compute P(MI|OC) and we cannot compute the risk ratio. However, the odds ratio however can be computed.\nThe disease odds ratio is the odds for the disease in the exposed group divided by the odds for the disease in the unexposed group, and we cannot validly compute and use these separate parts.\nBut we can validly compute and use the exposure odds ratio, which is the odds for exposure in the disease group divided by the odds for exposure in the non-diseased group (because exposure can be treated as random):\n\\[\n\\hat\\theta(OC|MI) =\n\\frac{\\hat{\\omega}(OC|MI)}{\\hat{\\omega}(OC|\\neg MI)}\n\\]\nAnd these two odds ratios, \\(\\hat\\theta(MI|OC)\\) and \\(\\hat\\theta(OC|MI)\\) are mathematically equivalent, as we saw in Section 3.2.4.2.\n\nExercise 3.10 Calculate the odds ratio of MI with respect to OC use, assuming that Table 3.1 comes from a case-control study. Confirm that the result is the same as in Example 3.7.\n\nSolution. \n\n\\(\\omega(OC|MI) = P(OC|MI)/(1 – P(OC|MI) = \\frac{13}{7} = 1.85714286\\)\n\\(\\omega(OC|\\neg MI) = P(OC|\\neg MI)/(1 – P(OC|\\neg MI) = \\frac{4987}{9993} = 0.49904933\\)\n\\(\\theta(OC,MI) = \\frac{\\omega(OC|MI)}{\\omega(OC|\\neg MI)} = \\frac{13/7}{4987/9993} = 3.72136125\\)\n\nThis is the same estimate we calculated in Example 3.7.\n\n\n\nCross-Sectional Studies\n\nIf a cross-sectional study is a probability sample of a population (which it rarely is) then we can estimate risks.\nIf it is a sample, but not an unbiased probability sample, then we need to treat it in the same way as a case-control study.\nWe can validly estimate odds ratios in either case.\nBut we can usually not validly estimate risks and risk ratios."
  },
  {
    "objectID": "logistic-regression.html#introduction-to-logistic-regression",
    "href": "logistic-regression.html#introduction-to-logistic-regression",
    "title": "3  Logistic Regression",
    "section": "3.3 Introduction to logistic regression",
    "text": "3.3 Introduction to logistic regression\n\nIn Example 3.1, we estimated the risk and the odds of MI for two discrete cases, as to whether of not the individual used oral contraceptives.\nIf the predictor is quantitative (dose) or there is more than one predictor, the task becomes more difficult.\nIn this case, we will use logistic regression, which is a generalization of the linear regression models you have been using that can account for a binary response instead of a continuous one.\n\n\n3.3.1 Binary outcomes models - one group, no covariates\n\\[\n\\begin{aligned}\np(Y=1) &= \\pi\\\\\np(Y=0) &= 1-\\pi\\\\\np(Y=y) &= \\pi^y (1-\\pi)^{1-y}\\\\\n\\mathbf y  &= (y_1, ..., y_n)\\\\\n\\mathcal L(\\pi;\\mathbf y) &= \\pi^{\\sum y_i} (1-\\pi)^{n - \\sum y_i}\\\\\n\\ell(\\pi, \\mathbf y) &= \\left({\\sum y_i}\\right) \\text{log}\\left\\{\\pi\\right\\} + \\left(n - \\sum y_i\\right) \\text{log}\\left\\{1-\\pi\\right\\}\\\\\n&= \\left({\\sum y_i}\\right) \\left(\\text{log}\\left\\{\\pi\\right\\} - \\text{log}\\left\\{1-\\pi\\right\\}\\right) + n \\cdot \\text{log}\\left\\{1-\\pi\\right\\}\\\\\n&= \\left({\\sum y_i}\\right) \\text{log}\\left\\{\\frac{\\pi}{ 1-\\pi}\\right\\} + n \\cdot \\text{log}\\left\\{1-\\pi\\right\\}\n\\end{aligned}\n\\]\n\n\n3.3.2 Binary outcomes - general\n\\[\n\\begin{aligned}\np(Y_i=1) &= \\pi_i\\\\\np(Y_i=0) &= 1-\\pi_i\\\\\np(Y_i=y) &= (\\pi_i)^y (1-\\pi_i)^{1-y}\\\\\n\\mathbf y  &= (y_1, ..., y_n)\\\\\n\\mathcal L(\\pi;\\mathbf y) &= \\prod_{i=1}^n\n(\\pi_i)^{y_i} (1-\\pi_i)^{1 - y_i}\\\\\n\\ell(\\pi, \\mathbf y) &=\n\\sum_{i=1}^n\ny_i \\text{log}\\left\\{\\pi_i\\right\\} + (1 - y_i) \\text{log}\\left\\{1-\\pi_i\\right\\}\n\\end{aligned}\n\\]\n\n\n3.3.3 Modeling \\(\\pi_i\\) as a function of \\(X_i\\)\nIf there are only a few distinct \\(X_i\\) values, we can model each one separately.\nOtherwise, we need regression.\n\\[\n\\begin{aligned}\n\\pi(x) &\\equiv \\text{E}(Y=1|X=x)\\\\\n&= f(x^\\top\\beta)\n\\end{aligned}\n\\]\nTypically, we use \\(f(\\eta) = \\text{expit}(\\eta)\\).\n\nDefinition 3.8 (expit function) The expit function (Figure 3.3), also known as the inverse-logit or logistic function, is:\n\\[\n\\begin{aligned}\n\\text{expit}(\\eta)\n&\\stackrel{\\text{def}}{=}\\frac{\\text{exp}\\left\\{\\eta\\right\\}}{1+\\text{exp}\\left\\{\\eta\\right\\}}\\\\\n&= (1 + \\text{exp}\\left\\{-\\eta\\right\\})^{-1}\n\\end{aligned}\n\\]\n\n\nCode\nexpit = function(eta) exp(eta)/(1+exp(eta))\nlibrary(ggplot2)\nexpit_plot = \n  ggplot() + \n  geom_function(fun = expit) + \n  xlim(-5, 5) + \n  ylim(0,1) +\n  ylab(expression(expit(eta))) +\n  xlab(expression(eta)) +\n  theme_bw()\nprint(expit_plot)\n\n\n\n\nFigure 3.3: The expit function\n\n\n\n\n\n\n\nDefinition 3.9 (logit function) The inverse of the expit function is the logit function:\n\\[g(p) = f^{-1}(p) = \\text{logit}(p) = \\text{log}\\left\\{\\frac{p}{1-p}\\right\\}\\]\n\n\n\nCode\nlogit = function(p) log(odds(p))\n\nlogit_plot = \n  ggplot() + \n  geom_function(fun = logit) + \n  xlim(.01, .99) + \n  ylab(\"logit(p)\") +\n  xlab(\"p\") +\n  theme_bw()\nprint(logit_plot)\n\n\n\n\nFigure 3.4: the logit function\n\n\n\n\n\n\n\n3.3.4 Diagram of expit and logit\n\\[\n\\underbrace{\\pi}_{\\atop{\\Pr(Y=1)} }\n\\overbrace{\n\\underbrace{\n\\underset{\n\\xleftarrow[\\frac{\\omega}{1+\\omega}]{}\n}\n{\n\\xrightarrow{\\frac{\\pi}{1-\\pi}}\n}\n\\underbrace{\\omega}_{\\text{odds}(Y=1)}\n\\underset{\n\\xleftarrow[\\text{exp}\\left\\{\\eta\\right\\}]{}\n}\n{\n\\xrightarrow{\\text{log}\\left\\{\\omega\\right\\}}\n}\n}_{\\text{expit}(\\eta)}\n}^{\\text{logit}(\\pi)}\n\\underbrace{\\eta}_{\\atop{\\text{log-odds}(Y=1)}}\n\\]\n\n\n3.3.5 Meet the beetles\n\n\nCode\nlibrary(glmx)\n\ndata(BeetleMortality, package = \"glmx\")\nbeetles = BeetleMortality |&gt;\n  mutate(\n    pct = died/n,\n    survived = n - died\n  )\n\nplot1 = \n  beetles |&gt; \n  ggplot(aes(x = dose, y = pct)) +\n  geom_point(aes(size = n)) +\n  xlab(\"Dose (log mg/L)\") +\n  ylab(\"Mortality rate (%)\") +\n  scale_y_continuous(labels = scales::percent) +\n  scale_size(range = c(1,2)) +\n  theme_bw(base_size = 18)\n\nprint(plot1)\n\n\n\n\n\nMortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\n3.3.6 Why don’t we use linear regression?\n\n\nCode\nbeetles_glm_grouped = beetles |&gt; \n  glm(formula = cbind(died, survived) ~ dose, family = \"binomial\")\n\nlm1 = \n  beetles  |&gt; \n  reframe(\n    .by = everything(),\n    outcome = c(\n      rep(1, times = died), \n      rep(0, times = survived))\n  ) |&gt; \n  lm(\n    formula = outcome ~ dose, \n    data = _)\n\nlm2 = \n  beetles  |&gt; \n  reframe(\n    .by = everything(),\n    outcome = c(\n      rep(1, times = died), \n      rep(0, times = survived))\n  ) |&gt; \n  lm(\n    formula = outcome ~ log(dose), \n    data = _)\n\nrange1 = range(beetles$dose) + c(-.2, .2)\nf = function(x) predict(beetles_glm_grouped, newdata = data.frame(dose = x), type = \"response\")\nf.linear = function(x) predict(lm1, newdata = data.frame(dose = x))\nf.linearlog = function(x) predict(lm2, newdata = data.frame(dose = x))\n\nplot2 = \n  plot1 + \n  geom_function(\n    fun = f.linear, \n    aes(col = \"Straight line\")) +\n  labs(colour=\"Model\", size = \"\")\n\nplot2 |&gt; print()\n\n\n\n\n\n\n\n3.3.7 Zoom out\n\n\nCode\n(plot2 + expand_limits(x = c(1.6, 2))) |&gt; print()\n\n\n\n\n\n\n\n3.3.8 log transformation of dose?\n\n\nCode\nplot3 = plot2 + \n  expand_limits(x = c(1.6, 2)) +\n  geom_function(fun = f.linearlog, aes(col = \"Log-transform dose\"))\n(plot3 + expand_limits(x = c(1.6, 2))) |&gt; print()\n\n\n\n\n\n\n\n3.3.9 Logistic regression\n\n\nCode\nplot4 = plot3 + geom_function(fun = f, aes(col = \"Logistic regression\"))\nplot4 |&gt; print()\n\n\n\n\n\n\n\n3.3.10 Three parts to regression models\n\nWhat distribution does the outcome have for a specific subpopulation defined by covariates? (outcome model)\nHow does the combination of covariates relate to the mean? (link function)\nHow do the covariates combine? (linear predictor, interactions)\n\n\n\n3.3.11 Logistic regression in R\n\n\nCode\nbeetles_glm_grouped = \n  beetles |&gt; \n  glm(\n    formula = cbind(died, survived) ~ dose, \n    family = \"binomial\")\n\nbeetles_glm_grouped |&gt; \n  parameters() |&gt; \n  print_md()\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n-60.72\n5.18\n(-71.44, -51.08)\n-11.72\n&lt; .001\n\n\ndose\n34.27\n2.91\n(28.85, 40.30)\n11.77\n&lt; .001\n\n\n\n\n\nFitted values:\n\n\nCode\nfitted.values(beetles_glm_grouped)\n\n\n     1      2      3      4      5      6      7      8 \n0.0586 0.1640 0.3621 0.6053 0.7952 0.9032 0.9552 0.9790 \n\n\nCode\npredict(beetles_glm_grouped, type = \"response\")\n\n\n     1      2      3      4      5      6      7      8 \n0.0586 0.1640 0.3621 0.6053 0.7952 0.9032 0.9552 0.9790 \n\n\nCode\npredict(beetles_glm_grouped, type = \"link\")\n\n\n      1       2       3       4       5       6       7       8 \n-2.7766 -1.6286 -0.5662  0.4277  1.3564  2.2337  3.0596  3.8444 \n\n\nCode\nfit_y = beetles$n * fitted.values(beetles_glm_grouped)\n\n\n\n\n3.3.12 Individual observations\n\n\nCode\nbeetles_long = \n  beetles  |&gt; \n  reframe(\n    .by = everything(),\n    outcome = c(\n      rep(1, times = died), \n      rep(0, times = survived))\n  )\nbeetles_long |&gt; tibble() |&gt; print()\n\n\n# A tibble: 481 × 6\n    dose  died     n   pct survived outcome\n   &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;    &lt;int&gt;   &lt;dbl&gt;\n 1  1.69     6    59 0.102       53       1\n 2  1.69     6    59 0.102       53       1\n 3  1.69     6    59 0.102       53       1\n 4  1.69     6    59 0.102       53       1\n 5  1.69     6    59 0.102       53       1\n 6  1.69     6    59 0.102       53       1\n 7  1.69     6    59 0.102       53       0\n 8  1.69     6    59 0.102       53       0\n 9  1.69     6    59 0.102       53       0\n10  1.69     6    59 0.102       53       0\n# ℹ 471 more rows\n\n\nHere’s the model with individual data\n\n\nCode\nbeetles_glm_ungrouped = \n  beetles_long |&gt; \n  glm(\n    formula = outcome ~ dose, \n    family = \"binomial\")\n\nbeetles_glm_ungrouped |&gt; parameters() |&gt; print_md()\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n-60.72\n5.18\n(-71.44, -51.08)\n-11.72\n&lt; .001\n\n\ndose\n34.27\n2.91\n(28.85, 40.30)\n11.77\n&lt; .001\n\n\n\n\n\nHere’s the previous version again:\n\n\nCode\nbeetles_glm_grouped |&gt; parameters() |&gt; print_md()\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n-60.72\n5.18\n(-71.44, -51.08)\n-11.72\n&lt; .001\n\n\ndose\n34.27\n2.91\n(28.85, 40.30)\n11.77\n&lt; .001\n\n\n\n\n\nThey seem the same! But not quite:\n\n\nCode\nlogLik(beetles_glm_grouped)\n\n\n'log Lik.' -18.72 (df=2)\n\n\nCode\nlogLik(beetles_glm_ungrouped)\n\n\n'log Lik.' -186.2 (df=2)\n\n\nThe difference is due to the binomial coefficient \\(\\left(n\\atop x \\right)\\) which isn’t included in the individual-observations (Bernoulli) version of the model."
  },
  {
    "objectID": "logistic-regression.html#multiple-logistic-regression",
    "href": "logistic-regression.html#multiple-logistic-regression",
    "title": "3  Logistic Regression",
    "section": "3.4 Multiple logistic regression",
    "text": "3.4 Multiple logistic regression\n\n3.4.1 Coronary heart disease (WCGS) study data\nLet’s use the data from the following study to explore multiple logistic regression:\n\nSummary of study\nFrom Vittinghoff et al. (2012):\n“The Western Collaborative Group Study (WCGS) was a large epidemiological study designed to investigate the association between the”type A” behavior pattern and coronary heart disease (CHD) (Rosenman et al. 1964).”\nFrom Wikipedia, “Type A and Type B personality theory”:\n“The hypothesis describes Type A individuals as outgoing, ambitious, rigidly organized, highly status-conscious, impatient, anxious, proactive, and concerned with time management….\nThe hypothesis describes Type B individuals as a contrast to those of Type A. Type B personalities, by definition, are noted to live at lower stress levels. They typically work steadily and may enjoy achievement, although they have a greater tendency to disregard physical or mental stress when they do not achieve.”\n\n\nStudy design\nfrom ?faraway::wcgs:\n3154 healthy young men aged 39-59 from the San Francisco area were assessed for their personality type. All were free from coronary heart disease at the start of the research. Eight and a half years later change in CHD status was recorded.\nDetails (from faraway::wcgs)\nThe WCGS began in 1960 with 3,524 male volunteers who were employed by 11 California companies. Subjects were 39 to 59 years old and free of heart disease as determined by electrocardiogram. After the initial screening, the study population dropped to 3,154 and the number of companies to 10 because of various exclusions. The cohort comprised both blue- and white-collar employees.\nAt baseline the following information was collected:\n\nsocio-demographic including:\nage\neducation\nmarital status\nincome\noccupation\nphysical and physiological including:\nheight\nweight\nblood pressure\nelectrocardiogram\ncorneal arcus;\nbiochemical including:\ncholesterol and lipoprotein fractions;\nmedical and family history and use of medications;\nbehavioral data including\nType A interview,\nsmoking,\nexercise\nalcohol use.\n\nLater surveys added data on:\n\nanthropometry\ntriglycerides\nJenkins Activity Survey\ncaffeine use\n\nAverage follow-up continued for 8.5 years with repeat examinations.\nReference: Coronary Heart Disease in the Western Collaborative Group Study Final Follow-up Experience of 8 1/2 Years Ray H. Rosenman, MD; Richard J. Brand, PhD; C. David Jenkins, PhD; Meyer Friedman, MD; Reuben Straus, MD; Moses Wurm, MD JAMA. 1975;233(8):872-877. doi:10.1001/jama.1975.03260080034016.\n\n\n\n3.4.2 Load the data\nHere, I load the data:\n\n\nCode\n## load the data directly from a UCSF website:\n# library(haven)\n# url = paste0( \n#     # I'm breaking up the url into two chunks for readability\n#     \"https://regression.ucsf.edu/sites/g/files/\",\n#     \"tkssra6706/f/wysiwyg/home/data/wcgs.dta\")\n# wcgs = haven::read_dta(url)\n\n\n# I presaved the data in my project's `data` folder\nlibrary(here) # provides the `here()` function\nlibrary(fs) # provides the `path()` function\nhere::here() |&gt; \n  fs::path('data/wcgs.rda') |&gt; \n  load()\n\n\n\n\n3.4.3 Now let’s do some data cleaning\n\n\nCode\nlibrary(arsenal) # provides `set_labels()`\n\nwcgs = wcgs |&gt; \n  mutate(\n    age = age |&gt; \n      arsenal::set_labels(\"Age (years)\"),\n    \n    arcus = \n      arcus |&gt; \n      as.logical() |&gt; \n      arsenal::set_labels(\"Arcus Senilis\"),\n    \n    time169 = \n      time169 |&gt; \n      as.numeric() |&gt; \n      arsenal::set_labels(\"Observation (follow up) time (days)\"),\n    \n    dibpat =\n      dibpat |&gt; \n      as_factor() |&gt; \n      relevel(ref = \"Type A\") |&gt; \n      arsenal::set_labels(\"Behavioral Pattern\"),\n    \n    typchd69 = typchd69 |&gt; \n      labelled(\n        label = \"Type of CHD Event\",\n        labels = \n          c(\n            \"None\" = 0, \n            \"infdeath\" = 1,\n            \"silent\" = 2,\n            \"angina\" = 3)),\n    \n    # turn stata-style labelled variables in to R-style factors:\n    across(\n      where(is.labelled), \n      haven::as_factor)\n  )\n\n\n\n\n3.4.4 What’s in the data\nHere’s a table of the data:\n\n\nCode\nwcgs |&gt;\n  select(-c(id, uni, t1)) |&gt;\n  tableby(chd69 ~ ., data = _) |&gt;\n  summary(\n    pfootnote = TRUE,\n    title =\n      \"Baseline characteristics by CHD status at end of follow-up\")\n\n\nBaseline characteristics by CHD status at end of follow-up\n\n\n\n\n\n\n\n\n\n\nNo (N=2897)\nYes (N=257)\nTotal (N=3154)\np value\n\n\n\n\nAge (years)\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n46.082 (5.457)\n48.490 (5.801)\n46.279 (5.524)\n\n\n\n   Range\n39.000 - 59.000\n39.000 - 59.000\n39.000 - 59.000\n\n\n\nArcus Senilis\n\n\n\n&lt; 0.0012\n\n\n   N-Miss\n0\n2\n2\n\n\n\n   FALSE\n2058 (71.0%)\n153 (60.0%)\n2211 (70.1%)\n\n\n\n   TRUE\n839 (29.0%)\n102 (40.0%)\n941 (29.9%)\n\n\n\nBehavioral Pattern\n\n\n\n&lt; 0.0012\n\n\n   A1\n234 (8.1%)\n30 (11.7%)\n264 (8.4%)\n\n\n\n   A2\n1177 (40.6%)\n148 (57.6%)\n1325 (42.0%)\n\n\n\n   B3\n1155 (39.9%)\n61 (23.7%)\n1216 (38.6%)\n\n\n\n   B4\n331 (11.4%)\n18 (7.0%)\n349 (11.1%)\n\n\n\nBody Mass Index (kg/m2)\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n24.471 (2.561)\n25.055 (2.579)\n24.518 (2.567)\n\n\n\n   Range\n11.191 - 37.653\n19.225 - 38.947\n11.191 - 38.947\n\n\n\nTotal Cholesterol\n\n\n\n&lt; 0.0011\n\n\n   N-Miss\n12\n0\n12\n\n\n\n   Mean (SD)\n224.261 (42.217)\n250.070 (49.396)\n226.372 (43.420)\n\n\n\n   Range\n103.000 - 400.000\n155.000 - 645.000\n103.000 - 645.000\n\n\n\nDiastolic Blood Pressure\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n81.723 (9.621)\n85.315 (10.311)\n82.016 (9.727)\n\n\n\n   Range\n58.000 - 150.000\n64.000 - 122.000\n58.000 - 150.000\n\n\n\nBehavioral Pattern\n\n\n\n&lt; 0.0012\n\n\n   Type A\n1411 (48.7%)\n178 (69.3%)\n1589 (50.4%)\n\n\n\n   Type B\n1486 (51.3%)\n79 (30.7%)\n1565 (49.6%)\n\n\n\nHeight (inches)\n\n\n\n0.2901\n\n\n   Mean (SD)\n69.764 (2.539)\n69.938 (2.410)\n69.778 (2.529)\n\n\n\n   Range\n60.000 - 78.000\n63.000 - 77.000\n60.000 - 78.000\n\n\n\nLn of Systolic Blood Pressure\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n4.846 (0.110)\n4.900 (0.125)\n4.850 (0.112)\n\n\n\n   Range\n4.585 - 5.438\n4.605 - 5.298\n4.585 - 5.438\n\n\n\nLn of Weight\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n5.126 (0.123)\n5.155 (0.118)\n5.128 (0.123)\n\n\n\n   Range\n4.357 - 5.670\n4.868 - 5.768\n4.357 - 5.768\n\n\n\nCigarettes per day\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n11.151 (14.329)\n16.665 (15.657)\n11.601 (14.518)\n\n\n\n   Range\n0.000 - 99.000\n0.000 - 60.000\n0.000 - 99.000\n\n\n\nSystolic Blood Pressure\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n128.034 (14.746)\n135.385 (17.473)\n128.633 (15.118)\n\n\n\n   Range\n98.000 - 230.000\n100.000 - 200.000\n98.000 - 230.000\n\n\n\nCurrent smoking\n\n\n\n&lt; 0.0012\n\n\n   No\n1554 (53.6%)\n98 (38.1%)\n1652 (52.4%)\n\n\n\n   Yes\n1343 (46.4%)\n159 (61.9%)\n1502 (47.6%)\n\n\n\nObservation (follow up) time (days)\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n2775.158 (562.205)\n1654.700 (859.297)\n2683.859 (666.524)\n\n\n\n   Range\n238.000 - 3430.000\n18.000 - 3229.000\n18.000 - 3430.000\n\n\n\nType of CHD Event\n\n\n\n\n\n\n   None\n0 (0.0%)\n0 (0.0%)\n0 (0.0%)\n\n\n\n   infdeath\n2897 (100.0%)\n0 (0.0%)\n2897 (91.9%)\n\n\n\n   silent\n0 (0.0%)\n135 (52.5%)\n135 (4.3%)\n\n\n\n   angina\n0 (0.0%)\n71 (27.6%)\n71 (2.3%)\n\n\n\n   4\n0 (0.0%)\n51 (19.8%)\n51 (1.6%)\n\n\n\nWeight (lbs)\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n169.554 (21.010)\n174.463 (21.573)\n169.954 (21.096)\n\n\n\n   Range\n78.000 - 290.000\n130.000 - 320.000\n78.000 - 320.000\n\n\n\nWeight Category\n\n\n\n&lt; 0.0012\n\n\n   &lt; 140\n217 (7.5%)\n15 (5.8%)\n232 (7.4%)\n\n\n\n   140-170\n1440 (49.7%)\n98 (38.1%)\n1538 (48.8%)\n\n\n\n   170-200\n1049 (36.2%)\n122 (47.5%)\n1171 (37.1%)\n\n\n\n   &gt; 200\n191 (6.6%)\n22 (8.6%)\n213 (6.8%)\n\n\n\nRECODE of age (Age)\n\n\n\n&lt; 0.0012\n\n\n   35-40\n512 (17.7%)\n31 (12.1%)\n543 (17.2%)\n\n\n\n   41-45\n1036 (35.8%)\n55 (21.4%)\n1091 (34.6%)\n\n\n\n   46-50\n680 (23.5%)\n70 (27.2%)\n750 (23.8%)\n\n\n\n   51-55\n463 (16.0%)\n65 (25.3%)\n528 (16.7%)\n\n\n\n   56-60\n206 (7.1%)\n36 (14.0%)\n242 (7.7%)\n\n\n\n\n\nLinear Model ANOVA\nPearson’s Chi-squared test\n\n\n\n\n3.4.5 Data by age and personality type\nFor now, we will look at the interaction between age and personality type (dibpat). To make it easier to visualize the data, we summarize the event rates for each combination of age:\n\n\nCode\nchd_grouped_data = \n  wcgs |&gt; \n  summarize(\n    .by = c(age, dibpat),\n    n = n(),\n    `p(chd)` = mean(chd69 == \"Yes\") |&gt; \n      labelled(label = \"CHD Event by 1969\"),\n    `odds(chd)` = `p(chd)`/(1-`p(chd)`),\n    `logit(chd)` = log(`odds(chd)`)\n  )\n\nchd_grouped_data\n\n\n\n\n\nage\ndibpat\nn\np(chd)\nodds(chd)\nlogit(chd)\n\n\n\n\n50\nType A\n76\n0.105263\n0.1176\n-2.140\n\n\n51\nType A\n67\n0.164179\n0.1964\n-1.627\n\n\n59\nType A\n30\n0.233333\n0.3043\n-1.190\n\n\n44\nType A\n113\n0.079646\n0.0865\n-2.447\n\n\n47\nType A\n72\n0.097222\n0.1077\n-2.228\n\n\n40\nType A\n133\n0.067669\n0.0726\n-2.623\n\n\n41\nType A\n108\n0.064815\n0.0693\n-2.669\n\n\n43\nType A\n97\n0.072165\n0.0778\n-2.554\n\n\n54\nType A\n53\n0.132075\n0.1522\n-1.883\n\n\n48\nType A\n80\n0.150000\n0.1765\n-1.735\n\n\n39\nType A\n128\n0.085938\n0.0940\n-2.364\n\n\n49\nType A\n67\n0.238806\n0.3137\n-1.159\n\n\n55\nType A\n55\n0.163636\n0.1957\n-1.631\n\n\n56\nType A\n49\n0.244898\n0.3243\n-1.126\n\n\n42\nType A\n101\n0.039604\n0.0412\n-3.188\n\n\n45\nType A\n77\n0.090909\n0.1000\n-2.303\n\n\n46\nType A\n91\n0.065934\n0.0706\n-2.651\n\n\n57\nType A\n31\n0.129032\n0.1481\n-1.909\n\n\n53\nType A\n62\n0.112903\n0.1273\n-2.061\n\n\n52\nType A\n65\n0.200000\n0.2500\n-1.386\n\n\n58\nType A\n34\n0.147059\n0.1724\n-1.758\n\n\n45\nType B\n109\n0.045872\n0.0481\n-3.035\n\n\n41\nType B\n125\n0.040000\n0.0417\n-3.178\n\n\n47\nType B\n75\n0.013333\n0.0135\n-4.304\n\n\n39\nType B\n138\n0.057971\n0.0615\n-2.788\n\n\n49\nType B\n67\n0.074627\n0.0806\n-2.518\n\n\n51\nType B\n56\n0.107143\n0.1200\n-2.120\n\n\n42\nType B\n121\n0.008264\n0.0083\n-4.787\n\n\n50\nType B\n59\n0.050847\n0.0536\n-2.927\n\n\n44\nType B\n122\n0.032787\n0.0339\n-3.384\n\n\n56\nType B\n27\n0.000000\n0.0000\n-Inf\n\n\n40\nType B\n144\n0.020833\n0.0213\n-3.850\n\n\n58\nType B\n22\n0.045455\n0.0476\n-3.045\n\n\n48\nType B\n84\n0.083333\n0.0909\n-2.398\n\n\n43\nType B\n118\n0.050847\n0.0536\n-2.927\n\n\n53\nType B\n43\n0.116279\n0.1316\n-2.028\n\n\n54\nType B\n54\n0.074074\n0.0800\n-2.526\n\n\n46\nType B\n79\n0.063291\n0.0676\n-2.695\n\n\n52\nType B\n48\n0.041667\n0.0435\n-3.135\n\n\n55\nType B\n25\n0.040000\n0.0417\n-3.178\n\n\n57\nType B\n32\n0.093750\n0.1034\n-2.269\n\n\n59\nType B\n17\n0.235294\n0.3077\n-1.179\n\n\n\n\n\n\n\n3.4.6 Graphical exploration\n\nProbability scale\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggeasy)\nlibrary(scales)\nchd_plot_probs = \n  chd_grouped_data |&gt; \n  ggplot(\n    aes(\n      x = age, \n      y = `p(chd)`, \n      col = dibpat)\n  ) +\n  geom_point(aes(size = n), alpha = .7) + \n  scale_size(range = c(1,4)) +\n  geom_line() +\n  theme_bw() +\n  ylab(\"P(CHD Event by 1969)\") +\n  scale_y_continuous(labels = scales::label_percent()) +\n  ggeasy::easy_labs()\n\n\n\n\nCode\nggplotly(chd_plot_probs)\n\n\n\n\n\n\n\n\nOdds scale\n\n\nCode\nchd_plot_odds = \n  chd_grouped_data |&gt; \n  ggplot(\n    aes(\n      x = age, \n      y = `odds(chd)`, \n      col = dibpat)\n  ) +\n  geom_point(aes(size = n), alpha = .7) + \n  scale_size(range = c(1,4)) +\n  geom_line() +\n  theme_bw() +\n  ylab(\"odds(CHD Event by 1969)\") +\n  ggeasy::easy_labs()\n\n\n\n\nCode\nggplotly(chd_plot_odds)\n\n\n\n\n\n\n\n\nLog-odds (logit) scale\n\n\nCode\nchd_plot_logit = \n  chd_grouped_data |&gt; \n  ggplot(\n    aes(\n      x = age, \n      y = `logit(chd)`, \n      col = dibpat)\n  ) +\n  geom_point(aes(size = n), alpha = .7) + \n  scale_size(range = c(1,4)) +\n  geom_line() +\n  theme_bw() +\n  ylab(\"log{odds(CHD Event by 1969)}\") +\n  ggeasy::easy_labs()\n\n\n\n\nCode\nggplotly(chd_plot_logit)\n\n\n\n\n\n\n\n\n\n3.4.7 Logistic regression models for CHD data\nHere, we fit stratified models for CHD by personality type.\n\n\nCode\nchd_glm_strat = glm(\n  \"formula\" = chd69 == \"Yes\" ~ dibpat + dibpat:age - 1, \n  \"data\" = wcgs,\n  \"family\" = binomial(link = \"logit\")\n)\n\nchd_glm_strat |&gt; parameters() |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\ndibpat (Type A)\n-5.50\n0.67\n(-6.83, -4.19)\n-8.18\n&lt; .001\n\n\ndibpat (Type B)\n-5.80\n0.98\n(-7.73, -3.90)\n-5.95\n&lt; .001\n\n\ndibpat (Type A) × age\n0.07\n0.01\n(0.05, 0.10)\n5.24\n&lt; .001\n\n\ndibpat (Type B) × age\n0.06\n0.02\n(0.02, 0.10)\n3.01\n0.003\n\n\n\n\n\nWe can get the corresponding odds ratios (\\(e^{\\beta}\\)s) by passing exponentiate = TRUE to parameters():\n\n\nCode\nchd_glm_strat |&gt; \n  parameters(exponentiate = TRUE) |&gt; \n  print_md()\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nOdds Ratio\nSE\n95% CI\nz\np\n\n\n\n\ndibpat (Type A)\n4.09e-03\n2.75e-03\n(1.08e-03, 0.02)\n-8.18\n&lt; .001\n\n\ndibpat (Type B)\n3.02e-03\n2.94e-03\n(4.40e-04, 0.02)\n-5.95\n&lt; .001\n\n\ndibpat (Type A) × age\n1.07\n0.01\n(1.05, 1.10)\n5.24\n&lt; .001\n\n\ndibpat (Type B) × age\n1.06\n0.02\n(1.02, 1.11)\n3.01\n0.003\n\n\n\n\n\n\n\n3.4.8 Models superimposed on data\nWe can graph our fitted models on each scale (probability, odds, log-odds).\n\nprobability scale\n\n\nCode\ncurve_type_A = function(x) \n{\n  chd_glm_strat |&gt; predict(\n    type = \"response\",\n    newdata = tibble(age = x, dibpat = \"Type A\"))\n}\n\ncurve_type_B = function(x) \n{\n  chd_glm_strat |&gt; predict(\n    type = \"response\",\n    newdata = tibble(age = x, dibpat = \"Type B\"))\n}\n\nchd_plot_probs_2 =\n  chd_plot_probs +\n  geom_function(\n    fun = curve_type_A,\n    aes(col = \"Type A\")\n  ) +\n  geom_function(\n    fun = curve_type_B,\n    aes(col = \"Type B\")\n  )\n\n\n\n\nCode\nggplotly(chd_plot_probs_2)\n\n\n\n\n\n\n\n\nodds scale\n\n\nCode\ncurve_type_A = function(x) \n{\n  chd_glm_strat |&gt; predict(\n    type = \"link\",\n    newdata = tibble(age = x, dibpat = \"Type A\")) |&gt; exp()\n}\ncurve_type_B = function(x) \n{\n  chd_glm_strat |&gt; predict(\n    type = \"link\",\n    newdata = tibble(age = x, dibpat = \"Type B\")) |&gt; exp()\n}\n\nchd_plot_odds_2 =\n  chd_plot_odds +\n  geom_function(\n    fun = curve_type_A,\n    aes(col = \"Type A\")\n  ) +\n  geom_function(\n    fun = curve_type_B,\n    aes(col = \"Type B\")\n  )\n\n\n\n\nCode\nggplotly(chd_plot_odds_2)\n\n\n\n\n\n\n\n\nlog-odds (logit) scale\n\n\nCode\ncurve_type_A = function(x) \n{\n  chd_glm_strat |&gt; predict(\n    type = \"link\",\n    newdata = tibble(age = x, dibpat = \"Type A\"))\n}\ncurve_type_B = function(x) \n{\n  chd_glm_strat |&gt; predict(\n    type = \"link\",\n    newdata = tibble(age = x, dibpat = \"Type B\"))\n}\n\nchd_plot_logit_2 =\n  chd_plot_logit +\n  geom_function(\n    fun = curve_type_A,\n    aes(col = \"Type A\")\n  ) +\n  geom_function(\n    fun = curve_type_B,\n    aes(col = \"Type B\")\n  )\n\n\n\n\nCode\nggplotly(chd_plot_logit_2)\n\n\n\n\n\n\n\n\n\n3.4.9 reference-group and contrast parametrization\nWe can also use the corner-point parametrization (with reference groups and contrasts):\n\n\nCode\nchd_glm_contrasts = \n  wcgs |&gt; \n  glm(\n    \"data\" = _,\n    \"formula\" = chd69 == \"Yes\" ~ dibpat*age, \n    \"family\" = binomial(link = \"logit\")\n  )\n\nchd_glm_contrasts |&gt; \n  parameters() |&gt; \n  print_md()\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n-5.50\n0.67\n(-6.83, -4.19)\n-8.18\n&lt; .001\n\n\ndibpat (Type B)\n-0.30\n1.18\n(-2.63, 2.02)\n-0.26\n0.797\n\n\nage\n0.07\n0.01\n(0.05, 0.10)\n5.24\n&lt; .001\n\n\ndibpat (Type B) × age\n-0.01\n0.02\n(-0.06, 0.04)\n-0.42\n0.674\n\n\n\n\n\nCompare with what we had before:\n\n\nCode\nchd_glm_strat |&gt; \n  parameters() |&gt; \n  print_md()\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\ndibpat (Type A)\n-5.50\n0.67\n(-6.83, -4.19)\n-8.18\n&lt; .001\n\n\ndibpat (Type B)\n-5.80\n0.98\n(-7.73, -3.90)\n-5.95\n&lt; .001\n\n\ndibpat (Type A) × age\n0.07\n0.01\n(0.05, 0.10)\n5.24\n&lt; .001\n\n\ndibpat (Type B) × age\n0.06\n0.02\n(0.02, 0.10)\n3.01\n0.003\n\n\n\n\n\n\nExercise 3.11 If I give you model 1, how would you get the coefficients of model 2?"
  },
  {
    "objectID": "logistic-regression.html#fitting-logistic-regression-models",
    "href": "logistic-regression.html#fitting-logistic-regression-models",
    "title": "3  Logistic Regression",
    "section": "3.5 Fitting logistic regression models",
    "text": "3.5 Fitting logistic regression models\n\n3.5.1 \nIn general, the estimating equation \\(\\ell'(\\beta; \\mathbf x) = 0\\) cannot be solved analytically.\nInstead, we have to use a variant of the Newton-Raphson method, which was discussed briefly in Epi 203. We won’t go over it in this class; if you need to learn it, see Chapter 4 of Dobson and Barnett.\nFor now, all you need to know is that we make an iterative series of guesses, and each guess helps us make the next guess better (higher log-likelihood).\nYou can see some information about this process like so:\n\n\nCode\noptions(digits = 8)\ntemp = \n  wcgs |&gt; \n  glm(\n    control = glm.control(trace = TRUE),\n    \"data\" = _,\n    \"formula\" = chd69 == \"Yes\" ~ dibpat*age, \n    \"family\" = binomial(link = \"logit\")\n  )\n\n\nDeviance = 1775.7899 Iterations - 1\nDeviance = 1708.5396 Iterations - 2\nDeviance = 1704.0434 Iterations - 3\nDeviance = 1703.9833 Iterations - 4\nDeviance = 1703.9832 Iterations - 5\nDeviance = 1703.9832 Iterations - 6\n\n\nAfter each iteration of the fitting procedure, the deviance (\\(2(\\ell_{\\text{full}} - \\ell(\\hat\\beta))\\) ) is printed. You can see that the algorithm took six iterations to converge to a solution where the likelihood wasn’t changing much anymore."
  },
  {
    "objectID": "logistic-regression.html#sec-gof",
    "href": "logistic-regression.html#sec-gof",
    "title": "3  Logistic Regression",
    "section": "3.6 Model comparisons for logistic models",
    "text": "3.6 Model comparisons for logistic models\n\n3.6.1 Deviance test\nWe can compare the maximized log-likelihood of our model, \\(\\ell(\\hat\\beta; \\mathbf x)\\), versus the log-likelihood of the full model (aka saturated model aka maximal model), \\(\\ell_{\\text{full}}\\), which has one parameter per covariate pattern. With enough data, \\(2(\\ell_{\\text{full}} - \\ell(\\hat\\beta; \\mathbf x)) \\dot \\sim \\chi^2(N - p)\\), where \\(N\\) is the number of distinct covariate patterns and \\(p\\) is the number of \\(\\beta\\) parameters in our model. A significant p-value for this deviance statistic indicates that there’s some detectable pattern in the data that our model isn’t flexible enough to catch.\n\n\n\n\n\n\nCaution\n\n\n\nThe deviance statistic needs to have a large amount of data for each covariate pattern for the \\(\\chi^2\\) approximation to hold. A guideline from Dobson is that if there are \\(q\\) distinct covariate patterns \\(x_1...,x_q\\), with \\(n_1,...,n_q\\) observations per pattern, then the expected frequencies \\(n_k \\cdot \\pi(x_k)\\) should be at least 1 for every pattern \\(k\\in 1:q\\).\n\n\nIf you have covariates measured on a continuous scale, you may not be able to use the deviance tests to assess goodness of fit.\n\n\n3.6.2 Hosmer-Lemeshow test\nIf our covariate patterns produce groups that are too small, a reasonable solution is to make bigger groups by merging some of the covariate-pattern groups together.\nHosmer and Lemeshow (1980) proposed that we group the patterns by their predicted probabilities according to the model of interest. For example, you could group all of the observations with predicted probabilities of 10% or less together, then group the observations with 11%-20% probability together, and so on; \\(g=10\\) categories in all.\nThen we can construct a statistic \\[X^2 = \\sum_{c=1}^g \\frac{(o_c - e_c)^2}{e_c}\\] where \\(o_c\\) is the number of events observed in group \\(c\\), and \\(e_c\\) is the number of events expected in group \\(c\\) (based on the sum of the fitted values \\(\\hat\\pi_i\\) for observations in group \\(c\\)).\nIf each group has enough observations in it, you can compare \\(X^2\\) to a \\(\\chi^2\\) distribution; by simulation, the degrees of freedom has been found to be approximately \\(g-2\\).\nFor our CHD model, this procedure would be:\n\n\nCode\nwcgs = \n  wcgs |&gt; \n  mutate(\n    pred_probs_glm1 = chd_glm_strat |&gt; fitted(),\n    pred_prob_cats1 = \n      pred_probs_glm1 |&gt; \n      cut(breaks = seq(0, 1, by = .1), \n          include.lowest = TRUE))\n\nHL_table = \n  wcgs |&gt; \n  summarize(\n    .by = pred_prob_cats1,\n    n = n(),\n    o = sum(chd69 == \"Yes\"),\n    e = sum(pred_probs_glm1)\n  )\n\nHL_table |&gt; pander()\n\n\n\n\n\n\n\n\n\n\n\npred_prob_cats1\nn\no\ne\n\n\n\n\n(0.1,0.2]\n785\n116\n108\n\n\n(0.2,0.3]\n64\n12\n13.77\n\n\n[0,0.1]\n2,305\n129\n135.2\n\n\n\n\n\nCode\nX2 = HL_table |&gt; \n  summarize(\n    `X^2` = sum((o-e)^2/e)\n  ) |&gt; \n  pull(`X^2`)\nprint(X2)\n\n\n[1] 1.1102871\n\n\nCode\npval1 = pchisq(X2, lower = FALSE, df = nrow(HL_table) - 2)\n\n\nOur statistic is \\(X^2 = 1.11028711\\); \\(p(\\chi^2(1) &gt; 1.11028711) = 0.29201955\\), which is our p-value for detecting a lack of goodness of fit.\nUnfortunately that grouping plan left us with just three categories with any observations, so instead of grouping by 10% increments of predicted probability, typically analysts use deciles of the predicted probabilities:\n\n\nCode\nwcgs = \n  wcgs |&gt; \n  mutate(\n    pred_probs_glm1 = chd_glm_strat |&gt; fitted(),\n    pred_prob_cats1 = \n      pred_probs_glm1 |&gt; \n      cut(breaks = quantile(pred_probs_glm1, seq(0, 1, by = .1)), \n          include.lowest = TRUE))\n\nHL_table = \n  wcgs |&gt; \n  summarize(\n    .by = pred_prob_cats1,\n    n = n(),\n    o = sum(chd69 == \"Yes\"),\n    e = sum(pred_probs_glm1)\n  )\n\nHL_table |&gt; pander()\n\n\n\n\n\n\n\n\n\n\n\npred_prob_cats1\nn\no\ne\n\n\n\n\n(0.114,0.147]\n275\n48\n36.81\n\n\n(0.147,0.222]\n314\n51\n57.19\n\n\n(0.0774,0.0942]\n371\n27\n32.56\n\n\n(0.0942,0.114]\n282\n30\n29.89\n\n\n(0.0633,0.069]\n237\n17\n15.97\n\n\n(0.069,0.0774]\n306\n20\n22.95\n\n\n(0.0487,0.0633]\n413\n27\n24.1\n\n\n(0.0409,0.0487]\n310\n14\n14.15\n\n\n[0.0322,0.0363]\n407\n16\n13.91\n\n\n(0.0363,0.0409]\n239\n7\n9.48\n\n\n\n\n\nCode\nX2 = HL_table |&gt; \n  summarize(\n    `X^2` = sum((o-e)^2/e)\n  ) |&gt; \n  pull(`X^2`)\n\nprint(X2)\n\n\n[1] 6.7811383\n\n\nCode\npval1 = pchisq(X2, lower = FALSE, df = nrow(HL_table) - 2)\n\n\nNow we have more evenly split categories. The p-value is \\(0.56041994\\), still not significant.\nGraphically, we have compared:\n\n\nCode\nHL_plot = \n  HL_table |&gt; \n  ggplot(aes(x = pred_prob_cats1)) + \n  geom_line(aes(y = e, x = pred_prob_cats1, group = \"Expected\", col = \"Expected\")) +\n  geom_point(aes(y = e, size = n, col = \"Expected\")) +\n  geom_point(aes(y = o, size = n, col = \"Observed\")) +\n  geom_line(aes(y = o, col = \"Observed\", group = \"Observed\")) +\n  scale_size(range = c(1,4)) +\n  theme_bw() +\n  ylab(\"number of CHD events\") +\n  theme(axis.text.x = element_text(angle = 45))\n\n\n\n\nCode\nggplotly(HL_plot)\n\n\n\n\n\n\n\n\n3.6.3 Comparing models\n\nAIC = \\(-2 * \\ell(\\hat\\theta) + 2 * p\\) [lower is better]\nBIC = \\(-2 * \\ell(\\hat\\theta) + p * \\text{log}(n)\\) [lower is better]\nlikelihood ratio [higher is better]"
  },
  {
    "objectID": "logistic-regression.html#residual-based-diagnostics",
    "href": "logistic-regression.html#residual-based-diagnostics",
    "title": "3  Logistic Regression",
    "section": "3.7 Residual-based diagnostics",
    "text": "3.7 Residual-based diagnostics\n\n3.7.1 Logistic regression residuals only work for grouped data\nResiduals only work if there is more than one observation for most covariate patterns.\nHere we will create the grouped-data version of our CHD model from the WCGS study:\n\n\nCode\nwcgs_grouped = \n  wcgs |&gt; \n  summarize(\n    .by = c(dibpat, age),\n    n = n(),\n    chd = sum(chd69 == \"Yes\"),\n    `!chd` = sum(chd69 == \"No\")\n  )\n\nchd_glm_strat_grouped = glm(\n  \"formula\" = cbind(chd, `!chd`) ~ dibpat + dibpat:age - 1, \n  \"data\" = wcgs_grouped,\n  \"family\" = binomial(link = \"logit\")\n)\n\nchd_glm_strat_grouped |&gt; parameters() |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\ndibpat (Type A)\n-5.50\n0.67\n(-6.83, -4.19)\n-8.18\n&lt; .001\n\n\ndibpat (Type B)\n-5.80\n0.98\n(-7.73, -3.90)\n-5.95\n&lt; .001\n\n\ndibpat (Type A) × age\n0.07\n0.01\n(0.05, 0.10)\n5.24\n&lt; .001\n\n\ndibpat (Type B) × age\n0.06\n0.02\n(0.02, 0.10)\n3.01\n0.003\n\n\n\n\n\n\n\n3.7.2 (Response) residuals\n\\[e_k \\stackrel{\\text{def}}{=}\\bar y_k - \\hat{\\pi}(x_k)\\]\n(\\(k\\) indexes the covariate patterns)\nWe can graph these residuals \\(e_k\\) against the fitted values \\(\\hat\\pi(x_k)\\):\n\n\nCode\nwcgs_grouped = \n  wcgs_grouped |&gt; \n  mutate(\n    fitted = chd_glm_strat_grouped |&gt; fitted(),\n    fitted_logit = fitted |&gt; logit(),\n    response_resids = \n      chd_glm_strat_grouped |&gt; resid(type = \"response\")\n  )\n\nwcgs_response_resid_plot = \n  wcgs_grouped |&gt; \n  ggplot(\n    mapping = aes(\n      x = fitted,\n      y = response_resids\n    )\n  ) + \n  geom_point(\n    aes(col = dibpat)\n  ) +\n  geom_hline(yintercept = 0) + \n1  geom_smooth(\n    se = TRUE,\n    method.args = list(\n      span=2/3,\n      degree=1,\n      family=\"symmetric\",\n      iterations=3),\n    method = stats::loess)\n\n\n\n1\n\nDon’t worry about these options for now; I chose them to match autoplot() as closely as I can. plot.glm and autoplot use stats::lowess instead of stats::loess; stats::lowess is older, hard to use with geom_smooth, and hard to match exactly with stats::loess; see https://support.bioconductor.org/p/2323/.]\n\n\n\n\n\n\nCode\nwcgs_response_resid_plot |&gt; ggplotly()\n\n\n\n\n\n\nWe can see a slight fan-shape here: observations on the right have larger variance (as expected since \\(var(\\bar y) = \\pi(1-\\pi)/n\\) is maximized when \\(\\pi = 0.5\\)).\n\n\n3.7.3 Pearson residuals\nThe fan-shape in the response residuals plot isn’t necessarily a concern here, since we haven’t made an assumption of constant residual variance, as we did for linear regression.\nHowever, we might want to divide by the standard error in order to make the graph easier to interpret. Here’s one way to do that:\nThe Pearson (chi-squared) residual for covariate pattern \\(k\\) is: \\[\n\\begin{aligned}\nX_k &= \\frac{\\bar y_k - \\hat\\pi_k}{\\sqrt{\\hat \\pi_k (1-\\hat\\pi_k)/n_k}}\n\\end{aligned}\n\\]\nwhere \\[\n\\begin{aligned}\n\\hat\\pi_k\n&\\stackrel{\\text{def}}{=}\\hat\\pi(x_k)\\\\\n&\\stackrel{\\text{def}}{=}\\hat P(Y=1|X=x_k)\\\\\n&\\stackrel{\\text{def}}{=}\\text{expit}(x_i'\\hat \\beta)\\\\\n&\\stackrel{\\text{def}}{=}\\text{expit}(\\hat \\beta_0 + \\sum_{j=1}^p \\hat \\beta_j x_{ij})\n\\end{aligned}\n\\]\nLet’s take a look at the Pearson residuals for our CHD model from the WCGS data (graphed against the fitted values on the logit scale):\n\n\nCode\nlibrary(ggfortify)\n\n\n\n\nCode\nautoplot(chd_glm_strat_grouped, which = 1, ncol = 1) |&gt; print()\n\n\n\n\n\nThe fan-shape is gone, and these residuals don’t show any obvious signs of model fit issues.\n\nPearson residuals plot for beetles data\nIf we create the same plot for the beetles model, we see some strong evidence of a lack of fit:\n\n\nCode\nautoplot(beetles_glm_grouped, which = 1, ncol = 1) |&gt; print()\n\n\n\n\n\n\n\nPearson residuals with individual (ungrouped) data\nWhat happens if we try to compute residuals without grouping the data by covariate pattern?\n\n\nCode\nlibrary(ggfortify)\n\n\n\n\nCode\nautoplot(chd_glm_strat, which = 1, ncol = 1) |&gt; print()\n\n\n\n\n\nMeaningless.\n\n\nResiduals plot by hand (optional section)\nIf you want to check your understanding of what these residual plots are, try building them yourself:\n\n\nCode\nwcgs_grouped = \n  wcgs_grouped |&gt; \n  mutate(\n    fitted = chd_glm_strat_grouped |&gt; fitted(),\n    fitted_logit = fitted |&gt; logit(),\n    resids = chd_glm_strat_grouped |&gt; resid(type = \"pearson\")\n  )\n\nwcgs_resid_plot1 = \n  wcgs_grouped |&gt; \n  ggplot(\n    mapping = aes(\n      x = fitted_logit,\n      y = resids\n      \n    ) \n    \n  ) + \n  geom_point(\n    aes(col = dibpat)\n  ) +\n  geom_hline(yintercept = 0) + \n  geom_smooth(se = FALSE, \n              method.args = list(\n                span=2/3,\n                degree=1,\n                family=\"symmetric\",\n                iterations=3,\n                surface=\"direct\"\n                # span = 2/3, \n                # iterations = 3\n              ),\n              method = stats::loess)\n# plot.glm and autoplot use stats::lowess, which is hard to use with \n# geom_smooth and hard to match exactly; \n# see https://support.bioconductor.org/p/2323/\n\n\n\n\nCode\nwcgs_resid_plot1 |&gt; ggplotly()\n\n\n\n\n\n\n\n\n\n3.7.4 Pearson chi-squared goodness of fit test\nThe Pearson chi-squared goodness of fit statistic is: \\[\nX^2 = \\sum_{k=1}^m X_k^2\n\\] Under the null hypothesis that the model in question is correct (i.e., sufficiently complex), \\(X^2\\ \\dot \\sim\\ \\chi^2(N-p)\\).\n\n\nCode\nX = chd_glm_strat_grouped |&gt; \n  resid(type = \"pearson\")\n\nchisq_stat = sum(X^2)\n\npval = pchisq(\n  chisq_stat, \n  lower = FALSE, \n  df = length(X) - length(coef(chd_glm_strat_grouped)))\n\n\nFor our CHD model, the p-value for this test is 0.26523556; no significant evidence of a lack of fit at the 0.05 level.\n\nStandardized Pearson residuals\nEspecially for small data sets, we might want to adjust our residuals for leverage (since outliers in \\(X\\) add extra variance to the residuals):\n\\[r_{P_k} = \\frac{X_k}{\\sqrt{1-h_k}}\\]\nwhere \\(h_k\\) is the leverage of \\(X_k\\). The functions autoplot() and plot.lm() use these for some of their graphs.\n\n\n\n3.7.5 Deviance residuals\nFor large sample sizes, the Pearson and deviance residuals will be approximately the same. For small sample sizes, the deviance residuals from covariate patterns with small sample sizes can be unreliable (high variance).\n\\[d_k = \\text{sign}(y_k - n_k \\hat \\pi_k)\\left\\{\\sqrt{2[\\ell_{\\text{full}}(x_k) - \\ell(\\hat\\beta; x_k)]}\\right\\}\\]\n\nStandardized deviance residuals\n\\[r_{D_k} = \\frac{d_k}{\\sqrt{1-h_k}}\\]\n\n\n\n3.7.6 Diagnostic plots\nLet’s take a look at the full set of autoplot() diagnostics now for our CHD model:\n\n\nCode\nchd_glm_strat_grouped |&gt; autoplot(which = 1:6) |&gt; print()\n\n\n\n\n\nThings look pretty good here. The QQ plot is still usable; with large samples; the residuals should be approximately Gaussian.\n\nBeetles\nLet’s look at the beetles model diagnostic plots for comparison:\n\n\nCode\nbeetles_glm_grouped |&gt; autoplot(which = 1:6) |&gt; print()\n\n\n\n\n\nHard to tell much from so little data, but there might be some issues here."
  },
  {
    "objectID": "logistic-regression.html#sec-OR-RR",
    "href": "logistic-regression.html#sec-OR-RR",
    "title": "3  Logistic Regression",
    "section": "3.8 Odds Ratios vs Probability (Risk) Ratios",
    "text": "3.8 Odds Ratios vs Probability (Risk) Ratios\n\nDefinition 3.10 (Relative risk) The relative risk comparing two probabilities \\(\\pi_1\\) and \\(\\pi_2\\), also known as the risk ratio, relative risk ratio, probability ratio, and rate ratio, is:\n\\[\\rho(1,2) \\stackrel{\\text{def}}{=}\\frac{\\pi_1}{\\pi_2}\\]\n\n\nCase 1: rare events\nFor rare events, odds ratios and probability (a.k.a. risk, a.k.a. prevalence) ratios will be close:\n\\(\\pi_1 = .01\\) \\(\\pi_2 = .02\\)\n\n\nCode\npi1 = .01\npi2 = .02\npi2/pi1\n\n\n[1] 2\n\n\nCode\nodds(pi2)/odds(pi1)\n\n\n[1] 2.0204082\n\n\n\n\nCase 2: frequent events\n\\(\\pi_1 = .4\\) \\(\\pi_2 = .5\\)\nFor more frequently-occurring outcomes, this won’t be the case:\n\n\nCode\npi1 = .4\npi2 = .5\npi2/pi1\n\n\n[1] 1.25\n\n\nCode\nodds(pi2)/odds(pi1)\n\n\n[1] 1.5\n\n\nIf you want risk ratios, you can sometimes get them by changing the link function:\n\n\nCode\ndata(anthers, package = \"dobson\")\nanthers.sum&lt;-aggregate(\n  anthers[c(\"n\",\"y\")], \n  by=anthers[c(\"storage\")],FUN=sum) \n\nanthers_glm_log = glm(\n  formula = cbind(y,n-y)~storage,\n  data=anthers.sum, \n  family=binomial(link=\"log\"))\n\nanthers_glm_log |&gt; parameters() |&gt; print_md()\n\n\n\n\n\nParameter\nLog-Risk\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n-0.80\n0.12\n(-1.04, -0.58)\n-6.81\n&lt; .001\n\n\nstorage\n0.17\n0.07\n(0.02, 0.31)\n2.31\n0.021\n\n\n\n\n\nNow \\(\\text{exp}\\left\\{\\beta\\right\\}\\) gives us risk ratios instead of odds ratios:\n\n\nCode\nanthers_glm_log |&gt; parameters(exponentiate = TRUE) |&gt; print_md()\n\n\n\n\n\nParameter\nRisk Ratio\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n0.45\n0.05\n(0.35, 0.56)\n-6.81\n&lt; .001\n\n\nstorage\n1.18\n0.09\n(1.03, 1.36)\n2.31\n0.021\n\n\n\n\n\nLet’s compare this model with a logistic model:\n\n\nCode\nanthers_glm_logit = glm(\n  cbind(y,n-y)~storage,\n  data=anthers.sum, \n  family=binomial(link=\"logit\"))\n\nanthers_glm_logit |&gt; parameters(exponentiate = TRUE) |&gt; print_md()\n\n\n\n\n\nParameter\nOdds Ratio\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n0.76\n0.20\n(0.45, 1.27)\n-1.05\n0.296\n\n\nstorage\n1.49\n0.26\n(1.06, 2.10)\n2.29\n0.022\n\n\n\n\n\n[to add: fitted plots on each outcome scale]\nWhen I try to use link =\"log\" in practice, I often get errors about not finding good starting values for the estimation procedure. This is likely because the model is producing fitted probabilities greater than 1.\nWhen this happens, you can try to fit Poisson regression models instead (we will see those soon!). But then the outcome distribution isn’t quite right, and you won’t get warnings about fitted probabilities greater than 1. In my opinion, the Poisson model for binary outcomes is confusing and not very appealing.\n\n\n\n\n\n\nDobson, Annette J, and Adrian G Barnett. 2018. An Introduction to Generalized Linear Models. 4th ed. CRC press. https://doi.org/10.1201/9781315182780.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. 2nd ed. Springer. https://doi.org/10.1007/978-1-4614-1353-0."
  },
  {
    "objectID": "count-regression.html#introduction",
    "href": "count-regression.html#introduction",
    "title": "\n4  Models for Count Outcomes\n",
    "section": "\n4.1 Introduction",
    "text": "4.1 Introduction\n\n4.1.1 Examples of count outcomes\n\nCyclones per season\nSeconds of tooth-brushing per session (if rounded)\nInfections per person-year\nVisits to ER per person-month\nCar accidents per 1000 miles driven\n\n\n\n\n\n\n\nNote\n\n\n\nIn many count outcomes, there is some sense of “exposure magnitude” or “duration of observation”: person-year, time at risk, session, miles driven, etc.\n\n\n\n4.1.2 Poisson distribution\n\\[P(Y=y) = \\frac{\\mu^y e^{-\\mu}}{y!}\\]\nProperties\n\n\\(\\mathbb{E}[Y] = \\mu\\)\n\\(\\text{Var}[Y] = \\mu\\)\n\n4.1.3 Accounting for exposure\nIf the exposures/observation durations, denoted \\(T=t\\), are not all equal, we model \\[\\mu = \\lambda t\\]\n\\(\\lambda\\) is interpreted as the “expected event rate per unit of exposure”; that is,\n\\[\\lambda = \\frac{\\mathbb E[Y|T=t]}{t}\\]\n\n\n\n\n\n\nImportant\n\n\n\nThe exposure magnitude, \\(T\\), is similar to a covariate in linear or logistic regression. However, there is an important difference: in count regression, there is no intercept corresponding to \\(\\mathbb E[Y|T=0]\\). In other words, this model assumes that if there is no exposure, there can’t be any events.\n\n\n\n4.1.4 Adding covariates\nWith covariates, \\(\\lambda\\) becomes a function of the covariates \\(\\tilde X = (X_1, \\dots,X_n)\\), with a \\(\\text{log}\\left\\{\\right\\}\\) link function (and thus an \\(\\text{exp}\\left\\{\\right\\}\\) inverse-link). That is:\n\\[\n\\begin{aligned}\n\\mathbb E[Y | \\tilde X = \\tilde x,T=t]\n&= \\mu(\\tilde x,t)\\\\\n\\mu(\\tilde x,t)\n&= \\lambda(\\tilde x)\\cdot t\\\\\n\\lambda(\\tilde x)\n&= \\text{exp}\\left\\{\\eta(\\tilde x)\\right\\}\\\\\n\\eta(\\tilde x)\n&= \\tilde x'\\tilde \\beta = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\end{aligned}\n\\]\nTherefore, \\[\n\\begin{aligned}\n\\text{log}\\left\\{\\{\\mathbb E[Y | \\tilde X = \\tilde x,T=t] \\}\\right\\}\n&= \\text{log}\\left\\{\\{\\mu(\\tilde x)\\}\\right\\}\\\\\n&=\\text{log}\\left\\{\\{\\lambda(\\tilde x) \\cdot t \\}\\right\\}\\\\\n&=\\text{log}\\left\\{\\lambda(\\tilde x)\\right\\} + \\text{log}\\left\\{t\\right\\}\\\\\n&=\\text{log}\\left\\{\\text{exp}\\left\\{\\eta(\\tilde x)\\right\\}\\right\\} + \\text{log}\\left\\{t\\right\\}\\\\\n&=\\eta(\\tilde x) + \\text{log}\\left\\{t\\right\\}\\\\\n&=\\tilde x'\\tilde\\beta + \\text{log}\\left\\{t\\right\\}\\\\\n&=(\\beta_0 +\\beta_1 x_1+\\dots + \\beta_p x_p) + \\text{log}\\left\\{t\\right\\}\\\\\n\\end{aligned}\n\\]\nIn contrast with the \\(X\\)s, \\(T\\) enters this expression with a \\(\\text{log}\\left\\{\\right\\}\\) transformation and without a corresponding \\(\\beta\\) coefficient.\n\n\n\n\n\n\nNote\n\n\n\nTerms that enter the linear component of a model without a coefficient, such as \\(\\text{log}\\left\\{t\\right\\}\\) here, are called offsets.\n\n\n\n4.1.5 Rate ratios\nDifferences on the log-rate scale become ratios on the rate scale.\n\n\n\n\n\n\nTip\n\n\n\n\\[\\text{exp}\\left\\{a-b\\right\\} = \\frac{\\text{exp}\\left\\{a\\right\\}}{\\text{exp}\\left\\{b\\right\\}}\\]\n\n\nTherefore, according to this model, differences of \\(\\delta\\) in covariate \\(x_j\\) correspond to rate ratios of \\(\\text{exp}\\left\\{\\beta_j \\cdot\\delta\\right\\}\\).\nThat is, letting \\(\\tilde X_{-j}\\) denote vector \\(\\tilde X\\) with element \\(j\\) removed:\n\\[\n\\begin{aligned}\n&{\n\\left\\{\n    \\text{log}\\left\\{\\mathbb E[Y |{\\color{red}{X_j = a}}, \\tilde X_{-j}=\\tilde x_{-j},T=t]\\right\\}\n    \\atop\n    {-\\text{log}\\left\\{\\mathbb E[Y |{\\color{red}{X_j = b}}, \\tilde X_{-j}=\\tilde x_{-j},T=t]\\right\\}}\n    \\right\\}\n}\\\\\n&=\n{\\left\\{\n\\text{log}\\left\\{t\\right\\} + \\beta_0 + \\beta_1 x_1 + ... + {\\color{red}{\\beta_j (a)}} + ...+\\beta_p x_p\n\\atop\n{-\\text{log}\\left\\{t\\right\\} + \\beta_0 + \\beta_1 x_1 + ... + {\\color{red}{\\beta_j (b)}} + ...+\\beta_p x_p}\n\\right\\}}\\\\\n&= \\color{red}{\\beta_j(a-b)}\n\\end{aligned}\n\\]\nAnd accordingly,\n\\[\n\\begin{aligned}\n\\frac\n{\\mathbb{E}[Y |{\\color{red}{X_j = a}}, \\tilde X_{-j} = \\tilde x_{-j}, T = t]\n}\n{\n\\mathbb E[Y |{\\color{red}{X_j = b}}, \\tilde X_{-j}=\\tilde x_{-j},T=t]\n}\n=\n\\text{exp}\\left\\{{\\color{red}{\\beta_j(a-b)}}\\right\\}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "count-regression.html#inference-for-count-regression-models",
    "href": "count-regression.html#inference-for-count-regression-models",
    "title": "\n4  Models for Count Outcomes\n",
    "section": "\n4.2 Inference for count regression models",
    "text": "4.2 Inference for count regression models\n\n4.2.1 Confidence intervals for regression coefficients and rate ratios\nAs usual:\n\\[\n\\beta \\in \\left[\\hat\\beta{\\color{red}\\pm} z_{1 - \\frac{\\alpha}{2}}\\cdot\\hat{\\text{se}}\\left(\\hat\\beta\\right)\\right]\n\\]\nRate ratios: exponentiate CI endpoints\n\\[\n\\text{exp}\\left\\{\\beta\\right\\} \\in \\left[\\text{exp}\\left\\{\\hat\\beta{\\color{red}\\pm} z_{1 - \\frac{\\alpha}{2}}\\cdot\\hat{\\text{se}}\\left(\\hat\\beta\\right)\\right\\} \\right]\n\\]\n\n4.2.2 Hypothesis tests for regression coefficients\n\\[\nt = \\frac{\\hat \\beta - \\beta_0}{\\hat{\\text{se}}\\left(\\hat\\beta\\right)}\n\\]\nCompare \\(t\\) or \\(|t|\\) to the tails of the standard Gaussian distribution, according to the null hypothesis.\n\n4.2.3 Comparing nested models\nlog(likelihood ratio) tests, as usual."
  },
  {
    "objectID": "count-regression.html#prediction",
    "href": "count-regression.html#prediction",
    "title": "\n4  Models for Count Outcomes\n",
    "section": "\n4.3 Prediction",
    "text": "4.3 Prediction\n\\[\n\\begin{aligned}\n\\hat y\n&\\stackrel{\\text{def}}{=}\\hat{\\mathbb E}[Y|\\tilde X= \\tilde x,T=t]\\\\\n&=\\hat\\mu(\\tilde x, t)\\\\\n&=\\hat\\lambda(\\tilde x) \\cdot t\\\\\n&=\\text{exp}\\left\\{\\hat\\eta(\\tilde x)\\right\\} \\cdot t\\\\\n&=\\text{exp}\\left\\{\\tilde x'\\hat{\\boldsymbol{\\beta}}\\right\\} \\cdot t\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "count-regression.html#diagnostics",
    "href": "count-regression.html#diagnostics",
    "title": "\n4  Models for Count Outcomes\n",
    "section": "\n4.4 Diagnostics",
    "text": "4.4 Diagnostics\n\n4.4.1 Residuals\nObservation residuals\n\\[e \\stackrel{\\text{def}}{=}y - \\hat y\\]\nPearson residuals\n\\[r = \\frac{e}{\\hat{\\text{se}}\\left(e\\right)} \\approx \\frac{e}{\\sqrt{\\hat y}}\\]\nStandardized Pearson residuals\n\\[r_p = \\frac{r}{\\sqrt{1-h}}\\] where \\(h\\) is the “leverage” (which we will continue to leave undefined).\nDeviance residuals\n\\[\nd_k = \\text{sign}(y - \\hat y)\\left\\{\\sqrt{2[\\ell_{\\text{full}}(y) - \\ell(\\hat\\beta; y)]}\\right\\}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\\[\\text{sign}(x) \\stackrel{\\text{def}}{=}\\frac{x}{|x|}\\] In other words:\n\n\n\\(\\text{sign}(x) = -1\\) if \\(x &lt; 0\\)\n\n\n\\(\\text{sign}(x) = 0\\) if \\(x = 0\\)\n\n\n\\(\\text{sign}(x) = 1\\) if \\(x &gt; 0\\)"
  },
  {
    "objectID": "count-regression.html#zero-inflation",
    "href": "count-regression.html#zero-inflation",
    "title": "\n4  Models for Count Outcomes\n",
    "section": "\n4.5 Zero-inflation",
    "text": "4.5 Zero-inflation\n\n4.5.1 Models for zero-inflated counts\nWe assume a latent (unobserved) binary variable, \\(Z\\), which we model using logistic regression:\n\\[P(Z=1|X=x) = \\pi(x) = \\text{expit}(\\gamma_0 + \\gamma_1x_1 +...)\\]\nAccording to this model, if \\(Z=1\\), then \\(Y\\) will always be zero, regardless of \\(X\\) and \\(T\\):\n\\[P(Y=0|Z=1,X=x,T=t) = 1\\]\nOtherwise (if \\(Z=0\\)), \\(Y\\) will have a Poisson distribution, conditional on \\(X\\) and \\(T\\), as above.\nEven though we never observe \\(Z\\), we can estimate the parameters \\(\\gamma_0\\)-\\(\\gamma_p\\), via maximum likelihood:\n\\[\n\\begin{aligned}\nP(Y=y|X=x,T=t) &= P(Y=y,Z=1|...) + P(Y=y,Z=0|...)\n\\end{aligned}\n\\] (by the Law of Total Probability)\nwhere \\[\n\\begin{aligned}\nP(Y=y,Z=z|...)\n&= P(Y=y|Z=z,...)P(Z=z|...)\n\\end{aligned}\n\\]\nExercise\nExpand \\(P(Y=0|X=x,T=t)\\), \\(P(Y=1|X=x,T=t)\\) and \\(P(Y=y|X=x,T=t)\\) into expressions involving \\(P(Z=1|X=x,T=t)\\) and \\(P(Y=y|Z=0,X=x,T=t)\\).\nExercise\nDerive the expected value and variance of \\(Y\\), conditional on \\(X\\) and \\(T\\), as functions of \\(P(Z=1|X=x,T=t)\\) and \\(\\mathbb E[Y|Z=0,X=x,T=t]\\)."
  },
  {
    "objectID": "count-regression.html#over-dispersion",
    "href": "count-regression.html#over-dispersion",
    "title": "\n4  Models for Count Outcomes\n",
    "section": "\n4.6 Over-dispersion",
    "text": "4.6 Over-dispersion\n\n4.6.1 Negative binomial models\nThe Poisson distribution model forces the variance to equal the mean. In practice, many count distributions will have a variance substantially larger than the mean (or occasionally smaller).\nWhen we encounter this, we can try to reduce the residual variance by adding more covariates. However, there are also alternatives to the Poisson model.\nMost notably, the negative binomial model:\n\\[P(Y=y) = \\frac{\\mu^y}{y!} \\cdot \\frac{\\Gamma(\\rho + y)}{\\Gamma(\\rho) \\cdot (\\rho + \\mu)^y} \\cdot \\left(1+\\frac{\\mu}{\\rho}\\right)^{-\\rho}\\]\nwhere \\(\\rho\\) is an overdispersion parameter and \\(\\Gamma(x) = (x-1)!\\) for integers \\(x\\).\nYou don’t need to memorize or understand this expression, but as \\(\\rho \\rightarrow \\infty\\), the second term converges to 1 and the third term converges to \\(\\text{exp}\\left\\{-\\mu\\right\\}\\), which brings us back to the Poisson distribution.\nFor this distribution, \\(\\mathbb E[Y] = \\mu\\) and \\(\\text{Var}(Y) = \\mu + \\frac{\\mu^2}{\\rho} &gt; \\mu\\).\nWe can still model \\(\\mu\\) as a function of \\(X\\) and \\(T\\) as before, and we can combine this model with zero-inflation by using it in place of the Poisson distribution for \\(P(Y=y|Z=0,X=x,T=t)\\).\n\n4.6.2 Quasipoisson\nAn alternative to Negative binomial is the “quasipoisson” distribution. I’ve never used it, but it seems to be a method-of-moments type approach rather than maximum likelihood. It models the variance as \\(\\text{Var}(Y) = \\mu\\theta\\), and estimates \\(\\theta\\) accordingly.\nSee ?quasipoisson in R for more.\n\n\n\n\n\n\nDobson, Annette J, and Adrian G Barnett. 2018. An Introduction to Generalized Linear Models. 4th ed. CRC press. https://doi.org/10.1201/9781315182780.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. 2nd ed. Springer. https://doi.org/10.1007/978-1-4614-1353-0."
  },
  {
    "objectID": "time-to-event-models.html#footnotes",
    "href": "time-to-event-models.html#footnotes",
    "title": "Time to Event Models",
    "section": "",
    "text": "Binary outcomes are typically defined for a specific time-point. It is important to clearly define whether we are interested in outcome status at end of study, at end of life, or at some other time.↩︎"
  },
  {
    "objectID": "intro-to-survival-analysis.html#overview",
    "href": "intro-to-survival-analysis.html#overview",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.1 Overview",
    "text": "5.1 Overview\n\n5.1.1 Time-to-event outcomes\nSurvival analysis is a framework for modeling time-to-event outcomes. It is used in:\n\nclinical trials, where the event is often death or recurrence of disease.\nengineering reliability analysis, where the event is failure of a device or system.\ninsurance, particularly life insurance, where the event is death."
  },
  {
    "objectID": "intro-to-survival-analysis.html#time-to-event-outcome-distributions",
    "href": "intro-to-survival-analysis.html#time-to-event-outcome-distributions",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.2 Time-to-event outcome distributions",
    "text": "5.2 Time-to-event outcome distributions\n\n5.2.1 Distributions of Time-to-Event Data\n\nThe distribution of event times is asymmetric and can be long-tailed, and starts at 0 (that is, \\(P(T&lt;0) = 0\\)).\nThe base distribution is not normal, but exponential.\nThere are usually censored observations, which are ones in which the failure time is not observed.\nOften, these are right-censored, meaning that we know that the event occurred after some known time \\(t\\), but we don’t know the actual event time, as when a patient is still alive at the end of the study.\nObservations can also be left-censored, meaning we know the event has already happened at time \\(t\\), or interval-censored, meaning that we only know that the event happened between times \\(t_1\\) and \\(t_2\\).\nAnalysis is difficult if censoring is associated with treatment.\n\n5.2.2 Right Censoring\n\nPatients are in a clinical trial for cancer, some on a new treatment and some on standard of care.\nSome patients in each group have died by the end of the study. We know the survival time (measured for example from time of diagnosis—each person on their own clock).\nPatients still alive at the end of the study are right censored.\nPatients who are lost to follow-up or withdraw from the study may be right-censored.\n\n5.2.3 Left and Interval Censoring\n\nAn individual tests positive for HIV.\nIf the event is infection with HIV, then we only know that it has occurred before the testing time \\(t\\), so this is left censored.\nIf an individual has a negative HIV test at time \\(t_1\\) and a positive HIV test at time \\(t_2\\), then the infection event is interval censored."
  },
  {
    "objectID": "intro-to-survival-analysis.html#distribution-functions-for-time-to-event-variables",
    "href": "intro-to-survival-analysis.html#distribution-functions-for-time-to-event-variables",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.3 Distribution functions for time-to-event variables",
    "text": "5.3 Distribution functions for time-to-event variables\n\n5.3.1 The Probability Density Function (PDF)\nFor a time-to-event variable \\(T\\) with a continuous distribution, the probability density function is defined as usual:\n\\[f(t)\\stackrel{\\text{def}}{=}p(t) \\stackrel{\\text{def}}{=}p(T=t)\\]\nTypically, this density is assumed to be 0 for all \\(t&lt;0\\); that is, \\(f(t) = 0, \\forall t&lt;0\\). In other words, the range of \\(T\\) is typically \\([0,\\infty)\\).\n\nExample 5.1 (exponential distribution) Recall from Epi 202: the pdf of the exponential distribution family of models is:\n\\[p(T=t) = \\mathbb{1}_{t \\ge 0}\\cdot \\lambda  \\text{e}^{-\\lambda t}\\]\nwhere \\(\\lambda &gt; 0\\).\nHere are some examples of exponential pdfs:\n\n\n\n\n\n\n\n5.3.2 The Cumulative Distribution Function (CDF)\nThe cumulative distribution function is defined as:\n\\[\n\\begin{aligned}\nF(t) &\\stackrel{\\text{def}}{=}\\Pr(T \\le t)\\\\\n&=\\int_{u=0}^t f(u) du\n\\end{aligned}\n\\]\n\nExample 5.2 (exponential distribution) Recall from Epi 202: the cdf of the exponential distribution family of models is:\n\\[\nP(T\\le t) = \\mathbb{1}_{t \\ge 0}\\cdot(1- \\text{e}^{-\\lambda t})\n\\] where \\(\\lambda &gt; 0\\).\nHere are some examples of exponential cdfs:\n\n\n\n\n\n\n\n5.3.3 The Survival Function\nFor survival data, a more important quantity is the survival function:\n\\[\n\\begin{aligned}\nS(t) &\\stackrel{\\text{def}}{=}\\Pr(T &gt; t)\\\\\n&=\\int_{u=t}^\\infty p(u) du\\\\\n&=1-F(t)\\\\\n\\end{aligned}\n\\]\nThe survival function \\(S(t)\\) is the probability that the event time is later than \\(t\\). If the event in a clinical trial is death, then \\(S(t)\\) is the expected fraction of the original population at time 0 who have survived up to time \\(t\\) and are still alive at time \\(t\\); that is, if \\(X_t\\) represents survival status at time \\(t\\), with \\(X_t = 1\\) denoting alive at time \\(t\\) and \\(X_t = 0\\) denoting deceased at time \\(t\\), then:\n\\[S(t) = \\mathbb{E}\\left[X_t\\right]\\]\n\nExample 5.3 (exponential distribution) Since \\(S(t) = 1 - F(t)\\), the survival function of the exponential distribution family of models is:\n\\[\nP(T&gt; t) = \\left\\{ {{\\text{e}^{-\\lambda t}, t\\ge0} \\atop {1, t \\le 0}}\\right.\n\\] where \\(\\lambda &gt; 0\\).\nHere are some examples of exponential pdfs:\n\n\n\n\n\n\n\n5.3.4 The Hazard Function\nAnother important quantity is the hazard function:\n\nDefinition 5.1 (Hazard function) The hazard function for a random variable \\(T\\) at value \\(t\\) is the conditional density of \\(T\\) at \\(t\\), given \\(T\\ge t\\); that is:\n\\[h(t) \\stackrel{\\text{def}}{=}p(T=t|T\\ge t)\\]\nIf \\(T\\) represents the time at which an event occurs, then \\(h(t)\\) is the probability that the event occurs at time \\(t\\), given that it has not occurred prior to time \\(t\\).\n\nThe hazard function has an important relationship to the density and survival functions, which we can use to derive the hazard function for a given probability distribution.\n\nTheorem 5.1 \\[h(t)=\\frac{f(t)}{S(t)}\\]\n\nProof. Recall from Epi 202: if \\(A\\) and \\(B\\) are statistical events and \\(A\\subseteq B\\), then \\(p(A, B) = p(A)\\). In particular, \\(\\{T=t\\} \\subseteq \\{T\\geq t\\}\\), so:\n\\[p(T=t, T\\ge t) = p(T=t)\\]\nHence:\n\\[\n\\begin{aligned}\nh(t) &=p(T=t|T\\ge t)\\\\\n&=\\frac{p(T=t, T\\ge t)}{p(T \\ge t)}\\\\\n&=\\frac{p(T=t)}{p(T \\ge t)}\\\\\n&=\\frac{f(t)}{S(t)}\n\\end{aligned}\n\\]\n\n\n\nExample 5.4 (exponential distribution) The hazard function of the exponential distribution family of models is:\n\\[\n\\begin{aligned}\nP(T=t|T \\ge t)\n&= \\frac{f(t)}{S(t)}\\\\\n&= \\frac{\\mathbb{1}_{t \\ge 0}\\cdot \\lambda  \\text{e}^{-\\lambda t}}{\\text{e}^{-\\lambda t}}\\\\\n&=\\mathbb{1}_{t \\ge 0}\\cdot \\lambda\n\\end{aligned}\n\\]\nFigure 5.1 shows some examples of exponential hazard functions:\n\n\n\nFigure 5.1: Examples of hazard functions for exponential distributions\n\n\n\n\n\nWe can also view the hazard function as the derivative of the negative of the logarithm of the survival function:\n\nTheorem 5.2 \\[h(t) = \\frac{d}{dt}\\left\\{-\\text{log}\\left\\{S(t)\\right\\}\\right\\}\\]\n\nProof. \\[\n\\begin{aligned}\nh(t)\n&= \\frac{f(t)}{S(t)}\\\\\n&= \\frac{-S'(t)}{S(t)}\\\\\n&= -\\frac{S'(t)}{S(t)}\\\\\n&=-\\frac{d}{dt}\\text{log}\\left\\{S(t)\\right\\}\\\\\n&=\\frac{d}{dt}\\left\\{-\\text{log}\\left\\{S(t)\\right\\}\\right\\}\n\\end{aligned}\n\\]\n\n\n\n5.3.5 The Cumulative Hazard Function\nSince \\(h(t) = \\frac{d}{dt}\\left\\{-\\text{log}\\left\\{S(t)\\right\\}\\right\\}\\) (see Theorem 5.2), we also have:\n\nCorollary 5.1 \\[S(t) = \\text{exp}\\left\\{-\\int_{u=0}^t h(u)du\\right\\} \\tag{5.1}\\]\n\nThe integral in Equation 5.1 is important enough to have its own name: cumulative hazard.\n\nDefinition 5.2 (cumulative hazard) The cumulative hazard function \\(H(t)\\) is defined as:\n\\[H(t) \\stackrel{\\text{def}}{=}\\int_{u=0}^t h(u) du\\]\n\nAs we will see below, \\(H(t)\\) is tractable to estimate, and we can then derive an estimate of the hazard function using an approximate derivative of the estimated cumulative hazard.\n\nExample 5.5 The cumulative hazard function of the exponential distribution family of models is:\n\\[\nH(t) = \\mathbb{1}_{t \\ge 0}\\cdot \\lambda t\n\\]\nHere are some examples of exponential cumulative hazard functions:\n\n\n\n\n\n\n\n5.3.6 Some Key Mathematical Relationships among Survival Concepts\nDiagram:\n\\[\nh(t) \\xrightarrow[]{\\int_{u=0}^t h(u)du} H(t)\n\\xrightarrow[]{\\text{exp}\\left\\{-H(t)\\right\\}} S(t)\n\\xrightarrow[]{1-S(t)} F(t)\n\\]\n\\[\nh(t) \\xleftarrow[\\frac{d}{dt}H(t)]{} H(t)\n\\xleftarrow[-\\text{log}\\left\\{S(t)\\right\\}]{} S(t)\n\\xleftarrow[1-F(t)]{} F(t)\n\\]\nIdentities:\n\\[\n\\begin{aligned}\nS(t) &= 1 - F(t)\\\\\n&= \\text{exp}\\left\\{-H(t)\\right\\}\\\\\nS'(t) &= -f(t)\\\\\nH(t) &= -\\text{log}\\left\\{S(t)\\right\\}\\\\\nH'(t) &= h(t)\\\\\nh(t) &= \\frac{f(t)}{S(t)}\\\\\n&= -\\frac{d}{dt}\\text{log}\\left\\{S(t)\\right\\} \\\\\nf(t) &= h(t)\\cdot S(t)\\\\\n\\end{aligned}\n\\]\nSome proofs (others left as exercises):\n\\[\n\\begin{aligned}\nS'(t) &= \\frac{d}{dt}(1-F(t))\\\\\n&= -F'(t)\\\\\n&= -f(t)\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{d}{dt}\\text{log}\\left\\{S(t)\\right\\}\n&= \\frac{S'(t)}{S(t)}\\\\\n&= -\\frac{f(t)}{S(t)}\\\\\n&= -h(t)\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nH(t)\n&\\stackrel{\\text{def}}{=}\\int_{u=0}^t h(u) du\\\\\n&= \\int_0^t -\\frac{d}{du}\\text{log}\\left\\{S(u)\\right\\} du\\\\\n&= \\left[-\\text{log}\\left\\{S(u)\\right\\}\\right]_{u=0}^{u=t}\\\\\n&= \\left[\\text{log}\\left\\{S(u)\\right\\}\\right]_{u=t}^{u=0}\\\\\n&= \\text{log}\\left\\{S(0)\\right\\} - \\text{log}\\left\\{S(t)\\right\\}\\\\\n&= \\text{log}\\left\\{1\\right\\} - \\text{log}\\left\\{S(t)\\right\\}\\\\\n&= 0 - \\text{log}\\left\\{S(t)\\right\\}\\\\\n&=-\\text{log}\\left\\{S(t)\\right\\}\n\\end{aligned}\n\\]\nEquivalently:\n\\[S(t) = \\text{exp}\\left\\{-H(t)\\right\\}\\]\nExample: Time to death the US in 2004\nDaily hazard rates for US Females in 2004\nThe first day is the most dangerous:\n\n\n\nDaily Hazard Rates in 2004 for US Females\n\n\n\n\nDaily hazard rates for US Males and Females in 2004\nExercise: hypothesize why these curves differ where they do?\n\n\n\nDaily Hazard Rates in 2004 for US Males and Females 1-40\n\n\n\n\nSurvival curve for US females\nExercise: compare and contrast this curve with the corresponding hazard curve.\n\n\n\nSurvival Curve in 2004 for US Females\n\n\n\n\nLikelihood with censoring *\n\n\n\n\n\n\nNote\n\n\n\nThis subsection was not presented in class in 2023; it is not necessary to understand for the qualifying exam.\n\n\nIf an event time \\(T\\) is observed exactly as \\(T=t\\), then the likelihood of that observation is just its probability density function:\n\\[\n\\begin{aligned}\n\\mathcal L(t)\n&= p(T=t)\\\\\n&\\stackrel{\\text{def}}{=}f_T(t)\\\\\n&= h_T(t)S_T(t)\\\\\n\\ell(t)\n&\\stackrel{\\text{def}}{=}\\text{log}\\left\\{\\mathcal L(t)\\right\\}\\\\\n&= \\text{log}\\left\\{h_T(t)S_T(t)\\right\\}\\\\\n&= \\text{log}\\left\\{h_T(t)\\right\\} + \\text{log}\\left\\{S_T(t)\\right\\}\\\\\n&= \\text{log}\\left\\{h_T(t)\\right\\} - H_T(t)\\\\\n\\end{aligned}\n\\]\nIf instead the event time \\(T\\) is censored and only known to be after time \\(y\\), then the likelihood of that censored observation is instead the survival function evaluated at the censoring time:\n\\[\n\\begin{aligned}\n\\mathcal L(y)\n&=p_T(T&gt;y)\\\\\n&\\stackrel{\\text{def}}{=}S_T(y)\\\\\n\\ell(y)\n&\\stackrel{\\text{def}}{=}\\text{log}\\left\\{\\mathcal L(y)\\right\\}\\\\\n&=\\text{log}\\left\\{S(y)\\right\\}\\\\\n&=-H(y)\\\\\n\\end{aligned}\n\\]\nWhat’s written above is incomplete. We also observed whether or not the observation was censored. Let \\(C\\) denote the time when censoring would occur (if the event did not occur first); let \\(f_C(y)\\) and \\(S_C(y)\\) be the corresponding density and survival functions for the censoring event.\nLet \\(Y\\) denote the time when observation ended (either by censoring or by the event of interest occurring), and let \\(D\\) be an indicator variable for the event occurring at \\(Y\\) (so \\(D=0\\) represents a censored observation and \\(D=1\\) represents an uncensored observation). In other words, let \\(Y \\stackrel{\\text{def}}{=}\\min(T,C)\\) and \\(D \\stackrel{\\text{def}}{=}\\mathbb 1{\\{T&lt;=C\\}}\\).\nThen the complete likelihood of the observed data \\((Y,D)\\) is:\n\\[\n\\begin{aligned}\n\\mathcal L(y,d)\n&= p(Y=y, D=d)\\\\\n&= \\left[p(T=y,C&gt; y)\\right]^d \\cdot\n\\left[p(T&gt;y,C=y)\\right]^{1-d}\\\\\n\\end{aligned}\n\\]\nTypically, survival analyses assume that \\(C\\) and \\(T\\) are mutually independent; this assumption is called “non-informative” censoring.\nThen the joint likelihood \\(p(Y,D)\\) factors into the product \\(p(Y), p(D)\\), and the likelihood reduces to:\n\\[\n\\begin{aligned}\n\\mathcal L(y,d)\n&= \\left[p(T=y,C&gt; y)\\right]^d\\cdot\n\\left[p(T&gt;y,C=y)\\right]^{1-d}\\\\\n&= \\left[p(T=y)p(C&gt; y)\\right]^d\\cdot\n\\left[p(T&gt;y)p(C=y)\\right]^{1-d}\\\\\n&= \\left[f_T(y)S_C(y)\\right]^d\\cdot\n\\left[S(y)f_C(y)\\right]^{1-d}\\\\\n&= \\left[f_T(y)^d S_C(y)^d\\right]\\cdot\n\\left[S_T(y)^{1-d}f_C(y)^{1-d}\\right]\\\\\n&= \\left(f_T(y)^d \\cdot S_T(y)^{1-d}\\right)\\cdot\n\\left(f_C(y)^{1-d} \\cdot S_C(y)^{d}\\right)\n\\end{aligned}\n\\]\nThe corresponding log-likelihood is:\n\\[\n\\begin{aligned}\n\\ell(y,d)\n&= \\text{log}\\left\\{\\mathcal L(y,d) \\right\\}\\\\\n&= \\text{log}\\left\\{\n\\left(f_T(y)^d \\cdot S_T(y)^{1-d}\\right)\\cdot\n\\left(f_C(y)^{1-d} \\cdot S_C(y)^{d}\\right)\n\\right\\}\\\\\n&= \\text{log}\\left\\{\nf_T(y)^d \\cdot S_T(y)^{1-d}\n\\right\\}\n+\n\\text{log}\\left\\{\nf_C(y)^{1-d} \\cdot S_C(y)^{d}\n\\right\\}\\\\\n\\end{aligned}\n\\] Let\n\n\n\\(\\theta_T\\) represent the parameters of \\(p_T(t)\\),\n\n\\(\\theta_C\\) represent the parameters of \\(p_C(c)\\),\n\n\\(\\theta = (\\theta_T, \\theta_C)\\) be the combined vector of all parameters.\n\nThen corresponding score function is:\n\\[\n\\begin{aligned}\n\\ell'(y,d)\n&= \\frac{d}{d\\theta}\n\\left[\n\\text{log}\\left\\{\nf_T(y)^d \\cdot S_T(y)^{1-d}\n\\right\\}\n+\n\\text{log}\\left\\{\nf_C(y)^{1-d} \\cdot S_C(y)^{d}\n\\right\\}\n\\right]\\\\\n&=\n\\left(\n\\frac{d}{d\\theta}\n\\text{log}\\left\\{\nf_T(y)^d \\cdot S_T(y)^{1-d}\n\\right\\}\n\\right)\n+\n\\left(\n\\frac{d}{d\\theta}\n\\text{log}\\left\\{\nf_C(y)^{1-d} \\cdot S_C(y)^{d}\n\\right\\}\n\\right)\\\\\n\\end{aligned}\n\\]\nAs long as \\(\\theta_C\\) and \\(\\theta_T\\) don’t share any parameters, then if censoring is non-informative, the partial derivative with respect to \\(\\theta_T\\) is:\n\\[\n\\begin{aligned}\n\\ell'_{\\theta_T}(y,d)\n&\\stackrel{\\text{def}}{=}\\frac{d}{d\\theta_T}\\ell(y,d)\\\\\n&=\n\\left(\n\\frac{d}{d\\theta_T}\n\\text{log}\\left\\{\nf_T(y)^d \\cdot S_T(y)^{1-d}\n\\right\\}\n\\right)\n+\n\\left(\n\\frac{d}{d\\theta_T}\n\\text{log}\\left\\{\nf_C(y)^{1-d} \\cdot S_C(y)^{d}\n\\right\\}\n\\right)\\\\\n&=\n\\left(\n\\frac{d}{d\\theta_T}\n\\text{log}\\left\\{\nf_T(y)^d \\cdot S_T(y)^{1-d}\n\\right\\}\n\\right) + 0\\\\\n&=\n\\frac{d}{d\\theta_T}\n\\text{log}\\left\\{\nf_T(y)^d \\cdot S_T(y)^{1-d}\n\\right\\}\\\\\n\\end{aligned}\n\\] Thus, the MLE for \\(\\theta_T\\) won’t depend on \\(\\theta_C\\), and we can ignore the distribution of \\(C\\) when estimating the parameters of \\(f_T(t)=p(T=t)\\).\nThen:\n\\[\n\\begin{aligned}\n\\mathcal L(y,d)\n&= f_T(y)^d \\cdot S_T(y)^{1-d}\\\\\n&= \\left(h_T(y)^d  S_T(y)^d\\right) \\cdot S_T(y)^{1-d}\\\\\n&= h_T(y)^d  \\cdot S_T(y)^d \\cdot S_T(y)^{1-d}\\\\\n&= h_T(y)^d \\cdot S_T(y)\\\\\n&= S_T(y) \\cdot h_T(y)^d \\\\\n\\end{aligned}\n\\]\nThat is, if the event occurred at time \\(y\\) (i.e., if \\(d=1\\)), then the likelihood of \\((Y,D) = (y,d)\\) is equal to the hazard function at \\(y\\) times the survival function at \\(y\\). Otherwise, the likelihood is equal to just the survival function at \\(y\\).\nThe corresponding log-likelihood is:\n\\[\n\\begin{aligned}\n\\ell(y,d)\n&=\\text{log}\\left\\{\\mathcal L(y,d)\\right\\}\\\\\n&= \\text{log}\\left\\{S_T(y) \\cdot h_T(y)^d\\right\\}\\\\\n&= \\text{log}\\left\\{S_T(y)\\right\\} + \\text{log}\\left\\{h_T(y)^d\\right\\}\\\\\n&= \\text{log}\\left\\{S_T(y)\\right\\} + d\\cdot \\text{log}\\left\\{h_T(y)\\right\\}\\\\\n&= -H_T(y) + d\\cdot \\text{log}\\left\\{h_T(y)\\right\\}\\\\\n\\end{aligned}\n\\]\nIn other words, the log-likelihood contribution from a single observation \\((Y,D) = (y,d)\\) is equal to the negative cumulative hazard at \\(y\\), plus the log of the hazard at \\(y\\) if the event occurred at time \\(y\\).\n\n\n\n\n\n\nNote\n\n\n\nEnd of extra section."
  },
  {
    "objectID": "intro-to-survival-analysis.html#parametric-models-for-time-to-event-outcomes",
    "href": "intro-to-survival-analysis.html#parametric-models-for-time-to-event-outcomes",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.4 Parametric Models for Time-to-Event Outcomes",
    "text": "5.4 Parametric Models for Time-to-Event Outcomes\n\n5.4.1 Exponential Distribution\n\nThe exponential distribution is the base distribution for survival analysis.\nThe distribution has a constant hazard \\(\\lambda\\)\n\nThe mean survival time is \\(\\lambda^{-1}\\)\n\n\nMathematical details of exponential distribution\n\\[\n\\begin{aligned}\nf(t) &= \\lambda \\text{e}^{-\\lambda t}\\\\\nE(t) &= \\lambda^{-1}\\\\\nVar(t) &= \\lambda^{-2}\\\\\nF(t) &= 1-\\text{e}^{-\\lambda x}\\\\\nS(t)&= \\text{e}^{-\\lambda x}\\\\\n\\ln(S(t))&=-\\lambda x\\\\\nh(t) &= -\\frac{f(t)}{S(t)} = -\\frac{\\lambda \\text{e}^{-\\lambda t}}{\\text{e}^{-\\lambda t}}=\\lambda\n\\end{aligned}\n\\]\nEstimation of \\(\\lambda\\)\n\n\nSuppose we have \\(m\\) exponential survival times of \\(t_1, t_2,\\ldots,t_m\\) and \\(k\\) right-censored values at \\(u_1,u_2,\\ldots,u_k\\).\nA survival time of \\(t_i=10\\) means that subject \\(i\\) died at time 10. A right-censored time \\(u_i=10\\) means that at time 10, subject \\(i\\) was still alive and that we have no further follow-up.\nFor the moment we will assume that the survival distribution is exponential and that all the subjects have the same parameter \\(\\lambda\\).\n\nWe have \\(m\\) exponential survival times of \\(t_1, t_2,\\ldots,t_m\\) and \\(k\\) right-censored values at \\(u_1,u_2,\\ldots,u_k\\). The log-likelihood of an observed survival time \\(t_i\\) is \\[\n\\text{log}\\left\\{\\lambda \\text{e}^{-\\lambda t_i}\\right\\} =\n\\text{log}\\left\\{\\lambda\\right\\}-\\lambda t_i\n\\] and the likelihood of a censored value is the probability of that outcome (survival greater than \\(u_j\\)) so the log-likelihood is \\[\n\\text{log}\\left\\{\\lambda \\text{e}^{u_j}\\right\\} =-\\lambda u_j.\n\\]\nLet \\(T=\\sum t_i\\) and \\(U=\\sum u_j\\). Then:\n\\[\n\\begin{aligned}\n\\ell(\\lambda) &= \\sum_{i=1}^m( \\ln \\lambda-\\lambda t_i) + \\sum_{j=1}^k (-\\lambda u_j)\\\\\n&= m \\ln \\lambda -(T+U)\\lambda\\\\\n\\ell'(\\lambda)\n&=m\\lambda^{-1} -(T+U)\\\\\n\\hat{\\lambda} &= \\frac{m}{T+U}\\\\\n\\ell''&=-m/\\lambda^2\\\\\n&&lt; 0\\\\\n\\hat E[T] &= \\hat\\lambda^{-1}\\\\\n&= \\frac{T+U}{m}\n\\end{aligned}\n\\]\nFisher Information and Standard Error\n\\[\n\\begin{aligned}\nE[-\\ell'']\n& = m/\\lambda^2\\\\\n\\text{Var}\\left(\\hat\\lambda\\right)\n&\\approx \\left(E[-\\ell'']\\right)^{-1}\\\\\n&=\\lambda^2/m\\\\\n\\text{SE}\\left(\\hat\\lambda\\right)\n&= \\sqrt{\\text{Var}\\left(\\hat\\lambda\\right)}\\\\\n&\\approx \\lambda/\\sqrt{m}\n\\end{aligned}\n\\]\n\\(\\hat\\lambda\\) depends on the censoring times of the censored observations, but \\(\\text{Var}\\left(\\hat\\lambda\\right)\\) only depends on the number of uncensored observations, \\(m\\), and not on the number of censored observations (\\(k\\)).\nOther Parametric Survival Distributions\n\nAny density on \\([0,\\infty)\\) can be a survival distribution, but the most useful ones are all skew right.\nThe commonest generalization of the exponential is the Weibull.\nOther common choices are the gamma, log-normal, log-logistic, Gompertz, inverse Gaussian, and Pareto.\nMost of what we do going forward is non-parametric or semi-parametric, but sometimes these parametric distributions provide a useful approach.\n\n5.4.2 Weibull Distribution\n\\[\n\\begin{aligned}\np(t)&= \\alpha\\lambda x^{\\alpha-1}\\text{e}^{-\\lambda x^\\alpha}\\\\\nh(t)&=\\alpha\\lambda x^{\\alpha-1}\\\\\nS(t)&=\\text{e}^{-\\lambda x^\\alpha}\\\\\nE(T)&= \\Gamma(1+1/\\alpha)\\cdot \\lambda^{-1/\\alpha}\n\\end{aligned}\n\\]\nWhen \\(\\alpha=1\\) this is the exponential. When \\(\\alpha&gt;1\\) the hazard is increasing and when \\(\\alpha &lt; 1\\) the hazard is decreasing. This provides more flexibility than the exponential.\nWe will see more of this distribution later."
  },
  {
    "objectID": "intro-to-survival-analysis.html#nonparametric-survival-analysis",
    "href": "intro-to-survival-analysis.html#nonparametric-survival-analysis",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.5 Nonparametric Survival Analysis",
    "text": "5.5 Nonparametric Survival Analysis\n\n5.5.1 Basic ideas\n\nMostly, we work without a parametric model.\nThe first task is to estimate a survival function from data listing survival times, and censoring times for censored data.\nFor example one patient may have relapsed at 10 months. Another might have been followed for 32 months without a relapse having occurred (censored).\nThe minimum information we need for each patient is a time and a censoring variable which is 1 if the event occurred at the indicated time and 0 if this is a censoring time."
  },
  {
    "objectID": "intro-to-survival-analysis.html#example-clinical-trial-for-pediatric-acute-leukemia",
    "href": "intro-to-survival-analysis.html#example-clinical-trial-for-pediatric-acute-leukemia",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.6 Example: clinical trial for pediatric acute leukemia",
    "text": "5.6 Example: clinical trial for pediatric acute leukemia\n\n5.6.1 Overview of study\nThis is from a clinical trial in 1963 for 6-MP treatment vs. placebo for Acute Leukemia in 42 children.\n\n\nPairs of children:\n\nmatched by remission status at the time of treatment (remstat: 1 = partial, 2 = complete)\nrandomized to 6-MP (exit times in t2) or placebo (exit times in t1)\n\n\nFollowed until relapse or end of study.\nAll of the placebo group relapsed, but some of the 6-MP group were censored (which means they were still in remission); indicated by relapse variable (0 = censored, 1 = relapse).\n6-MP = 6-Mercaptopurine (Purinethol) is an anti-cancer (“antineoplastic” or “cytotoxic”) chemotherapy drug used currently for Acute lymphoblastic leukemia (ALL). It is classified as an antimetabolite.\n\n5.6.2 Study design\nClinical trial in 1963 for 6-MP treatment vs. placebo for Acute Leukemia in 42 children. Pairs of children matched by remission status at the time of treatment (1 = partial or 2 = complete) and randomized to 6-MP or placebo. Followed until relapse or end of study. All of the placebo group relapsed, but some of the 6-MP group were censored.\n\nCodelibrary(KMsurv)\ndata(drug6mp)\ndrug6mp |&gt; tibble() |&gt; print()\n\n# A tibble: 21 × 5\n    pair remstat    t1    t2 relapse\n   &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;\n 1     1       1     1    10       1\n 2     2       2    22     7       1\n 3     3       2     3    32       0\n 4     4       2    12    23       1\n 5     5       2     8    22       1\n 6     6       1    17     6       1\n 7     7       2     2    16       1\n 8     8       2    11    34       0\n 9     9       2     8    32       0\n10    10       2    12    25       0\n# ℹ 11 more rows\n\n\n\n5.6.3 Data documentation for drug6mp\n\n\nCodelibrary(printr) # inserts help-file output into markdown output\nlibrary(KMsurv)\n?drug6mp\n\n\n\n\n\n\ndrug6mp\nR Documentation\n\ndata from Section 1.2\n\nDescription\n\nThe drug6mp data frame has 21 rows and 5 columns.\n\n\n\nFormat\n\nThis data frame contains the following columns:\n\n\n\npair\n\npair number\n\n\nremstat\n\nRemission status at randomization (1=partial, 2=complete)\n\n\nt1\n\nTime to relapse for placebo patients, months \n\n\nt2\n\nTime to relapse for 6-MP patients, months \n\n\nrelapse\n\nRelapse indicator (0=censored, 1=relapse) for 6-MP patients\n\n\n\n\n\n\n\n\n5.6.4 Descriptive Statistics\n\nThe average time in each group is not useful. Some of the 6-MP patients have not relapsed at the time recorded, while all of the placebo patients have relapsed.\nThe median time is not really useful either because so many of the 6-MP patients have not relapsed (12/21).\nBoth are biased down in the 6-MP group. Remember that lower times are worse since they indicate sooner recurrence.\nWe can compute the average hazard rate, which is the estimate of the exponential parameter: number of relapses divided by the sum of the times.\nFor the placebo, that is just the reciprocal of the mean time = 1/8.667 = 0.115.\nFor the 6-MP group this is 9/359 = 0.025\nThe estimated average hazard in the placebo group is 4.6 times as large (if the hazard is constant over time)."
  },
  {
    "objectID": "intro-to-survival-analysis.html#the-kaplan-meier-product-limit-estimator",
    "href": "intro-to-survival-analysis.html#the-kaplan-meier-product-limit-estimator",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.7 The Kaplan-Meier Product Limit Estimator",
    "text": "5.7 The Kaplan-Meier Product Limit Estimator\n\nThe estimated survival function for the placebo patients is easy to compute. For any time \\(t\\) in months, \\(S(t)\\) is the fraction of patients with times greater than \\(t\\).\nFor the 6-MP patients, we cannot ignore the censored data because we know that the time to relapse is greater than the censoring time.\nFor any time \\(t\\) in months, we know that 6-MP patients with times greater than \\(t\\) have not relapsed, and those with relapse time less than \\(t\\) have relapsed, but we don’t know if patients with censored time less than \\(t\\) have relapsed or not.\nThe procedure we usually use is the Kaplan-Meier product-limit estimator of the survival function.\nThe Kaplan-Meier estimator is a step function (like the empirical cdf), which changes value only at the event times, not at the censoring times.\nAt each event time \\(t\\), we compute the at-risk group size \\(Y\\), which is all those observations whose event time or censoring time is at least \\(t\\).\nIf \\(d\\) of the observations have an event time (not a censoring time) of \\(t\\), then the group of survivors immediately following time \\(t\\) is reduced by the fraction \\[\\frac{Y-d}{Y}=1-\\frac{d}{Y}\\]\n\nIf the event times are \\(t_i\\) with events per time of \\(d_i\\) (\\(1\\le i \\le k\\)), then \\[\\hat S(t) = \\prod_{t_i &lt; t}[1-d_i/Y_i]\\] where \\(Y_i\\) is the set of observations whose time (event or censored) is \\(\\ge t_i\\), the group at risk at time \\(t_i\\).\nIf there are no censored data, and there are \\(n\\) data points, then just after (say) the third event time \\[\n\\begin{aligned}\n\\hat S(t)\n&= \\prod_{t_i &lt; t}[1-d_i/Y_i]\\\\\n&=[\\frac{n-d_1}{n}][\\frac{n-d_1-d_2}{n-d_1}][\\frac{n-d_1-d_2-d_3}{n-d_1-d_2}]\\\\\n&=\\frac{n-d_1-d_2-d_3}{n}\\\\\n&=1-\\frac{d_1+d_2+d_3}{n}\\\\\n&=1-\\hat F(t)\\\\\n\\end{aligned}\n\\]\nwhere \\(\\hat F(t)\\) is the usual empirical CDF estimate.\n\n5.7.1 Kaplan-Meier curve for drug6mp data\nHere is the Kaplan-Meier estimated survival curve for the patients who received 6-MP in the drug6mp dataset (we will see code to produce figures like this one shortly):\n\n\n\nKaplan-Meier Survival Curve for 6-MP Patients\n\n\n\n\n\n5.7.2 Kaplan-Meier calculations\nLet’s compute these estimates and build the chart by hand:\n\nCodelibrary(KMsurv)\nlibrary(dplyr)\ndata(drug6mp)\n\ndrug6mp.v2 = \n  drug6mp |&gt; \n  as_tibble() |&gt; \n  mutate(\n    remstat = remstat |&gt; \n      case_match(\n        1 ~ \"partial\",\n        2 ~ \"complete\"\n      ),\n    # renaming to \"outcome\" while relabeling is just a style choice:\n    outcome = relapse |&gt; \n      case_match(\n        0 ~ \"censored\",\n        1 ~ \"relapsed\"\n      )\n  )\n\nkm.6mp =\n  drug6mp.v2 |&gt; \n  summarize(\n    .by = t2,\n    Relapses = sum(outcome == \"relapsed\"),\n    Censored = sum(outcome == \"censored\")) |&gt;\n  # here we add a start time row, so the graph starts at time 0:\n  bind_rows(\n    tibble(\n      t2 = 0, \n      Relapses = 0, \n      Censored = 0)\n  ) |&gt; \n  # sort in time order:\n  arrange(t2) |&gt;\n  mutate(\n    Exiting = Relapses + Censored,\n    `Study Size` = sum(Exiting),\n    Exited = cumsum(Exiting) |&gt; dplyr::lag(default = 0),\n    `At Risk` = `Study Size` - Exited,\n    Hazard = Relapses / `At Risk`,\n    `KM Factor` = 1 - Hazard,\n    `Cumulative Hazard` = cumsum(`Hazard`),\n    `KM Survival Curve` = cumprod(`KM Factor`)\n  )\n\nlibrary(pander)  \npander(km.6mp)\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n\n\nt2\nRelapses\nCensored\nExiting\nStudy Size\nExited\nAt Risk\nHazard\n\n\n\n0\n0\n0\n0\n21\n0\n21\n0\n\n\n6\n3\n1\n4\n21\n0\n21\n0.1429\n\n\n7\n1\n0\n1\n21\n4\n17\n0.05882\n\n\n9\n0\n1\n1\n21\n5\n16\n0\n\n\n10\n1\n1\n2\n21\n6\n15\n0.06667\n\n\n11\n0\n1\n1\n21\n8\n13\n0\n\n\n13\n1\n0\n1\n21\n9\n12\n0.08333\n\n\n16\n1\n0\n1\n21\n10\n11\n0.09091\n\n\n17\n0\n1\n1\n21\n11\n10\n0\n\n\n19\n0\n1\n1\n21\n12\n9\n0\n\n\n20\n0\n1\n1\n21\n13\n8\n0\n\n\n22\n1\n0\n1\n21\n14\n7\n0.1429\n\n\n23\n1\n0\n1\n21\n15\n6\n0.1667\n\n\n25\n0\n1\n1\n21\n16\n5\n0\n\n\n32\n0\n2\n2\n21\n17\n4\n0\n\n\n34\n0\n1\n1\n21\n19\n2\n0\n\n\n35\n0\n1\n1\n21\n20\n1\n0\n\n\n\n\n\n\n\n\n\n\nKM Factor\nCumulative Hazard\nKM Survival Curve\n\n\n\n1\n0\n1\n\n\n0.8571\n0.1429\n0.8571\n\n\n0.9412\n0.2017\n0.8067\n\n\n1\n0.2017\n0.8067\n\n\n0.9333\n0.2683\n0.7529\n\n\n1\n0.2683\n0.7529\n\n\n0.9167\n0.3517\n0.6902\n\n\n0.9091\n0.4426\n0.6275\n\n\n1\n0.4426\n0.6275\n\n\n1\n0.4426\n0.6275\n\n\n1\n0.4426\n0.6275\n\n\n0.8571\n0.5854\n0.5378\n\n\n0.8333\n0.7521\n0.4482\n\n\n1\n0.7521\n0.4482\n\n\n1\n0.7521\n0.4482\n\n\n1\n0.7521\n0.4482\n\n\n1\n0.7521\n0.4482\n\n\n\n\n\nSummary\nFor the 6-MP patients at time 6 months, there are 21 patients at risk. At \\(t=6\\) there are 3 relapses and 1 censored observations.\nThe Kaplan-Meier factor is \\((21-3)/21 = 0.857\\). The number at risk for the next time (\\(t=7\\)) is \\(21-3-1=17\\).\nAt time 7 months, there are 17 patients at risk. At \\(t=7\\) there is 1 relapse and 0 censored observations. The Kaplan-Meier factor is \\((17-1)/17 = 0.941\\). The Kaplan Meier estimate is \\(0.857\\times0.941=0.807\\). The number at risk for the next time (\\(t=9\\)) is \\(17-1=16\\).\nNow, let’s graph this estimated survival curve using ggplot():\n\nCodelibrary(ggplot2)\nconflicts_prefer(dplyr::filter)\nkm.6mp |&gt; \n  ggplot(aes(x = t2, y = `KM Survival Curve`)) +\n  geom_step() +\n  geom_point(data = km.6mp |&gt; filter(Censored &gt; 0), shape = 3) +\n  expand_limits(y = c(0,1), x = 0) +\n  xlab('Time since diagnosis (months)') +\n  ylab(\"KM Survival Curve\") +\n  scale_y_continuous(labels = scales::percent)"
  },
  {
    "objectID": "intro-to-survival-analysis.html#using-the-survival-package-in-r",
    "href": "intro-to-survival-analysis.html#using-the-survival-package-in-r",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.8 Using the survival package in R",
    "text": "5.8 Using the survival package in R\nWe don’t have to do these calculations by hand every time; the survival package and several others have functions available to automate many of these tasks (full list: https://cran.r-project.org/web/views/Survival.html).\n\n5.8.1 The Surv function\nTo use the survival package, the first step is telling R how to combine the exit time and exit reason (censoring versus event) columns. The Surv() function accomplishes this task.\nExample: Surv() with drug6mp data\n\nCodelibrary(survival)\ndrug6mp.v3 = \n  drug6mp.v2 |&gt; \n  mutate(\n    surv2 = Surv(\n      time = t2,\n      event = (outcome == \"relapsed\")))\n\nprint(drug6mp.v3)\n\n# A tibble: 21 × 7\n    pair remstat     t1    t2 relapse outcome   surv2\n   &lt;int&gt; &lt;chr&gt;    &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;chr&gt;    &lt;Surv&gt;\n 1     1 partial      1    10       1 relapsed    10 \n 2     2 complete    22     7       1 relapsed     7 \n 3     3 complete     3    32       0 censored    32+\n 4     4 complete    12    23       1 relapsed    23 \n 5     5 complete     8    22       1 relapsed    22 \n 6     6 partial     17     6       1 relapsed     6 \n 7     7 complete     2    16       1 relapsed    16 \n 8     8 complete    11    34       0 censored    34+\n 9     9 complete     8    32       0 censored    32+\n10    10 complete    12    25       0 censored    25+\n# ℹ 11 more rows\n\n\nThe output of Surv() is a vector of objects with class Surv. When we print this vector:\n\nobservations where the event was observed are printed as the event time (for example, surv2 = 10 on line 1)\nobservations where the event was right-censored are printed as the censoring time with a plus sign (+; for example, surv2 = 32+ on line 3).\n\n5.8.2 The survfit function\nOnce we have constructed our Surv variable, we can calculate the Kaplan-Meier estimate of the survival curve using the survfit() function.\n\n\n\n\n\n\nNote\n\n\n\nThe documentation for ?survfit isn’t too helpful; the survfit.formula documentation is better.\n\n\nExample: survfit() with drug6mp data\nHere we use survfit() to create a survfit object, which contains the Kaplan-Meier estimate:\n\nCodedrug6mp.km_model = survfit(\n  formula = surv2 ~ 1, \n  data = drug6mp.v3)\n\n\nprint.survfit() just gives some summary statistics:\n\nCodeprint(drug6mp.km_model)\n\nCall: survfit(formula = surv2 ~ 1, data = drug6mp.v3)\n\n      n events median 0.95LCL 0.95UCL\n[1,] 21      9     23      16      NA\n\n\nsummary.survfit() shows us the underlying Kaplan-Meier table:\n\nCodesummary(drug6mp.km_model)\n\nCall: survfit(formula = surv2 ~ 1, data = drug6mp.v3)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    6     21       3    0.857  0.0764        0.720        1.000\n    7     17       1    0.807  0.0869        0.653        0.996\n   10     15       1    0.753  0.0963        0.586        0.968\n   13     12       1    0.690  0.1068        0.510        0.935\n   16     11       1    0.627  0.1141        0.439        0.896\n   22      7       1    0.538  0.1282        0.337        0.858\n   23      6       1    0.448  0.1346        0.249        0.807\n\n\nsummary.survfit() shows us the underlying Kaplan-Meier table:\n\nCodesummary(drug6mp.km_model)\n\nCall: survfit(formula = surv2 ~ 1, data = drug6mp.v3)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    6     21       3   0.8571 0.07636       0.7198       1.0000\n    7     17       1   0.8067 0.08694       0.6531       0.9964\n   10     15       1   0.7529 0.09635       0.5859       0.9676\n   13     12       1   0.6902 0.10681       0.5096       0.9348\n   16     11       1   0.6275 0.11405       0.4394       0.8960\n   22      7       1   0.5378 0.12823       0.3370       0.8582\n   23      6       1   0.4482 0.13459       0.2488       0.8074\n\n\nWe can specify which time points we want using the times argument:\n\nCodesummary(\n  drug6mp.km_model, \n  times = c(0, drug6mp.v3$t2))\n\nCall: survfit(formula = surv2 ~ 1, data = drug6mp.v3)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     21       0   1.0000 0.00000       1.0000       1.0000\n    6     21       3   0.8571 0.07636       0.7198       1.0000\n    6     21       0   0.8571 0.07636       0.7198       1.0000\n    6     21       0   0.8571 0.07636       0.7198       1.0000\n    6     21       0   0.8571 0.07636       0.7198       1.0000\n    7     17       1   0.8067 0.08694       0.6531       0.9964\n    9     16       0   0.8067 0.08694       0.6531       0.9964\n   10     15       1   0.7529 0.09635       0.5859       0.9676\n   10     15       0   0.7529 0.09635       0.5859       0.9676\n   11     13       0   0.7529 0.09635       0.5859       0.9676\n   13     12       1   0.6902 0.10681       0.5096       0.9348\n   16     11       1   0.6275 0.11405       0.4394       0.8960\n   17     10       0   0.6275 0.11405       0.4394       0.8960\n   19      9       0   0.6275 0.11405       0.4394       0.8960\n   20      8       0   0.6275 0.11405       0.4394       0.8960\n   22      7       1   0.5378 0.12823       0.3370       0.8582\n   23      6       1   0.4482 0.13459       0.2488       0.8074\n   25      5       0   0.4482 0.13459       0.2488       0.8074\n   32      4       0   0.4482 0.13459       0.2488       0.8074\n   32      4       0   0.4482 0.13459       0.2488       0.8074\n   34      2       0   0.4482 0.13459       0.2488       0.8074\n   35      1       0   0.4482 0.13459       0.2488       0.8074\n\n\n\nCode?summary.survfit\n\n\n\n\n\n\nsummary.survfit\nR Documentation\n\n\n5.9 Summary of a Survival Curve\n\n5.9.1 Description\nReturns a list containing the survival curve, confidence limits for the curve, and other information.\n\n5.9.2 Usage\n  ## S3 method for class 'survfit'\nsummary(object, times, censored=FALSE, scale=1,\n  extend=FALSE, rmean=getOption('survfit.rmean'), ...)\n  \n\n5.9.3 Arguments\n\n\nobject\nthe result of a call to the survfit function.\n\n\ntimes\nvector of times; the returned matrix will contain 1 row for each time. The vector will be sorted into increasing order; missing values are not allowed. If censored=T, the default times vector contains all the unique times in fit, otherwise the default times vector uses only the event (death) times.\n\n\ncensored\nlogical value: should the censoring times be included in the output? This is ignored if the times argument is present.\n\n\nscale\nnumeric value to rescale the survival time, e.g., if the input data to survfit were in days, scale = 365.25 would scale the output to years.\n\n\nextend\nlogical value: if TRUE, prints information for all specified times, even if there are no subjects left at the end of the specified times. This is only used if the times argument is present.\n\n\nrmean\nShow restricted mean: see print.survfit for details\n\n\n...\nfor future methods\n\n\n\n\n\n\n\n\n\n5.9.4 Plotting estimated survival functions\nWe can plot survfit objects with plot(), autoplot(), or ggsurvplot():\n\nCodelibrary(ggfortify)\nautoplot(drug6mp.km_model)\n\n\nKaplan-Meier Survival Curve for 6-MP Patients\n\n\n\nCode# not shown:\n# plot(drug6mp.km_model)\n\n# library(survminer)\n# ggsurvplot(drug6mp.km_model)\n\n\nquantiles of survival curve\nWe can extract quantiles with quantile():\n\nCodedrug6mp.km_model |&gt; \n  quantile(p = c(.25, .5)) |&gt; \n  as_tibble() |&gt; \n  mutate(p = c(.25, .5)) |&gt; \n  relocate(p, .before = everything())\n\n\n\np\nquantile\nlower\nupper\n\n\n\n0.25\n13\n6\nNA\n\n\n0.50\n23\n16\nNA\n\n\n\n\n\n\n5.9.5 Two-sample tests\nThe survdiff function\n\nCode?survdiff\n\n\n\n\n\n\nsurvdiff\nR Documentation\n\n\nTest Survival Curve Differences\n\n\nDescription\n\nTests if there is a difference between two or more survival curves using\nthe G^\\rho family of tests, or for a single curve against a known alternative.\n\n\n\nUsage\n\nsurvdiff(formula, data, subset, na.action, rho=0, timefix=TRUE)\n\n\n\n\n\n\n\nExample: survdiff() with drug6mp data\nNow we are going to compare the placebo and 6-MP data. We need to reshape the data to make it usable with the standard survival workflow:\n\nCodelibrary(survival)\n\ndrug6mp.v4 = \n  drug6mp.v3 |&gt;\n  select(pair, remstat, t1, t2, outcome) |&gt; \n  # here we are going to change the data from a wide format to long:\n  pivot_longer(\n    cols = c(t1, t2),\n    names_to = \"treatment\",\n    values_to = \"exit_time\") |&gt; \n  mutate(\n    treatment = treatment |&gt; \n      case_match(\n        \"t1\" ~ \"placebo\",\n        \"t2\" ~ \"6-MP\"\n      ),\n    outcome = if_else(\n      treatment == \"placebo\",\n      \"relapsed\",\n      outcome\n    ),\n    surv = Surv(\n      time = exit_time,\n      event = (outcome == \"relapsed\"))\n  )\n\n\nUsing this long data format, we can fit a Kaplan-Meier curve for each treatment group simultaneously:\n\nCodedrug6mp.km_model2 = \n   survfit(\n  formula = surv ~ treatment, \n  data = drug6mp.v4)\n\n\nWe can plot the curves in the same graph:\n\nCodedrug6mp.km_model2 |&gt; autoplot()\n\n\n\n\nWe can also perform something like a t-test, where the null hypothesis is that the curves are the same:\n\nCodesurvdiff(\n  formula = surv ~ treatment,\n  data = drug6mp.v4)\n\nCall:\nsurvdiff(formula = surv ~ treatment, data = drug6mp.v4)\n\n                   N Observed Expected (O-E)^2/E (O-E)^2/V\ntreatment=6-MP    21        9    19.25     5.458     16.79\ntreatment=placebo 21       21    10.75     9.775     16.79\n\n Chisq= 16.8  on 1 degrees of freedom, p= 4.2e-05 \n\n\nBy default, survdiff() ignores any pairing, but we can use strata() to perform something similar to a paired t-test:\n\nCodesurvdiff(\n  formula = surv ~ treatment + strata(pair),\n  data = drug6mp.v4)\n\nCall:\nsurvdiff(formula = surv ~ treatment + strata(pair), data = drug6mp.v4)\n\n                   N Observed Expected (O-E)^2/E (O-E)^2/V\ntreatment=6-MP    21        9     16.5     3.409     10.71\ntreatment=placebo 21       21     13.5     4.167     10.71\n\n Chisq= 10.7  on 1 degrees of freedom, p= 0.0011 \n\n\nInterestingly, accounting for pairing reduces the significant of the difference."
  },
  {
    "objectID": "intro-to-survival-analysis.html#summary.survfit",
    "href": "intro-to-survival-analysis.html#summary.survfit",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.9 Summary of a Survival Curve",
    "text": "5.9 Summary of a Survival Curve\n\n5.9.1 Description\nReturns a list containing the survival curve, confidence limits for the curve, and other information.\n\n5.9.2 Usage\n  ## S3 method for class 'survfit'\nsummary(object, times, censored=FALSE, scale=1,\n  extend=FALSE, rmean=getOption('survfit.rmean'), ...)\n  \n\n5.9.3 Arguments\n\n\nobject\nthe result of a call to the survfit function.\n\n\ntimes\nvector of times; the returned matrix will contain 1 row for each time. The vector will be sorted into increasing order; missing values are not allowed. If censored=T, the default times vector contains all the unique times in fit, otherwise the default times vector uses only the event (death) times.\n\n\ncensored\nlogical value: should the censoring times be included in the output? This is ignored if the times argument is present.\n\n\nscale\nnumeric value to rescale the survival time, e.g., if the input data to survfit were in days, scale = 365.25 would scale the output to years.\n\n\nextend\nlogical value: if TRUE, prints information for all specified times, even if there are no subjects left at the end of the specified times. This is only used if the times argument is present.\n\n\nrmean\nShow restricted mean: see print.survfit for details\n\n\n...\nfor future methods"
  },
  {
    "objectID": "intro-to-survival-analysis.html#example-bone-marrow-transplant-data",
    "href": "intro-to-survival-analysis.html#example-bone-marrow-transplant-data",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.10 Example: Bone Marrow Transplant Data",
    "text": "5.10 Example: Bone Marrow Transplant Data\n(Copelan et al., 1991)\nTreatment\n\n\nallogeneic (from a donor) bone marrow transplant therapy\n\nInclusion criteria\n\nacute myeloid leukemia (AML)\nacute lymphoblastic leukemia (ALL).\nPossible intermediate events\n\n\ngraft vs. host disease (GVHD): an immunological rejection response to the transplant\n\nplatelet recovery: a return of platelet count to normal levels.\n\nOne or the other, both in either order, or neither may occur.\nEnd point events\n\nrelapse of the disease\ndeath\n\nAny or all of these events may be censored.\n\n5.10.1 KMsurv::bmt data in R\n\nCodelibrary(printr) # inserts help-file output into markdown output\nlibrary(KMsurv)\n?bmt\n\n\n\n\n\n\nbmt\nR Documentation\n\ndata from Section 1.3\n\nDescription\n\nThe bmt data frame has 137 rows and 22 columns.\n\n\n\nFormat\n\nThis data frame contains the following columns:\n\n\n\ngroup\n\nDisease Group 1-ALL, 2-AML Low Risk, 3-AML High Risk\n\n\nt1\n\nTime To Death Or On Study Time\n\n\nt2\n\nDisease Free Survival Time (Time To Relapse, Death Or End Of Study)\n\n\nd1\n\nDeath Indicator 1-Dead 0-Alive\n\n\nd2\n\nRelapse Indicator 1-Relapsed, 0-Disease Free\n\n\nd3\n\nDisease Free Survival Indicator 1-Dead Or Relapsed, 0-Alive Disease Free)\n\n\nta\n\nTime To Acute Graft-Versus-Host Disease\n\n\nda\n\nAcute GVHD Indicator 1-Developed Acute GVHD 0-Never Developed Acute GVHD)\n\n\ntc\n\nTime To Chronic Graft-Versus-Host Disease\n\n\ndc\n\nChronic GVHD Indicator 1-Developed Chronic GVHD 0-Never Developed Chronic GVHD \n\n\ntp\n\nTime To Chronic Graft-Versus-Host Disease\n\n\ndp\n\nPlatelet Recovery Indicator 1-Platelets Returned To Normal, 0-Platelets Never Returned to Normal\n\n\nz1\n\nPatient Age In Years\n\n\nz2\n\nDonor Age In Years\n\n\nz3\n\nPatient Sex: 1-Male, 0-Female\n\n\nz4\n\nDonor Sex: 1-Male, 0-Female\n\n\nz5\n\nPatient CMV Status: 1-CMV Positive, 0-CMV Negative\n\n\nz6\n\nDonor CMV Status: 1-CMV Positive, 0-CMV Negative\n\n\nz7\n\nWaiting Time to Transplant In Days\n\n\nz8\n\nFAB: 1-FAB Grade 4 Or 5 and AML, 0-Otherwise\n\n\nz9\n\nHospital: 1-The Ohio State University, 2-Alferd , 3-St. Vincent, 4-Hahnemann\n\n\nz10\n\nMTX Used as a Graft-Versus-Host- Prophylactic: 1-Yes 0-No\n\n\n\nSource\n\nKlein and Moeschberger (1997) Survival Analysis Techniques for Censored\nand truncated data, Springer.\n\n\n\nExamples\n\ndata(bmt)\n\n\n\n\n\n\n\n\n5.10.2 Analysis plan\n\nWe concentrate for now on disease-free survival (t2 and d3) for the three risk groups, ALL, AML Low Risk, and AML High Risk.\nWe will construct the Kaplan-Meier survival curves, compare them, and test for differences.\nWe will construct the cumulative hazard curves and compare them.\nWe will estimate the hazard functions, interpret, and compare them.\n\n5.10.3 Survival Function Estimate and Variance\n\\[\\hat S(t) = \\prod_{t_i &lt; t}\\left[1-\\frac{d_i}{Y_i}\\right]\\] where \\(Y_i\\) is the group at risk at time \\(t_i\\).\nThe estimated variance of \\(\\hat S(t)\\) is (Greenwood’s formula) \\[\\hat{\\text{Var}}\\left(\\hat S (t)\\right) = \\hat S(t)^2\\sum_{t_i &lt;t}\\frac{d_i}{Y_i(Y_i-d_i)}\\] which we can use for confidence intervals for a survival function or a difference of survival functions.\nKaplan-Meier survival curves\n\ncode to preprocess and model bmt datalibrary(KMsurv)\nlibrary(survival)\ndata(bmt)\n\nbmt = \n  bmt |&gt; \n  as_tibble() |&gt; \n  mutate(\n    group = \n      group |&gt; \n      factor(\n        labels = c(\"ALL\",\"Low Risk AML\",\"High Risk AML\")),\n    surv = Surv(t2,d3))\n\nkm_model1 = survfit(\n  formula = surv ~ group, \n  data = bmt)\n\n\n\nCodelibrary(ggfortify)\nautoplot(\n  km_model1, \n  conf.int = TRUE,\n  ylab = \"Pr(disease-free survival)\",\n  xlab = \"Time since transplant (days)\") + \n  theme_bw() +\n  theme(legend.position=\"bottom\")\n\n\nDisease-Free Survival by Disease Group\n\n\n\n\nUnderstanding Greenwood’s formula (optional)\nTo see where Greenwood’s formula comes from, let \\(x_i=Y_i-d_i\\). We approximate the solution treating each time as independent, with \\(Y_i\\) fixed and ignore randomness in times of failure and we treat \\(x_i\\) as independent binomials \\(\\text{Bin}(Y_i,p_i)\\). Letting \\(S(t)\\) be the “true” survival function\n\\[\n\\begin{aligned}\n\\hat S(t) &=\\prod_{t_i&lt;t}x_i/Y_i\\\\\nS(t)&=\\prod_{t_i&lt;t}p_i\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\hat S(t)}{S(t)}\n&= \\prod_{t_i&lt;t} \\frac{x_i}{p_iY_i}=\\prod_{t_i&lt;t} \\frac{\\hat p_i}{p_i}\\\\\n&=\\prod_{t_i&lt;t} \\left( 1+\\frac{\\hat p_i-p_i}{p_i} \\right)\\\\\n&\\approx 1+\\sum_{t_i&lt;t} \\frac{\\hat p_i-p_i}{p_i} \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\text{Var}\\left(\\frac{\\hat S(t)}{S(t)}\\right)\n&\\approx \\text{Var}\\left(1+\\sum_{t_i&lt;t} \\frac{\\hat p_i-p_i}{p_i}\\right) \\\\\n&=\\sum_{t_i&lt;t} \\frac{1}{p_i^2}\\frac{p_i(1-p_i)}{Y_i} \\\\\n&= \\sum_{t_i&lt;t} \\frac{(1-p_i)}{p_iY_i}\n\\approx\\sum_{t_i&lt;t} \\frac{(1-x_i/Y_i)}{x_i}\\\\\n&=\\sum_{t_i&lt;t} \\frac{Y_i-x_i}{x_iY_i}=\\sum_{t_i&lt;t} \\frac{d_i}{Y_i(Y_i-d_i)}\\\\\n\\text{Var}\\left(\\hat S(t)\\right)\n&\\approx \\hat S(t)^2\\sum_{t_i&lt;t} \\frac{d_i}{Y_i(Y_i-d_i)}\n\\end{aligned}\n\\]\n\n5.10.4 Test for differences among the disease groups\nHere we compute a chi-square test for assocation between disease group (group) and disease-free survival:\n\nCodesurvdiff(surv ~ group, data = bmt)\n\nCall:\nsurvdiff(formula = surv ~ group, data = bmt)\n\n                     N Observed Expected (O-E)^2/E (O-E)^2/V\ngroup=ALL           38       24    21.85    0.2112    0.2893\ngroup=Low Risk AML  54       25    39.97    5.6044   11.0121\ngroup=High Risk AML 45       34    21.18    7.7564   10.5286\n\n Chisq= 13.8  on 2 degrees of freedom, p= 0.001 \n\n\n\n5.10.5 Cumulative Hazard\n\\[\n\\begin{aligned}\nh(t)\n&\\stackrel{\\text{def}}{=}P(T=t|T\\ge t)\\\\\n&= \\frac{p(T=t)}{P(T\\ge t)}\\\\\n&= -\\frac{d}{dt}\\text{log}\\left\\{S(t)\\right\\}\n\\end{aligned}\n\\]\nThe cumulative hazard (or integrated hazard) function is\n\\[H(t)\\stackrel{\\text{def}}{=}\\int_0^t h(t) dt\\] Since \\(h(t) = -\\frac{d}{dt}\\text{log}\\left\\{S(t)\\right\\}\\) as shown above, we have:\n\\[\nH(t)=-\\text{log}\\left\\{S\\right\\}(t)\n\\]\nSo we can estimate \\(H(t)\\) as:\n\\[\n\\begin{aligned}\n\\hat H(t)\n&= -\\text{log}\\left\\{\\hat S(t)\\right\\}\\\\\n&= -\\text{log}\\left\\{\\prod_{t_i &lt; t}\\left[1-\\frac{d_i}{Y_i}\\right]\\right\\}\\\\\n&= -\\sum_{t_i &lt; t}\\text{log}\\left\\{1-\\frac{d_i}{Y_i}\\right\\}\\\\\n\\end{aligned}\n\\]\nThis is the Kaplan-Meier (product-limit) estimate of cumulative hazard.\nExample: Cumulative Hazard Curves for Bone-Marrow Transplant (bmt) data\n\nCodeautoplot(\n  fun = \"cumhaz\",\n  km_model1, \n  conf.int = FALSE,\n  ylab = \"Cumulative hazard (disease-free survival)\",\n  xlab = \"Time since transplant (days)\") + \n  theme_bw() +\n  theme(legend.position=\"bottom\")\n\n\nDisease-Free Cumulative Hazard by Disease Group"
  },
  {
    "objectID": "intro-to-survival-analysis.html#nelson-aalen-estimates-of-cumulative-hazard-and-survival",
    "href": "intro-to-survival-analysis.html#nelson-aalen-estimates-of-cumulative-hazard-and-survival",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.11 Nelson-Aalen Estimates of Cumulative Hazard and Survival",
    "text": "5.11 Nelson-Aalen Estimates of Cumulative Hazard and Survival\nThe point hazard at time \\(t_i\\) can be estimated by \\(d_i/Y_i\\), which leads to the Nelson-Aalen estimator of the cumulative hazard:\n\\[\\hat H_{NA}(t) \\stackrel{\\text{def}}{=}\\sum_{t_i &lt; t}\\frac{d_i}{Y_i}\\]\nThe variance of this estimator is approximately: \\[\n\\begin{aligned}\n\\hat{\\text{Var}}\\left(\\hat H_{NA} (t)\\right)\n&= \\sum_{t_i &lt;t}\\frac{(d_i/Y_i)(1-d_i/Y_i)}{Y_i}\\\\\n&\\approx \\sum_{t_i &lt;t}\\frac{d_i}{Y_i^2}\n\\end{aligned}\n\\]\nSince \\(S(t)=\\text{exp}\\left\\{-H(t)\\right\\}\\), the Nelson-Aalen cumulative hazard estimate can be converted into an alternate estimate of the survival function:\n\\[\n\\begin{aligned}\n\\hat S_{NA}(t)\n&= \\text{exp}\\left\\{-\\hat H_{NA}(t)\\right\\}\\\\\n&= \\text{exp}\\left\\{-\\sum_{t_i &lt; t}\\frac{d_i}{Y_i}\\right\\}\\\\\n&= \\prod_{t_i &lt; t}\\text{exp}\\left\\{-\\frac{d_i}{Y_i}\\right\\}\\\\\n\\end{aligned}\n\\]\nCompare these with the corresponding Kaplan-Meier estimates:\n\\[\n\\begin{aligned}\n\\hat H_{KM}(t) &= -\\sum_{t_i &lt; t}\\text{log}\\left\\{1-\\frac{d_i}{Y_i}\\right\\}\\\\\n\\hat S_{KM}(t) &= \\prod_{t_i &lt; t}\\left[1-\\frac{d_i}{Y_i}\\right]\n\\end{aligned}\n\\]\nThe product limit estimate and the Nelson-Aalen estimate often do not differ by much. The latter is considered more accurate in small samples and also directly estimates the cumulative hazard. The \"fleming-harrington\" method for survfit() reduces to Nelson-Aalen when the data are unweighted. We can also estimate the cumulative hazard as the negative log of the KM survival function estimate.\n\n5.11.1 Application to bmt dataset\n\nCodena_fit = survfit(\n  formula = surv ~ group,\n  type = \"fleming-harrington\",\n  data = bmt)\n\nkm_fit = survfit(\n  formula = surv ~ group,\n  type = \"kaplan-meier\",\n  data = bmt)\n\nkm_and_na = \n  bind_rows(\n    .id = \"model\",\n    \"Kaplan-Meier\" = km_fit |&gt; fortify(surv.connect = TRUE),\n    \"Nelson-Aalen\" = na_fit |&gt; fortify(surv.connect = TRUE)\n  ) |&gt; \n  as_tibble()\n\n\n\nCodekm_and_na |&gt; \n  ggplot(aes(x = time, y = surv, col = model)) +\n  geom_step() +\n  facet_grid(. ~ strata) +\n  theme_bw() + \n  ylab(\"S(t) = P(T&gt;=t)\") +\n  xlab(\"Survival time (t, days)\") +\n  theme(legend.position = \"bottom\")\n\n\nKaplan-Meier and Nelson-Aalen Survival Function Estimates, stratified by disease group\n\n\n\n\nThe Kaplan-Meier and Nelson-Aalen survival estimates are very similar for this dataset."
  },
  {
    "objectID": "proportional-hazards-models.html#the-proportional-hazards-model",
    "href": "proportional-hazards-models.html#the-proportional-hazards-model",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.1 The proportional hazards model",
    "text": "6.1 The proportional hazards model\n\n6.1.1 Background on the Proportional Hazards Model\nThe exponential distribution has constant hazard:\n\\[\n\\begin{aligned}\nf(t) &= \\lambda e^{-\\lambda t}\\\\\nS(t) &= e^{-\\lambda t}\\\\\nh(t) &= \\lambda\n\\end{aligned}\n\\]\nLet’s make two generalizations. First, we let the hazard depend on some covariates \\(x_1,x_2, \\dots, x_p\\); we will indicate this dependence by extending our notation for hazard:\n\\[h(t|\\boldsymbol x) \\stackrel{\\text{def}}{=}p(T=t|T\\ge t, \\boldsymbol X = \\boldsymbol x)\\]\nSecond, we let the base hazard depend on \\(t\\), but not on the covariates (for now). We can do this using either parametric or semi-parametric approaches.\n\n6.1.2 Cox’s Proportional Hazards Model\nThe generalization is that the hazard function is\n\\[\n\\begin{aligned}\nh(t|x)&= h_0(t)\\theta(x)\\\\\n\\theta(x) &= \\text{exp}\\left\\{\\eta(x)\\right\\}\\\\\n\\eta(x) &= x'\\beta\\\\\n&\\stackrel{\\text{def}}{=}\\beta_1x_1+\\cdots+\\beta_px_p\n\\end{aligned}\n\\]\nThe relationship between \\(h(t|x)\\) and \\(\\eta(x)\\) has a log link (that is, \\(\\text{log}\\left\\{h(t|x)\\right\\} = \\text{log}\\left\\{h_0(t)\\right\\} + \\eta(x)\\)), as in a generalized linear model.\nThis model is semi-parametric, because the linear predictor depends on estimated parameters but the base hazard function is unspecified. There is no constant term in \\(\\eta(x)\\), because it is absorbed in the base hazard.\nAlternatively, we could define \\(\\beta_0(t) = \\text{log}\\left\\{h_0(t)\\right\\}\\), and then \\(\\eta(x,t) = \\beta_0(t) + \\beta_1x_1+\\cdots+\\beta_px_p\\).\nFor two different individuals with covariate patterns \\(\\boldsymbol x_1\\) and \\(\\boldsymbol x_2\\), the ratio of the hazard functions (a.k.a. hazard ratio, a.k.a. relative hazard) is:\n\\[\n\\begin{aligned}\n\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\n&=\\frac{h_0(t)\\theta(\\boldsymbol x_1)}{h_0(t)\\theta(\\boldsymbol x_2)}\\\\\n&=\\frac{\\theta(\\boldsymbol x_1)}{\\theta(\\boldsymbol x_2)}\\\\\n\\end{aligned}\n\\]\nUnder the proportional hazards model, this ratio (a.k.a. proportion) does not depend on \\(t\\). This property is a structural limitation of the model; it is called the proportional hazards assumption.\n\nDefinition 6.1 (proportional hazards) A conditional probability distribution \\(p(T|X)\\) has proportional hazards if the hazard ratio \\(h(t|\\boldsymbol x_1)/h(t|\\boldsymbol x_2)\\) does not depend on \\(t\\). Mathematically, it can be written as:\n\\[\n\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\n= \\theta(\\boldsymbol x_1,\\boldsymbol x_2)\n\\]\n\nAs we saw above, Cox’s proportional hazards model has this property, with \\(\\theta(\\boldsymbol x_1,\\boldsymbol x_2) = \\frac{\\theta(\\boldsymbol x_1)}{\\theta(\\boldsymbol x_2)}\\).\n\n\n\n\n\n\nNote\n\n\n\nWe are using two similar notations, \\(\\theta(\\boldsymbol x_1,\\boldsymbol x_2)\\) and \\(\\theta(\\boldsymbol x)\\). We can link these notations if we define \\(\\theta(\\boldsymbol x) \\stackrel{\\text{def}}{=}\\theta(\\boldsymbol x, \\boldsymbol 0)\\) and \\(\\theta(\\boldsymbol 0) = 1\\).\n\n\nIt also has additional notable properties:\n\\[\n\\begin{aligned}\n\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\n&=\\frac{\\theta(\\boldsymbol x_1)}{\\theta(\\boldsymbol x_2)}\\\\\n&=\\frac{\\text{exp}\\left\\{\\eta(\\boldsymbol x_1)\\right\\}}{\\text{exp}\\left\\{\\eta(\\boldsymbol x_2)\\right\\}}\\\\\n&=\\text{exp}\\left\\{\\eta(\\boldsymbol x_1)-\\eta(\\boldsymbol x_2)\\right\\}\\\\\n&=\\text{exp}\\left\\{\\boldsymbol x_1'\\beta-\\boldsymbol x_2'\\beta\\right\\}\\\\\n&=\\text{exp}\\left\\{(\\boldsymbol x_1 - \\boldsymbol x_2)'\\beta\\right\\}\\\\\n\\end{aligned}\n\\]\nHence on the log scale, we have:\n\\[\n\\begin{aligned}\n\\text{log}\\left\\{\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\\right\\}\n&=\\eta(\\boldsymbol x_1)-\\eta(\\boldsymbol x_2)\\\\\n&= \\boldsymbol x_1'\\beta-\\boldsymbol x_2'\\beta\\\\\n&= (\\boldsymbol x_1 - \\boldsymbol x_2)'\\beta\n\\end{aligned}\n\\]\nIf only one covariate \\(x_j\\) is changing, then:\n\\[\n\\begin{aligned}\n\\text{log}\\left\\{\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\\right\\}\n&=  (x_{1j} - x_{2j}) \\cdot \\beta_j\\\\\n&\\propto (x_{1j} - x_{2j})\n\\end{aligned}\n\\]\nThat is, under Cox’s model \\(h(t|\\boldsymbol x) = h_0(t)\\text{exp}\\left\\{\\boldsymbol x'\\beta\\right\\}\\), the log of the hazard ratio is proportional to the difference in \\(x_j\\), with the proportionality coefficient equal to \\(\\beta_j\\).\nFurther,\n\\[\n\\begin{aligned}\n\\text{log}\\left\\{h(t|\\boldsymbol x)\\right\\}\n&=\\text{log}\\left\\{h_0(t)\\right\\}  + x'\\beta\n\\end{aligned}\n\\]\nThat is, the covariate effects are additive on the log-hazard scale.\nSee also:\nhttps://en.wikipedia.org/wiki/Proportional_hazards_model#Why_it_is_called_%22proportional%22\n\n6.1.3 Additional properties of the proportional hazards model\nIf \\(h(t|x)= h_0(t)\\theta(x)\\), then:\nCumulative hazards are also proportional to \\(H_0(t)\\)\n\n\\[\n\\begin{aligned}\nH(t|x)\n&\\stackrel{\\text{def}}{=}\\int_{u=0}^t h(u)du\\\\\n&= \\int_{u=0}^t h_0(u)\\theta(x)du\\\\\n&= \\theta(x)\\int_{u=0}^t h_0(u)du\\\\\n&= \\theta(x)H_0(t)\n\\end{aligned}\n\\]\nwhere \\(H_0(t) \\stackrel{\\text{def}}{=}H(t|0) = \\int_{u=0}^t h_0(u)du\\).\nSurvival functions are exponential multiples of \\(S_0(t)\\)\n\n\\[\n\\begin{aligned}\nS(t|x)\n&= \\text{exp}\\left\\{-H(t|x)\\right\\}\\\\\n&= \\text{exp}\\left\\{-\\theta(x)\\cdot H_0(t)\\right\\}\\\\\n&= \\left(\\text{exp}\\left\\{- H_0(t)\\right\\}\\right)^{\\theta(x)}\\\\\n&= \\left(S_0(t)\\right)^{\\theta(x)}\\\\\n\\end{aligned}\n\\]\nwhere \\(S_0(t) \\stackrel{\\text{def}}{=}P(T\\ge t | \\boldsymbol X = 0)\\) is the survival function for an individual whose covariates are all equal to their default values.\n\n6.1.4 Testing the proportional hazards assumption\nThe Nelson-Aalen estimate of the cumulative hazard is usually used for estimates of the hazard and often the cumulative hazard.\nIf the hazards of the three groups are proportional, that means that the ratio of the hazards is constant over \\(t\\). We can test this using the ratios of the estimated cumulative hazards, which also would be proportional, as shown above.\n\nCodelibrary(KMsurv)\nlibrary(survival)\ndata(bmt)\n\nbmt = \n  bmt |&gt; \n  as_tibble() |&gt; \n  mutate(\n    group = \n      group |&gt; \n      factor(\n        labels = c(\"ALL\",\"Low Risk AML\",\"High Risk AML\")))\n\nnafit = survfit(\n  formula = Surv(t2,d3) ~ group,\n  type = \"fleming-harrington\",\n  data = bmt)\n\nbmt_curves = tibble(timevec = 1:1000)\nsf1 &lt;- with(nafit[1], stepfun(time,c(1,surv)))\nsf2 &lt;- with(nafit[2], stepfun(time,c(1,surv)))\nsf3 &lt;- with(nafit[3], stepfun(time,c(1,surv)))\n\nbmt_curves = \n  bmt_curves |&gt; \n  mutate(\n    cumhaz1 = -log(sf1(timevec)),\n    cumhaz2 = -log(sf2(timevec)),\n    cumhaz3 = -log(sf3(timevec)))\n\n\n\nCodelibrary(ggplot2)\nbmt_rel_hazard_plot = \n  bmt_curves |&gt; \n  ggplot(\n    aes(\n      x = timevec,\n      y = cumhaz1/cumhaz2)\n  ) +\n  geom_line(aes(col = \"ALL/Low Risk AML\")) + \n  ylab(\"Hazard Ratio\") +\n  xlab(\"Time\") + \n  ylim(0,6) +\n  geom_line(aes(y = cumhaz3/cumhaz1, col = \"High Risk AML/ALL\")) +\n  geom_line(aes(y = cumhaz3/cumhaz2, col = \"High Risk AML/Low Risk AML\")) +\n  theme_bw() +\n  labs(colour = \"Comparison\") +\n  theme(legend.position=\"bottom\")\n\nprint(bmt_rel_hazard_plot)\n\n\nHazard Ratios by Disease Group\n\n\n\n\nWe can zoom in on 30-300 days to take a closer look:\n\nCodebmt_rel_hazard_plot + xlim(c(30,300))\n\n\nHazard Ratios by Disease Group (30-300 Days)\n\n\n\n\n\n6.1.5 Smoothed hazard functions\nThe Nelson-Aalen estimate of the cumulative hazard is usually used for estimates of the hazard. Since the hazard is the derivative of the cumulative hazard, we need a smooth estimate of the cumulative hazard, which is provided by smoothing the step-function cumulative hazard.\nThe R package muhaz handles this for us. What we are looking for is whether the hazard function is more or less the same shape, increasing, decreasing, constant, etc. Are the hazards “proportional”?\n\nCodeplot(\n  survfit(Surv(t2,d3)~group,data=bmt),\n  col=1:3,\n  lwd=2,\n  fun=\"cumhaz\",\n  mark.time = TRUE)\nlegend(\"bottomright\",c(\"ALL\",\"Low Risk AML\",\"High Risk AML\"),col=1:3,lwd=2)\n\n\nDisease-Free Cumulative Hazard by Disease Group\n\n\n\n\n\nCodelibrary(muhaz)\n\nmuhaz(bmt$t2,bmt$d3,bmt$group==\"High Risk AML\") |&gt; plot(lwd=2,col=3)\nmuhaz(bmt$t2,bmt$d3,bmt$group==\"ALL\") |&gt; lines(lwd=2,col=1)\nmuhaz(bmt$t2,bmt$d3,bmt$group==\"Low Risk AML\") |&gt; lines(lwd=2,col=2)\nlegend(\"topright\",c(\"ALL\",\"Low Risk AML\",\"High Risk AML\"),col=1:3,lwd=2)\n\n\nSmoothed Hazard Rate Estimates by Disease Group\n\n\n\n\nGroup 3 was plotted first because it has the highest hazard.\nWe will see that except for an initial blip in the high risk AML group, the hazards look roughly proportional . They are all strongly decreasing.\n\n6.1.6 Fitting the Proportional Hazards Model\nHow do we fit a proportional hazards regression model? We need to estimate the coefficients of the covariates, and we need to estimate the base hazard \\(h_0(t)\\). For the covariates, supposing for simplicity that there are no tied event times, let the event times for the whole data set be \\(t_1, t_2,\\ldots,t_D\\). Let the risk set at time \\(t_i\\) be \\(R(t_i)\\) and\n\\[\n\\begin{aligned}\n\\eta(\\boldsymbol{x}) &= \\beta_1x_{1}+\\cdots+\\beta_p x_{p}\\\\\n\\theta(\\boldsymbol{x}) &= e^{\\eta(\\boldsymbol{x})}\\\\\nh(t|X=x)&= h_0(t)e^{\\eta(\\boldsymbol{x})}=\\theta(\\boldsymbol{x}) h_0(t)\n\\end{aligned}\n\\]\nConditional on a single failure at time \\(t\\), the probability that the event is due to subject \\(f\\in R(t)\\) is approximately\n\\[\n\\begin{aligned}\n\\Pr(f \\text{ fails}|\\text{1 failure at } t)\n&= \\frac{h_0(t)e^{\\eta(\\boldsymbol{x}_f)}}{\\sum_{k \\in R(t)}h_0(t)e^{\\eta(\\boldsymbol{x}_f)}}\\\\\n&=\\frac{\\theta(\\boldsymbol{x}_f)}{\\sum_{k \\in R(t)} \\theta(\\boldsymbol{x}_k)}\n\\end{aligned}\n\\]\nThe logic behind this has several steps. We first fix (ex post) the failure times and note that in this discrete context, the probability \\(p_j\\) that a subject \\(j\\) in the risk set fails at time \\(t\\) is just the hazard of that subject at that time.\nIf all of the \\(p_j\\) are small, the chance that exactly one subject fails is\n\\[\n\\sum_{k\\in R(t)}p_k\\left[\\prod_{m\\in R(t), m\\ne k} (1-p_m)\\right]\\approx\\sum_{k\\in R(t)}p_k\n\\]\nIf subject \\(i\\) is the one who experiences the event of interest at time \\(t_i\\), then the partial likelihood is\n\\[\n\\mathcal L^*(\\beta|T)=\n\\prod_i \\frac{\\theta(x_i)}{\\sum_{k \\in R(t_i)} \\theta(\\boldsymbol{x}_k)}\n\\]\nand we can numerically maximize this with respect to the coefficients \\(\\boldsymbol{\\beta}\\) that specify \\(\\eta(\\boldsymbol{x}) = \\boldsymbol{x}'\\boldsymbol{\\beta}\\). When there are tied event times adjustments need to be made, but the likelihood is still similar. Note that we don’t need to know the base hazard to solve for the coefficients.\nOnce we have coefficient estimates \\(\\hat{\\boldsymbol{\\beta}} =(\\hat \\beta_1,\\ldots,\\hat\\beta_p)\\), this also defines \\(\\hat\\eta(x)\\) and \\(\\hat\\theta(x)\\) and then the estimated base cumulative hazard function is \\[\\hat H(t)=\n\\sum_{t_i &lt; t} \\frac{d_i}{\\sum_{k\\in R(t_i)} \\theta(x_k)}\\] which reduces to the Nelson-Aalen estimate when there are no covariates. There are numerous other estimates that have been proposed as well."
  },
  {
    "objectID": "proportional-hazards-models.html#cox-model-for-the-bmt-data",
    "href": "proportional-hazards-models.html#cox-model-for-the-bmt-data",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.2 Cox Model for the bmt data",
    "text": "6.2 Cox Model for the bmt data\n\n6.2.1 Fit the model\n\nCodebmt.cox &lt;- coxph(Surv(t2, d3) ~ group, data = bmt)\nsummary(bmt.cox)\n\nCall:\ncoxph(formula = Surv(t2, d3) ~ group, data = bmt)\n\n  n= 137, number of events= 83 \n\n                       coef exp(coef) se(coef)       z Pr(&gt;|z|)  \ngroupLow Risk AML  -0.57420   0.56316  0.28730 -1.9986  0.04565 *\ngroupHigh Risk AML  0.38341   1.46728  0.26738  1.4340  0.15158  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                   exp(coef) exp(-coef) lower .95 upper .95\ngroupLow Risk AML    0.56316    1.77570   0.32069   0.98896\ngroupHigh Risk AML   1.46728    0.68153   0.86881   2.47802\n\nConcordance= 0.625  (se = 0.03 )\nLikelihood ratio test= 13.45  on 2 df,   p=0.001\nWald test            = 13.03  on 2 df,   p=0.001\nScore (logrank) test = 13.81  on 2 df,   p=0.001\n\n\nThe table provides hypothesis tests comparing groups 2 and 3 to group 1. Group 3 has the highest hazard, so the most significant comparison is not directly shown.\nThe coefficient 0.3834 is on the log-hazard-ratio scale, as in log-risk-ratio. The next column gives the hazard ratio 1.4673, and a hypothesis (Wald) test.\nThe (not shown) group 3 vs. group 2 log hazard ratio is 0.3834 + 0.5742 = 0.9576. The hazard ratio is then exp(0.9576) or 2.605.\nInference on all coefficients and combinations can be constructed using coef(bmt.cox) and vcov(bmt.cox) as with logistic and poisson regression.\nConcordance is agreement of first failure between pairs of subjects and higher predicted risk between those subjects, omitting non-informative pairs.\nThe Rsquare value is Cox and Snell’s pseudo R-squared and is not very useful.\nsummary() prints three tests for whether the model with the group covariate is better than the one without\n\n\nLikelihood ratio test (chi-squared)\n\nWald test (also chi-squared), obtained by adding the squares of the z-scores\n\nScore = log-rank test, as with comparison of survival functions.\n\nThe likelihood ratio test is probably best in smaller samples, followed by the Wald test.\n\n6.2.2 Survival Curves from the Cox Model\nWe can take a look at the resulting group-specific curves:\n\nCode#| fig-cap: \"Survival Functions for Three Groups by KM and Cox Model\"\n\nkm_fit = survfit(Surv(t2, d3) ~ group, data = as.data.frame(bmt))\n\ncox_fit = survfit(\n  bmt.cox, \n  newdata = \n    data.frame(\n      group = unique(bmt$group), \n      row.names = unique(bmt$group)))\n\nlibrary(survminer)\n\nlist(KM = km_fit, Cox = cox_fit) |&gt; \n  survminer::ggsurvplot(\n    # facet.by = \"group\",\n    legend = \"bottom\", \n    legend.title = \"\",\n    combine = TRUE, \n    fun = 'pct', \n    size = .5,\n    ggtheme = theme_bw(), \n    conf.int = FALSE, \n    censor = FALSE) |&gt; \n  suppressWarnings() # ggsurvplot() throws some warnings that aren't too worrying\n\n\n\n\nWhen we use survfit() with a Cox model, we have to specify the covariate levels we are interested in; the argument newdata should include a data.frame with the same named columns as the predictors in the Cox model and one or more levels of each.\nOtherwise (that is, if the newdata argument is missing), a curve is produced for a single “pseudo” subject with covariate values equal to the means component of the fit.\nThe resulting curve(s) almost never make sense, but the default remains due to an unwarranted attachment to the option shown by some users and by other packages.\nTwo particularly egregious examples are factor variables and interactions. Suppose one were studying interspecies transmission of a virus, and the data set has a factor variable with levels (“pig”, “chicken”) and about equal numbers of observations for each. The “mean” covariate level will be 0.5 – is this a flying pig?\n\n6.2.3 Examining survfit\n\n\nCodesurvfit(Surv(t2, d3)~group,data=bmt)\n\nCall: survfit(formula = Surv(t2, d3) ~ group, data = bmt)\n\n                     n events median 0.95LCL 0.95UCL\ngroup=ALL           38     24    418     194      NA\ngroup=Low Risk AML  54     25   2204     704      NA\ngroup=High Risk AML 45     34    183     115     456\n\n\n\nCodesurvfit(Surv(t2, d3)~group,data=bmt) |&gt; summary()\n\nCall: survfit(formula = Surv(t2, d3) ~ group, data = bmt)\n\n                group=ALL \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1     38       1   0.9737 0.02597       0.9241       1.0000\n   55     37       1   0.9474 0.03622       0.8790       1.0000\n   74     36       1   0.9211 0.04374       0.8392       1.0000\n   86     35       1   0.8947 0.04978       0.8023       0.9978\n  104     34       1   0.8684 0.05484       0.7673       0.9828\n  107     33       1   0.8421 0.05915       0.7338       0.9664\n  109     32       1   0.8158 0.06289       0.7014       0.9488\n  110     31       1   0.7895 0.06613       0.6699       0.9303\n  122     30       2   0.7368 0.07143       0.6093       0.8910\n  129     28       1   0.7105 0.07357       0.5800       0.8704\n  172     27       1   0.6842 0.07541       0.5513       0.8492\n  192     26       1   0.6579 0.07696       0.5231       0.8274\n  194     25       1   0.6316 0.07825       0.4954       0.8052\n  230     23       1   0.6041 0.07952       0.4667       0.7819\n  276     22       1   0.5767 0.08051       0.4386       0.7582\n  332     21       1   0.5492 0.08122       0.4110       0.7339\n  383     20       1   0.5217 0.08167       0.3839       0.7091\n  418     19       1   0.4943 0.08186       0.3573       0.6838\n  466     18       1   0.4668 0.08179       0.3311       0.6581\n  487     17       1   0.4394 0.08146       0.3055       0.6319\n  526     16       1   0.4119 0.08086       0.2803       0.6052\n  609     14       1   0.3825 0.08026       0.2535       0.5771\n  662     13       1   0.3531 0.07930       0.2273       0.5483\n\n                group=Low Risk AML \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n   10     54       1   0.9815 0.01835       0.9462       1.0000\n   35     53       1   0.9630 0.02570       0.9139       1.0000\n   48     52       1   0.9444 0.03117       0.8853       1.0000\n   53     51       1   0.9259 0.03564       0.8586       0.9985\n   79     50       1   0.9074 0.03945       0.8333       0.9881\n   80     49       1   0.8889 0.04277       0.8089       0.9768\n  105     48       1   0.8704 0.04571       0.7852       0.9647\n  211     47       1   0.8519 0.04834       0.7622       0.9521\n  219     46       1   0.8333 0.05072       0.7396       0.9389\n  248     45       1   0.8148 0.05286       0.7175       0.9253\n  272     44       1   0.7963 0.05481       0.6958       0.9113\n  288     43       1   0.7778 0.05658       0.6744       0.8970\n  381     42       1   0.7593 0.05818       0.6534       0.8823\n  390     41       1   0.7407 0.05964       0.6326       0.8674\n  414     40       1   0.7222 0.06095       0.6121       0.8521\n  421     39       1   0.7037 0.06214       0.5919       0.8367\n  481     38       1   0.6852 0.06320       0.5719       0.8210\n  486     37       1   0.6667 0.06415       0.5521       0.8050\n  606     36       1   0.6481 0.06499       0.5325       0.7889\n  641     35       1   0.6296 0.06571       0.5132       0.7725\n  704     34       1   0.6111 0.06634       0.4940       0.7560\n  748     33       1   0.5926 0.06686       0.4750       0.7393\n 1063     26       1   0.5698 0.06807       0.4509       0.7201\n 1074     25       1   0.5470 0.06905       0.4271       0.7006\n 2204      6       1   0.4558 0.10118       0.2950       0.7043\n\n                group=High Risk AML \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    2     45       1   0.9778 0.02197       0.9356       1.0000\n   16     44       1   0.9556 0.03072       0.8972       1.0000\n   32     43       1   0.9333 0.03718       0.8632       1.0000\n   47     42       2   0.8889 0.04685       0.8017       0.9856\n   48     40       1   0.8667 0.05067       0.7728       0.9719\n   63     39       1   0.8444 0.05403       0.7449       0.9573\n   64     38       1   0.8222 0.05699       0.7178       0.9419\n   74     37       1   0.8000 0.05963       0.6913       0.9258\n   76     36       1   0.7778 0.06197       0.6653       0.9092\n   80     35       1   0.7556 0.06406       0.6399       0.8922\n   84     34       1   0.7333 0.06592       0.6149       0.8746\n   93     33       1   0.7111 0.06757       0.5903       0.8567\n  100     32       1   0.6889 0.06901       0.5661       0.8383\n  105     31       1   0.6667 0.07027       0.5422       0.8197\n  113     30       1   0.6444 0.07136       0.5187       0.8006\n  115     29       1   0.6222 0.07227       0.4955       0.7813\n  120     28       1   0.6000 0.07303       0.4727       0.7617\n  157     27       1   0.5778 0.07363       0.4501       0.7417\n  162     26       1   0.5556 0.07407       0.4278       0.7215\n  164     25       1   0.5333 0.07437       0.4058       0.7010\n  168     24       1   0.5111 0.07452       0.3841       0.6802\n  183     23       1   0.4889 0.07452       0.3626       0.6591\n  242     22       1   0.4667 0.07437       0.3415       0.6378\n  268     21       1   0.4444 0.07407       0.3206       0.6161\n  273     20       1   0.4222 0.07363       0.3000       0.5943\n  318     19       1   0.4000 0.07303       0.2797       0.5721\n  363     18       1   0.3778 0.07227       0.2597       0.5496\n  390     17       1   0.3556 0.07136       0.2399       0.5269\n  422     16       1   0.3333 0.07027       0.2205       0.5039\n  456     15       1   0.3111 0.06901       0.2014       0.4805\n  467     14       1   0.2889 0.06757       0.1827       0.4569\n  625     13       1   0.2667 0.06592       0.1643       0.4329\n  677     12       1   0.2444 0.06406       0.1462       0.4086\n\n\n\nCodesurvfit(bmt.cox)\n\nCall: survfit(formula = bmt.cox)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 137     83    422     268      NA\n\nCodesurvfit(bmt.cox, newdata = tibble(group = unique(bmt$group)))\n\nCall: survfit(formula = bmt.cox, newdata = tibble(group = unique(bmt$group)))\n\n    n events median 0.95LCL 0.95UCL\n1 137     83    422     268      NA\n2 137     83     NA     625      NA\n3 137     83    268     162     467\n\n\n\nCodebmt.cox |&gt; \n  survfit(newdata = tibble(group = unique(bmt$group))) |&gt; \n  summary()\n\nCall: survfit(formula = bmt.cox, newdata = tibble(group = unique(bmt$group)))\n\n time n.risk n.event survival1 survival2 survival3\n    1    137       1    0.9926    0.9958    0.9891\n    2    136       1    0.9852    0.9916    0.9783\n   10    135       1    0.9777    0.9874    0.9675\n   16    134       1    0.9703    0.9832    0.9568\n   32    133       1    0.9629    0.9789    0.9460\n   35    132       1    0.9554    0.9746    0.9353\n   47    131       2    0.9405    0.9661    0.9139\n   48    129       2    0.9255    0.9574    0.8927\n   53    127       1    0.9180    0.9530    0.8821\n   55    126       1    0.9106    0.9486    0.8715\n   63    125       1    0.9031    0.9442    0.8611\n   64    124       1    0.8956    0.9398    0.8506\n   74    123       2    0.8805    0.9308    0.8297\n   76    121       1    0.8730    0.9263    0.8193\n   79    120       1    0.8654    0.9218    0.8088\n   80    119       2    0.8502    0.9127    0.7882\n   84    117       1    0.8427    0.9081    0.7779\n   86    116       1    0.8351    0.9035    0.7676\n   93    115       1    0.8275    0.8988    0.7574\n  100    114       1    0.8199    0.8942    0.7472\n  104    113       1    0.8122    0.8895    0.7370\n  105    112       2    0.7969    0.8800    0.7167\n  107    110       1    0.7892    0.8752    0.7066\n  109    109       1    0.7815    0.8704    0.6965\n  110    108       1    0.7739    0.8656    0.6865\n  113    107       1    0.7662    0.8607    0.6766\n  115    106       1    0.7585    0.8559    0.6666\n  120    105       1    0.7508    0.8509    0.6567\n  122    104       2    0.7352    0.8410    0.6368\n  129    102       1    0.7275    0.8359    0.6270\n  157    101       1    0.7197    0.8309    0.6172\n  162    100       1    0.7119    0.8258    0.6074\n  164     99       1    0.7040    0.8207    0.5975\n  168     98       1    0.6961    0.8155    0.5877\n  172     97       1    0.6882    0.8102    0.5779\n  183     96       1    0.6803    0.8050    0.5682\n  192     95       1    0.6723    0.7996    0.5584\n  194     94       1    0.6643    0.7943    0.5487\n  211     93       1    0.6563    0.7889    0.5391\n  219     92       1    0.6484    0.7835    0.5295\n  230     90       1    0.6404    0.7780    0.5200\n  242     89       1    0.6324    0.7726    0.5105\n  248     88       1    0.6244    0.7670    0.5010\n  268     87       1    0.6164    0.7615    0.4916\n  272     86       1    0.6083    0.7558    0.4822\n  273     85       1    0.6003    0.7502    0.4730\n  276     84       1    0.5923    0.7446    0.4637\n  288     83       1    0.5842    0.7388    0.4545\n  318     82       1    0.5762    0.7331    0.4454\n  332     81       1    0.5682    0.7273    0.4363\n  363     80       1    0.5601    0.7215    0.4272\n  381     79       1    0.5520    0.7156    0.4182\n  383     78       1    0.5440    0.7097    0.4093\n  390     77       2    0.5279    0.6978    0.3916\n  414     75       1    0.5198    0.6918    0.3829\n  418     74       1    0.5118    0.6858    0.3742\n  421     73       1    0.5038    0.6797    0.3657\n  422     72       1    0.4958    0.6736    0.3573\n  456     71       1    0.4878    0.6675    0.3488\n  466     70       1    0.4798    0.6612    0.3404\n  467     69       1    0.4717    0.6550    0.3320\n  481     68       1    0.4636    0.6486    0.3236\n  486     67       1    0.4555    0.6422    0.3154\n  487     66       1    0.4475    0.6358    0.3073\n  526     65       1    0.4395    0.6294    0.2993\n  606     63       1    0.4313    0.6228    0.2911\n  609     62       1    0.4232    0.6161    0.2832\n  625     61       1    0.4151    0.6095    0.2753\n  641     60       1    0.4069    0.6027    0.2673\n  662     59       1    0.3988    0.5959    0.2596\n  677     58       1    0.3907    0.5891    0.2519\n  704     57       1    0.3826    0.5821    0.2442\n  748     56       1    0.3745    0.5751    0.2366\n 1063     47       1    0.3653    0.5671    0.2282\n 1074     46       1    0.3562    0.5592    0.2199\n 2204      9       1    0.3133    0.5201    0.1821"
  },
  {
    "objectID": "proportional-hazards-models.html#adjustment-for-ties-optional",
    "href": "proportional-hazards-models.html#adjustment-for-ties-optional",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.3 Adjustment for Ties (optional)",
    "text": "6.3 Adjustment for Ties (optional)\n\n6.3.1 \nAt each time \\(t_i\\) at which more than one of the subjects has an event, let \\(d_i\\) be the number of events at that time, \\(D_i\\) the set of subjects with events at that time, and let \\(s_i\\) be a covariate vector for an artificial subject obtained by adding up the covariate values for the subjects with an event at time \\(t_i\\). Let \\[\\bar\\eta_i = \\beta_1s_{i1}+\\cdots+\\beta_ps_{ip}\\] and \\(\\bar\\theta_i = \\text{exp}\\left\\{\\bar\\eta_i\\right\\}\\).\nLet \\(s_i\\) be a covariate vector for an artificial subject obtained by adding up the covariate values for the subjects with an event at time \\(t_i\\). Note that\n\\[\n\\begin{aligned}\n\\bar\\eta_i &=\\sum_{j \\in D_i}\\beta_1x_{j1}+\\cdots+\\beta_px_{jp}\\\\\n&= \\beta_1s_{i1}+\\cdots+\\beta_ps_{ip}\\\\\n\\bar\\theta_i &= \\text{exp}\\left\\{\\bar\\eta_i\\right\\}\\\\\n&= \\prod_{j \\in D_i}\\theta_i\n\\end{aligned}\n\\]\nBreslow’s method for ties\nBreslow’s method estimates the partial likelihood as\n\\[\n\\begin{aligned}\nL(\\beta|T) &=\n\\prod_i \\frac{\\bar\\theta_i}{[\\sum_{k \\in R(t_i)} \\theta_k]^{d_i}}\\\\\n&= \\prod_i \\prod_{j \\in D_i}\\frac{\\theta_j}{\\sum_{k \\in R(t_i)} \\theta_k}\n\\end{aligned}\n\\]\nThis method is equivalent to treating each event as distinct and using the non-ties formula. It works best when the number of ties is small. It is the default in many statistical packages, including PROC PHREG in SAS.\nEfron’s method for ties\nThe other common method is Efron’s, which is the default in R.\n\\[L(\\beta|T)=\n\\prod_i \\frac{\\bar\\theta_i}{\\prod_{j=1}^{d_i}[\\sum_{k \\in R(t_i)} \\theta_k-\\frac{j-1}{d_i}\\sum_{k \\in D_i} \\theta_k]}\\] This is closer to the exact discrete partial likelihood when there are many ties.\nThe third option in R (and an option also in SAS as discrete) is the “exact” method, which is the same one used for matched logistic regression.\nExample: Breslow’s method\nSuppose as an example we have a time \\(t\\) where there are 20 individuals at risk and three failures. Let the three individuals have risk parameters \\(\\theta_1, \\theta_2, \\theta_3\\) and let the sum of the risk parameters of the remaining 17 individuals be \\(\\theta_R\\). Then the factor in the partial likelihood at time \\(t\\) using Breslow’s method is\n\n\\[\n\\left(\\frac{\\theta_1}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_2}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_3}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\]\n\nIf on the other hand, they had died in the order 1,2, 3, then the contribution to the partial likelihood would be:\n\n\\[\n\\left(\\frac{\\theta_1}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_2}{\\theta_R+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_3}{\\theta_R+\\theta_3}\\right)\n\\]\n\nas the risk set got smaller with each failure. The exact method roughly averages the results for the six possible orderings of the failures.\nExample: Efron’s method\nBut we don’t know the order they failed in, so instead of reducing the denominator by one risk coefficient each time, we reduce it by the same fraction. This is Efron’s method.\n\n\\[\\left(\\frac{\\theta_1}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_2}{\\theta_R+2(\\theta_1+\\theta_2+\\theta_3)/3}\\right)\n\\left(\\frac{\\theta_3}{\\theta_R+(\\theta_1+\\theta_2+\\theta_3)/3}\\right)\\]"
  },
  {
    "objectID": "coxph-model-building.html#building-cox-proportional-hazards-models",
    "href": "coxph-model-building.html#building-cox-proportional-hazards-models",
    "title": "\n7  Building Cox Proportional Hazards models\n",
    "section": "\n7.1 Building Cox Proportional Hazards models",
    "text": "7.1 Building Cox Proportional Hazards models\n\n7.1.1 hodg Lymphoma Data Set from KMsurv\n\nParticipants\n43 bone marrow transplant patients at Ohio State University (Avalos 1993)\nVariables\n\n\ndtype: Disease type (Hodgkin’s or non-Hodgkins lymphoma)\n\ngtype: Bone marrow graft type:\nallogeneic: from HLA-matched sibling\nautologous: from self (prior to chemo)\n\ntime: time to study exit\n\ndelta: study exit reason (death/relapse vs censored)\n\nwtime: waiting time to transplant (in months)\n\nscore: Karnofsky score:\n80–100: Able to carry on normal activity and to work; no special care needed.\n50–70: Unable to work; able to live at home and care for most personal needs; varying amount of assistance needed.\n10–60: Unable to care for self; requires equivalent of institutional or hospital care; disease may be progressing rapidly.\nData\n\nCodedata(hodg, package = \"KMsurv\")\nhodg2 = hodg |&gt; \n  as_tibble() |&gt; \n  mutate(\n    # We add factor labels to the categorical variables:\n    gtype = gtype |&gt; \n      case_match(\n        1 ~ \"Allogenic\",\n        2 ~ \"Autologous\"),\n    dtype = dtype |&gt; \n      case_match(\n        1 ~ \"Non-Hodgkins\",\n        2 ~ \"Hodgkins\") |&gt; \n      factor() |&gt; \n      relevel(ref = \"Non-Hodgkins\"), \n    delta = delta |&gt; \n      case_match(\n        1 ~ \"dead\",\n        0 ~ \"alive\"),\n    surv = Surv(\n      time = time, \n      event = delta == \"dead\")\n  )\nhodg2 |&gt; print()\n\n# A tibble: 43 × 7\n   gtype     dtype         time delta score wtime   surv\n   &lt;chr&gt;     &lt;fct&gt;        &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;Surv&gt;\n 1 Allogenic Non-Hodgkins    28 dead     90    24    28 \n 2 Allogenic Non-Hodgkins    32 dead     30     7    32 \n 3 Allogenic Non-Hodgkins    49 dead     40     8    49 \n 4 Allogenic Non-Hodgkins    84 dead     60    10    84 \n 5 Allogenic Non-Hodgkins   357 dead     70    42   357 \n 6 Allogenic Non-Hodgkins   933 alive    90     9   933+\n 7 Allogenic Non-Hodgkins  1078 alive   100    16  1078+\n 8 Allogenic Non-Hodgkins  1183 alive    90    16  1183+\n 9 Allogenic Non-Hodgkins  1560 alive    80    20  1560+\n10 Allogenic Non-Hodgkins  2114 alive    80    27  2114+\n# ℹ 33 more rows\n\n\n\n7.1.2 Proportional hazards model\n\nCodehodg.cox1 = coxph(\n  formula = surv ~ gtype * dtype + score + wtime, \n  data = hodg2)\n\nsummary(hodg.cox1)\n\nCall:\ncoxph(formula = surv ~ gtype * dtype + score + wtime, data = hodg2)\n\n  n= 43, number of events= 26 \n\n                                 coef exp(coef) se(coef)     z Pr(&gt;|z|)    \ngtypeAutologous                0.6394    1.8953   0.5937  1.08   0.2815    \ndtypeHodgkins                  2.7603   15.8050   0.9474  2.91   0.0036 ** \nscore                         -0.0495    0.9517   0.0124 -3.98  6.8e-05 ***\nwtime                         -0.0166    0.9836   0.0102 -1.62   0.1046    \ngtypeAutologous:dtypeHodgkins -2.3709    0.0934   1.0355 -2.29   0.0220 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                              exp(coef) exp(-coef) lower .95 upper .95\ngtypeAutologous                  1.8953     0.5276    0.5920     6.068\ndtypeHodgkins                   15.8050     0.0633    2.4682   101.207\nscore                            0.9517     1.0507    0.9288     0.975\nwtime                            0.9836     1.0167    0.9641     1.003\ngtypeAutologous:dtypeHodgkins    0.0934    10.7074    0.0123     0.711\n\nConcordance= 0.776  (se = 0.059 )\nLikelihood ratio test= 32.1  on 5 df,   p=6e-06\nWald test            = 27.2  on 5 df,   p=5e-05\nScore (logrank) test = 37.7  on 5 df,   p=4e-07"
  },
  {
    "objectID": "coxph-model-building.html#diagnostic-graphs-for-proportional-hazards-assumption",
    "href": "coxph-model-building.html#diagnostic-graphs-for-proportional-hazards-assumption",
    "title": "\n7  Building Cox Proportional Hazards models\n",
    "section": "\n7.2 Diagnostic graphs for proportional hazards assumption",
    "text": "7.2 Diagnostic graphs for proportional hazards assumption\n\n7.2.1 Analysis plan\n\n\nsurvival function for the four combinations of disease type and graft type.\n\nobserved (nonparametric) vs. expected (semiparametric) survival functions.\n\ncomplementary log-log survival for the four groups.\n\n7.2.2 Kaplan-Meier survival functions\n\nCodekm_model = survfit(\n  formula = surv ~ dtype + gtype,\n  data = hodg2)\n\nkm_model |&gt; \n  autoplot(conf.int = FALSE) +\n  theme_bw() +\n  theme(\n    legend.position=\"bottom\",\n    legend.title = element_blank(),\n    legend.text = element_text(size = legend_text_size)\n  ) +\n  guides(col=guide_legend(ncol=2)) +\n  ylab('Survival probability, S(t)') +\n  xlab(\"Time since transplant (days)\")\n\n\nKaplan-Meier Survival Curves for HOD/NHL and Allo/Auto Grafts\n\n\n\n\n\n7.2.3 Observed and expected survival curves\n\nCode# we need to create a tibble of covariate patterns;\n# we will set score and wtime to mean values for disease and graft types:\nmeans = hodg2 |&gt; \n  summarize(\n    .by = c(dtype, gtype), \n    score = mean(score), \n    wtime = mean(wtime)) |&gt; \n  arrange(dtype, gtype) |&gt; \n  mutate(strata = paste(dtype, gtype, sep = \",\")) |&gt; \n  as.data.frame() \n\n# survfit.coxph() will use the rownames of its `newdata`\n# argument to label its output:\nrownames(means) = means$strata\n\ncox_model = \n  hodg.cox1 |&gt; \n  survfit(\n    data = hodg2, # ggsurvplot() will need this\n    newdata = means)\n\n\n\nCode# I couldn't find a good function to reformat `cox_model` for ggplot, \n# so I made my own:\nstack_surv_ph = function(cox_model)\n{\n  cox_model$surv |&gt; \n    as_tibble() |&gt; \n    mutate(time = cox_model$time) |&gt; \n    pivot_longer(\n      cols = -time,\n      names_to = \"strata\",\n      values_to = \"surv\") |&gt; \n    mutate(\n      cumhaz = -log(surv),\n      model = \"Cox PH\")\n}\n\nkm_and_cph =\n  km_model |&gt; \n  fortify(surv.connect = TRUE) |&gt; \n  mutate(\n    strata = trimws(strata),\n    model = \"Kaplan-Meier\",\n    cumhaz = -log(surv)) |&gt;\n  bind_rows(stack_surv_ph(cox_model))\n\n\n\nCodekm_and_cph |&gt; \n  ggplot(aes(x = time, y = surv, col = model)) +\n  geom_step() +\n  facet_wrap(~strata) +\n  theme_bw() + \n  ylab(\"S(t) = P(T&gt;=t)\") +\n  xlab(\"Survival time (t, days)\") +\n  theme(legend.position = \"bottom\")\n\n\nObserved and expected survival curves for bmt data\n\n\n\n\n\n7.2.4 Cumulative hazard (log-scale) curves\nAlso known as “complementary log-log (clog-log) survival curves”.\n\nCodena_model = survfit(\n  formula = surv ~ dtype + gtype,\n  data = hodg2,\n  type = \"fleming\")\n\nna_model |&gt; survminer::ggsurvplot(\n  legend = \"bottom\", \n  legend.title = \"\",\n  ylab = \"log(Cumulative Hazard)\",\n  xlab = \"Time since transplant (days, log-scale)\",\n  fun = 'cloglog', \n  size = .5,\n  ggtheme = theme_bw(),\n  conf.int = FALSE, \n  censor = TRUE) +\n  guides(\n    col = \n      guide_legend(\n        ncol = 2,\n        label.theme = \n          element_text(\n            size = legend_text_size)))\n\n\nComplementary log-log survival curves - Nelson-Aalen estimates\n\n\n\n\nLet’s compare these empirical (i.e., non-parametric) curves with the fitted curves from our coxph() model:\n\nCodecox_model |&gt; \n  survminer::ggsurvplot(\n    facet_by = \"\",\n    legend = \"bottom\", \n    legend.title = \"\",\n    ylab = \"log(Cumulative Hazard)\",\n    xlab = \"Time since transplant (days, log-scale)\",\n    fun = 'cloglog', \n    size = .5,\n    ggtheme = theme_bw(),\n    censor = FALSE, # doesn't make sense for cox model\n    conf.int = FALSE) + \n  guides(\n    col = \n      guide_legend(\n        ncol = 2,\n        label.theme = \n          element_text(\n            size = legend_text_size)))\n\n\nComplementary log-log survival curves - PH estimates\n\n\n\n\nNow let’s overlay these cumulative hazard curves:\n\nCodena_and_cph = \n  na_model |&gt; \n  fortify(fun = \"cumhaz\") |&gt; \n  # `fortify.survfit()` doesn't name cumhaz correctly:\n  rename(cumhaz = surv) |&gt;  \n  mutate(\n    surv = exp(-cumhaz),\n    strata = trimws(strata)) |&gt; \n  mutate(model = \"Nelson-Aalen\") |&gt; \n  bind_rows(stack_surv_ph(cox_model))\n\nna_and_cph |&gt; \n  ggplot(\n    aes(\n      x = time, \n      y = cumhaz, \n      col = model)) +\n  geom_step() +\n  facet_wrap(~strata) +\n  theme_bw() + \n  scale_y_continuous(\n    trans = \"log10\",\n    name = \"Cumulative hazard H(t) (log-scale)\") +\n  scale_x_continuous(\n    trans = \"log10\",\n    name = \"Survival time (t, days, log-scale)\") +\n  theme(legend.position = \"bottom\")\n\n\nObserved and expected cumulative hazard curves for bmt data (cloglog format)"
  },
  {
    "objectID": "coxph-model-building.html#predictions-and-residuals",
    "href": "coxph-model-building.html#predictions-and-residuals",
    "title": "\n7  Building Cox Proportional Hazards models\n",
    "section": "\n7.3 Predictions and Residuals",
    "text": "7.3 Predictions and Residuals\n\n7.3.1 Review: Predictions in Linear Regression\n\nIn linear regression, we have a linear predictor for each data point \\(i\\)\n\n\n\\[\n\\begin{aligned}\n\\eta_i &= \\beta_0+\\beta_1x_{1i}+\\cdots+\\beta_px_{pi}\\\\\n\\hat y_i &=\\hat\\eta_i = \\hat\\beta_0+\\hat\\beta_1x_{1i}+\\cdots+\\hat\\beta_px_{pi}\\\\\ny_i &\\sim N(\\eta_i,\\sigma^2)\n\\end{aligned}\n\\]\n\n\n\\(\\hat y_i\\) estimates the conditional mean of \\(y_i\\) given the covariate values \\(\\tilde x_i\\). This together with the prediction error says that we are predicting the distribution of values of \\(y\\).\n\n7.3.2 Review: Residuals in Linear Regression\n\nThe usual residual is \\(r_i=y_i-\\hat y_i\\), the difference between the actual value of \\(y\\) and a prediction of its mean.\nThe residuals are also the quantities the sum of whose squares is being minimized by the least squares/MLE estimation.\n\n7.3.3 Predictions and Residuals in survival models\n\nIn survival analysis, the equivalent of \\(y_i\\) is the event time \\(t_i\\), which is unknown for the censored observations.\nThe expected event time can be tricky to calculate:\n\n\\[\n\\hat{\\text{E}}[T|X=x] = \\int_{t=0}^{\\infty} \\hat S(t)dt\n\\]\n\n7.3.4 Wide prediction intervals\nThe nature of time-to-event data results in very wide prediction intervals:\n\nSuppose a cancer patient is predicted to have a mean lifetime of 5 years after diagnosis and suppose the distribution is exponential.\nIf we want a 95% interval for survival, the lower end is at the 0.025 percentage point of the exponential which is qexp(.025, rate = 1/5) = 0.1266 years, or 1/40 of the mean lifetime.\nThe upper end is at the 0.975 point which is qexp(.975, rate = 1/5) = 18.4444 years, or 3.7 times the mean lifetime.\nSaying that the survival time is somewhere between 6 weeks and 18 years does not seem very useful, but it may be the best we can do.\nFor survival analysis, something is like a residual if it is small when the model is accurate or if the accumulation of them is in some way minimized by the estimation algorithm, but there is no exact equivalence to linear regression residuals.\nAnd if there is, they are mostly quite large!\n\n7.3.5 Types of Residuals in Time-to-Event Models\n\nIt is often hard to make a decision from graph appearances, though the process can reveal much.\nSome diagnostic tests are based on residuals as with other regression methods:\n\nSchoenfeld residuals (via cox.zph) for proportionality.\n\nCox-Snell residuals for goodness of fit.\n\nmartingale residuals for non-linearity.\n\ndfbeta for influence.\n\n7.3.6 Schoenfeld residuals\n\nThere is a Schoenfeld residual for each subject \\(i\\) with an event (not censored) and for each predictor \\(x_{k}\\).\nAt the event time \\(t\\) for that subject, there is a risk set \\(R\\), and each subject \\(j\\) in the risk set has a risk coefficient \\(\\theta_j\\) and also a value \\(x_{jk}\\) of the predictor.\nThe Schoenfeld residual is the difference between \\(x_{ik}\\) and the risk-weighted average of all the \\(x_{jk}\\) over the risk set.\n\n\\[\nr^S_{ik} =\nx_{ik}-\\frac{\\sum_{k\\in R}x_{jk}\\theta_k}{\\sum_{k\\in R}\\theta_k}\n\\]\nThis residual measures how typical the individual subject is with respect to the covariate at the time of the event. Since subjects should fail more or less uniformly according to risk, the Schoenfeld residuals should be approximately level over time, not increasing or decreasing.\nWe can test this with the correlation with time on some scale, which could be the time itself, the log time, or the rank in the set of failure times.\nThe default is to use the KM curve as a transform, which is similar to the rank but deals better with censoring.\nThe cox.zph() function implements a score test proposed in Grambsch and Therneau (1994).\n\nCodehodg.zph = cox.zph(hodg.cox1)\nprint(hodg.zph)\n\n              chisq df     p\ngtype        0.5400  1 0.462\ndtype        1.8012  1 0.180\nscore        3.8805  1 0.049\nwtime        0.0173  1 0.895\ngtype:dtype  4.0474  1 0.044\nGLOBAL      13.7573  5 0.017\n\n\ngtype\n\nCodeggcoxzph(hodg.zph, var = \"gtype\")\n\n\n\n\ndtype\n\nCodeggcoxzph(hodg.zph, var = \"dtype\")\n\n\n\n\nscore\n\nCodeggcoxzph(hodg.zph, var = \"score\")\n\n\n\n\nwtime\n\nCodeggcoxzph(hodg.zph, var = \"wtime\")\n\n\n\n\ngtype:dtype\n\nCodeggcoxzph(hodg.zph, var = \"gtype:dtype\")\n\n\n\n\nConclusions\n\nFrom the correlation test, the Karnofsky score and the interaction with graft type disease type induce modest but statistically significant non-proportionality.\nThe sample size here is relatively small (26 events in 43 subjects). If the sample size is large, very small amounts of non-proportionality can induce a significant result.\nAs time goes on, autologous grafts are over-represented at their own event times, but those from HOD patients become less represented.\nBoth the statistical tests and the plots are useful."
  },
  {
    "objectID": "coxph-model-building.html#goodness-of-fit-using-the-cox-snell-residuals",
    "href": "coxph-model-building.html#goodness-of-fit-using-the-cox-snell-residuals",
    "title": "\n7  Building Cox Proportional Hazards models\n",
    "section": "\n7.4 Goodness of Fit using the Cox-Snell Residuals",
    "text": "7.4 Goodness of Fit using the Cox-Snell Residuals\n(references: Klein & Moeschberger textbook, §11.2, and Dobson & Barnett textbook, §10.6)\nSuppose that an individual has a survival time \\(T\\) which has survival function \\(S(t)\\), meaning that \\(\\Pr(T&gt; t) = S(t)\\). Then \\(S(T)\\) has a uniform distribution on \\((0,1)\\).\n\\[\n\\begin{aligned}\n\\Pr(S(T_i) \\le u)\n&= \\Pr(T_i &gt; S_i^{-1}(u))\\\\\n&= S_i(S_i^{-1}(u))\\\\\n&= u\n\\end{aligned}\n\\]\nAlso, if \\(U\\) has a uniform distribution on \\((0,1)\\), then what is the distribution of \\(-\\ln(U)\\)?\n\\[\n\\begin{aligned}\n\\Pr(-\\ln(U) &lt; x) &= \\Pr(U&gt;\\text{exp}\\left\\{-x\\right\\})\\\\\n&= 1-e^{-x}\n\\end{aligned}\n\\]\nwhich is the CDF of an exponential distribution with parameter \\(\\lambda=1\\).\nSo,\n\\[\n\\begin{aligned}\nr^{CS}_i&\n\\stackrel{\\text{def}}{=}-\\ln[\\hat S(t_i|x_i)]\n= \\hat H(t_i|\\tilde x_i)\n\\end{aligned}\n\\]\nshould have an exponential distribution with constant hazard \\(\\lambda=1\\) if the estimate \\(\\hat S_i\\) is accurate, which means that these values should look like a censored sample from this exponential distribution. These values are called generalized residuals or Cox-Snell residuals.\n\nCodehodg2 = hodg2 |&gt; \n  mutate(cs = predict(hodg.cox1, type = \"expected\"))\n\nsurv.csr = survfit(\n  data = hodg2,\n  formula = Surv(time = cs, event = delta == \"dead\") ~ 1,\n  type = \"fleming-harrington\")\n\nautoplot(surv.csr, fun = \"cumhaz\") + \n  geom_abline(aes(intercept = 0, slope = 1), col = \"red\") +\n  theme_bw()\n\n\nCumulative Hazard of Cox-Snell Residuals\n\n\n\n\nThe line with slope 1 and intercept 0 fits the curve relatively well, so we don’t see lack of fit using this procedure."
  },
  {
    "objectID": "coxph-model-building.html#martingale-residuals",
    "href": "coxph-model-building.html#martingale-residuals",
    "title": "\n7  Building Cox Proportional Hazards models\n",
    "section": "\n7.5 Martingale Residuals",
    "text": "7.5 Martingale Residuals\nThe martingale residuals are a slight modification of the Cox-Snell residuals. If the censoring indicator is \\(\\delta_i\\), then \\[r^M_i=\\delta_i-r^{CS}_i\\] These residuals can be interpreted as an estimate of the excess number of events seen in the data but not predicted by the model. We will use these to examine the functional forms of continuous covariates.\n\n7.5.1 Using Martingale Residuals\nMartingale residuals can be used to examine the functional form of a numeric variable.\n\nWe fit the model without that variable and compute the martingale residuals.\nWe then plot these martingale residuals against the values of the variable.\nWe can see curvature, or a possible suggestion that the variable can be discretized.\n\nLet’s use this to examine the score and wtime variables in the wtime data set.\nKarnofsky score\n\nCodehodg2 = hodg2 |&gt; \n  mutate(\n    mres = \n      hodg.cox1 |&gt; \n      update(. ~ . - score) |&gt; \n      residuals(type=\"martingale\"))\n\nhodg2 |&gt; \n  ggplot(aes(x = score, y = mres)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", aes(col = \"loess\")) +\n  geom_smooth(method = 'lm', aes(col = \"lm\")) +\n  theme_classic() +\n  xlab(\"Karnofsky Score\") +\n  ylab(\"Martingale Residuals\") +\n  guides(col=guide_legend(title = \"\"))\n\n\nMartingale Residuals vs. Karnofsky Score\n\n\n\n\nThe line is almost straight. It could be some modest transformation of the Karnofsky score would help, but it might not make much difference.\nWaiting time\n\nCodehodg2$mres = \n  hodg.cox1 |&gt; \n  update(. ~ . - wtime) |&gt; \n  residuals(type=\"martingale\")\n\nhodg2 |&gt; \n  ggplot(aes(x = wtime, y = mres)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", aes(col = \"loess\")) +\n  geom_smooth(method = 'lm', aes(col = \"lm\")) +\n  theme_classic() +\n  xlab(\"Waiting Time\") +\n  ylab(\"Martingale Residuals\") +\n  guides(col=guide_legend(title = \"\"))\n\n\nMartingale Residuals vs. Waiting Time\n\n\n\n\nThe line could suggest a step function. To see where the drop is, we can look at the largest waiting times and the associated martingale residual.\nThe martingale residuals are all negative for wtime &gt;83 and positive for the next smallest value. A reasonable cut-point is 80 days.\nUpdating the model\nLet’s reformulate the model with dichotomized wtime.\n\nCodehodg2 = \n  hodg2 |&gt; \n  mutate(\n    wt2 = cut(\n      wtime,c(0, 80, 200),\n      labels=c(\"short\",\"long\")))\n\nhodg.cox2 =\n  coxph(\n    formula = \n      Surv(time, event = delta == \"dead\") ~ \n      gtype*dtype + score + wt2,\n    data = hodg2)\n\n\n\nCodehodg.cox1 |&gt; drop1(test=\"Chisq\")\n\n\nModel summary table with waiting time on continuous scale \n\n\nDf\nAIC\nLRT\nPr(&gt;Chi)\n\n\n\n\nNA\n152.4\nNA\nNA\n\n\nscore\n1\n167.6\n17.236\n0.0000\n\n\nwtime\n1\n153.6\n3.279\n0.0702\n\n\ngtype:dtype\n1\n155.8\n5.436\n0.0197\n\n\n\n\n\n\nCodehodg.cox2 |&gt; drop1(test=\"Chisq\")\n\n\nModel summary table with dichotomized waiting time \n\n\nDf\nAIC\nLRT\nPr(&gt;Chi)\n\n\n\n\nNA\n149.0\nNA\nNA\n\n\nscore\n1\n168.6\n21.604\n0.0000\n\n\nwt2\n1\n153.6\n6.608\n0.0102\n\n\ngtype:dtype\n1\n152.0\n4.970\n0.0258\n\n\n\n\n\nThe new model has better (lower) AIC."
  },
  {
    "objectID": "coxph-model-building.html#checking-for-outliers-and-influential-observations",
    "href": "coxph-model-building.html#checking-for-outliers-and-influential-observations",
    "title": "\n7  Building Cox Proportional Hazards models\n",
    "section": "\n7.6 Checking for Outliers and Influential Observations",
    "text": "7.6 Checking for Outliers and Influential Observations\nWe will check for outliers using the deviance residuals. The martingale residuals show excess events or the opposite, but highly skewed, with the maximum possible value being 1, but the smallest value can be very large negative. Martingale residuals can detect unexpectedly long-lived patients, but patients who die unexpectedly early show up only in the deviance residual. Influence will be examined using dfbeta in a similar way to linear regression, logistic regression, or Poisson regression.\n\n7.6.1 Deviance Residuals\n\\[\n\\begin{aligned}\nr_i^D &= \\textrm{sign}(r_i^M)\\sqrt{-2\\left[ r_i^M+\\delta_i\\ln(\\delta_i-r_i^M)  \\right]}\\\\\nr_i^D &= \\textrm{sign}(r_i^M)\\sqrt{-2\\left[ r_i^M+\\delta_i\\ln(r_i^{CS})  \\right]}\n\\end{aligned}\n\\]\nRoughly centered on 0 with approximate standard deviation 1.\n\n7.6.2 \n\nCodehodg.mart = residuals(hodg.cox2,type=\"martingale\")\nhodg.dev = residuals(hodg.cox2,type=\"deviance\")\nhodg.dfb = residuals(hodg.cox2,type=\"dfbeta\")\nhodg.preds = predict(hodg.cox2)                   #linear predictor\n\n\n\nCodeplot(hodg.preds,\n     hodg.mart,\n     xlab=\"Linear Predictor\",\n     ylab=\"Martingale Residual\")\n\n\nMartingale Residuals vs. Linear Predictor\n\n\n\n\nThe smallest three martingale residuals in order are observations 1, 29, and 18.\n\nCodeplot(hodg.preds,hodg.dev,xlab=\"Linear Predictor\",ylab=\"Deviance Residual\")\n\n\nDeviance Residuals vs. Linear Predictor\n\n\n\n\nThe two largest deviance residuals are observations 1 and 29. Worth examining.\n\n7.6.3 dfbeta\n\ndfbeta is the approximate change in the coefficient vector if that observation were dropped\ndfbetas is the approximate change in the coefficients, scaled by the standard error for the coefficients.\n\nGraft type\n\nCodeplot(hodg.dfb[,1],xlab=\"Observation Order\",ylab=\"dfbeta for Graft Type\")\n\n\ndfbeta Values by Observation Order for Graft Type\n\n\n\n\nThe smallest dfbeta for graft type is observation 1.\nDisease type\n\nCodeplot(hodg.dfb[,2],\n     xlab=\"Observation Order\",\n     ylab=\"dfbeta for Disease Type\")\n\n\ndfbeta Values by Observation Order for Disease Type\n\n\n\n\nThe smallest two dfbeta values for disease type are observations 1 and 16.\nKarnofsky score\n\nCodeplot(hodg.dfb[,3],\n     xlab=\"Observation Order\",\n     ylab=\"dfbeta for Karnofsky Score\")\n\n\ndfbeta Values by Observation Order for Karnofsky Score\n\n\n\n\nThe two highest dfbeta values for score are observations 1 and 18. The next three are observations 17, 29, and 19. The smallest value is observation 2.\nWaiting time (dichotomized)\n\nCodeplot(\n  hodg.dfb[,4],\n  xlab=\"Observation Order\",\n  ylab=\"dfbeta for `Waiting Time &lt; 80`\")\n\n\ndfbeta Values by Observation Order for Waiting Time (dichotomized)\n\n\n\n\nThe two large values of dfbeta for dichotomized waiting time are observations 15 and 16. This may have to do with the discretization of waiting time.\nInteraction: graft type and disease type\n\nCodeplot(hodg.dfb[,5],\n     xlab=\"Observation Order\",\n     ylab=\"dfbeta for dtype:gtype\")\n\n\ndfbeta Values by Observation Order for dtype:gtype\n\n\n\n\nThe two largest values are observations 1 and 16. The smallest value is observation 35.\n\n\nTable 7.1: Observations to Examine by Residuals and Influence\n\nDiagnostic\nObservations to Examine\n\n\n\nMartingale Residuals\n1, 29, 18\n\n\nDeviance Residuals\n1, 29\n\n\nGraft Type Influence\n1\n\n\nDisease Type Influence\n1, 16\n\n\nKarnofsky Score Influence\n1, 18 (17, 29, 19)\n\n\nWaiting Time Influence\n15, 16\n\n\nGraft by Disease Influence\n1, 16, 35\n\n\n\n\nThe most important observations to examine seem to be 1, 15, 16, 18, and 29.\n\n7.6.4 \n\nCodewith(hodg,summary(time[delta==1]))\n\n\n\nMin.\n1st Qu.\nMedian\nMean\n3rd Qu.\nMax.\n\n\n2\n41.25\n62.5\n97.62\n83.25\n524\n\n\n\n\n\nCodewith(hodg,summary(wtime))\n\n\n\nMin.\n1st Qu.\nMedian\nMean\n3rd Qu.\nMax.\n\n\n5\n16\n24\n37.7\n55.5\n171\n\n\n\n\n\nCodewith(hodg,summary(score))\n\n\n\nMin.\n1st Qu.\nMedian\nMean\n3rd Qu.\nMax.\n\n\n20\n60\n80\n76.28\n90\n100\n\n\n\n\n\nCodehodg.cox2\n\nCall:\ncoxph(formula = Surv(time, event = delta == \"dead\") ~ gtype * \n    dtype + score + wt2, data = hodg2)\n\n                               coef exp(coef) se(coef)  z     p\ngtypeAutologous                0.67      1.94     0.59  1 0.263\ndtypeHodgkins                  2.33     10.25     0.73  3 0.002\nscore                         -0.06      0.95     0.01 -4 8e-06\nwt2long                       -2.06      0.13     1.05 -2 0.050\ngtypeAutologous:dtypeHodgkins -2.07      0.13     0.93 -2 0.026\n\nLikelihood ratio test=35  on 5 df, p=1e-06\nn= 43, number of events= 26 \n\n\n\nCodehodg2[c(1,15,16,18,29),] |&gt; \n  select(gtype, dtype, time, delta, score, wtime) |&gt; \n  mutate(\n    comment = \n      c(\n        \"early death, good score, low risk\",\n        \"high risk grp, long wait, poor score\",\n        \"high risk grp, short wait, poor score\",\n        \"early death, good score, med risk grp\",\n        \"early death, good score, med risk grp\"\n      ))\n\n\n\n\n\n\n\n\n\n\n\n\ngtype\ndtype\ntime\ndelta\nscore\nwtime\ncomment\n\n\n\nAllogenic\nNon-Hodgkins\n28\ndead\n90\n24\nearly death, good score, low risk\n\n\nAllogenic\nHodgkins\n77\ndead\n60\n102\nhigh risk grp, long wait, poor score\n\n\nAllogenic\nHodgkins\n79\ndead\n70\n71\nhigh risk grp, short wait, poor score\n\n\nAutologous\nNon-Hodgkins\n53\ndead\n90\n17\nearly death, good score, med risk grp\n\n\nAutologous\nHodgkins\n30\ndead\n90\n73\nearly death, good score, med risk grp\n\n\n\n\n\n\n7.6.5 Action Items\n\nUnusual points may need checking, particularly if the data are not completely cleaned. In this case, observations 15 and 16 may show some trouble with the dichotomization of waiting time, but it still may be useful.\nThe two largest residuals seem to be due to unexpectedly early deaths, but unfortunately this can occur.\nIf hazards don’t look proportional, then we may need to use strata, between which the base hazards are permitted to be different. For this problem, the natural strata are the two diseases, because they could need to be managed differently anyway.\nA main point that we want to be sure of is the relative risk difference by disease type and graft type.\n\n\nCodehodg.cox2 |&gt; \n  predict(\n    reference = \"zero\",\n    newdata = means |&gt; \n      mutate(\n        wt2 = \"short\", \n        score = 0), \n    type = \"lp\") |&gt; \n  data.frame('linear predictor' = _) |&gt; \n  pander()\n\n\nLinear Risk Predictors for Lymphoma \n\n\n\n\n\n \nlinear.predictor\n\n\n\nNon-Hodgkins,Allogenic\n0\n\n\nNon-Hodgkins,Autologous\n0.6651\n\n\nHodgkins,Allogenic\n2.327\n\n\nHodgkins,Autologous\n0.9256\n\n\n\n\n\nFor Non-Hodgkin’s, the allogenic graft is better. For Hodgkin’s, the autologous graft is much better."
  },
  {
    "objectID": "coxph-model-building.html#stratified-survival-models",
    "href": "coxph-model-building.html#stratified-survival-models",
    "title": "\n7  Building Cox Proportional Hazards models\n",
    "section": "\n7.7 Stratified survival models",
    "text": "7.7 Stratified survival models\n\n7.7.1 Revisiting the leukemia dataset (anderson)\nWe will analyze remission survival times on 42 leukemia patients, half on new treatment, half on standard treatment.\nThis is the same data as the drug6mp data from KMsurv, but with two other variables and without the pairing. This version comes from the Kleinbaum and Klein survival textbook (e.g., p281):\n\nCodeanderson = \n  paste0(\n    \"http://web1.sph.emory.edu/dkleinb/allDatasets/\",\n    \"surv2datasets/anderson.dta\") |&gt; \n  haven::read_dta() |&gt; \n  mutate(\n    status = status |&gt; \n      case_match(\n        1 ~ \"relapse\",\n        0 ~ \"censored\"\n      ),\n    \n    sex = sex |&gt; \n      case_match(\n        0 ~ \"female\",\n        1 ~ \"male\"\n      ) |&gt; \n      factor() |&gt; \n      relevel(ref = \"female\"),\n    \n    rx = rx |&gt; \n      case_match(\n        0 ~ \"new\",\n        1 ~ \"standard\"\n      ) |&gt; \n      factor() |&gt; relevel(ref = \"standard\"),\n    \n    surv = Surv(\n      time = survt, \n      event = (status == \"relapse\"))\n  )\n\nprint(anderson)\n\n\n\n7.7.2 Cox semi-parametric proportional hazards model\n\nCodeanderson.cox1 = coxph(\n  formula = surv ~ rx + sex + logwbc,\n  data = anderson)\n\nsummary(anderson.cox1)\n\nCall:\ncoxph(formula = surv ~ rx + sex + logwbc, data = anderson)\n\n  n= 42, number of events= 30 \n\n          coef exp(coef) se(coef)     z Pr(&gt;|z|)    \nrxnew   -1.504     0.222    0.462 -3.26   0.0011 ** \nsexmale  0.315     1.370    0.455  0.69   0.4887    \nlogwbc   1.682     5.376    0.337  5.00  5.8e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        exp(coef) exp(-coef) lower .95 upper .95\nrxnew       0.222      4.498     0.090     0.549\nsexmale     1.370      0.730     0.562     3.338\nlogwbc      5.376      0.186     2.779    10.398\n\nConcordance= 0.851  (se = 0.041 )\nLikelihood ratio test= 47.2  on 3 df,   p=3e-10\nWald test            = 33.5  on 3 df,   p=2e-07\nScore (logrank) test = 48  on 3 df,   p=2e-10\n\n\nTest the proportional hazards assumption\n\nCodecox.zph(anderson.cox1)\n\n       chisq df    p\nrx     0.036  1 0.85\nsex    5.420  1 0.02\nlogwbc 0.142  1 0.71\nGLOBAL 5.879  3 0.12\n\n\nGraph the K-M survival curves\n\nCodeanderson_km_model = survfit(\n  formula = surv ~ sex,\n  data = anderson)\n\nanderson_km_model |&gt; \n  autoplot(conf.int = FALSE) +\n  theme_bw() +\n  theme(legend.position=\"bottom\")\n\n\n\n\nThe survival curves cross, which indicates a problem in the proportionality assumption by sex.\n\n7.7.3 Graph the Nelson-Aalen cumulative hazard\nWe can also look at the log-hazard (“cloglog survival”) plots:\n\nCodeanderson_na_model = survfit(\n  formula = surv ~ sex,\n  data = anderson,\n  type = \"fleming\")\n\nanderson_na_model |&gt; \n  autoplot(\n    fun = \"cumhaz\",\n    conf.int = FALSE) +\n  theme_classic() +\n  theme(legend.position=\"bottom\") +\n  ylab(\"log(Cumulative Hazard)\") +\n  scale_y_continuous(\n    trans = \"log10\",\n    name = \"Cumulative hazard (H(t), log scale)\") +\n  scale_x_continuous(\n    breaks = c(1,2,5,10,20,50),\n    trans = \"log\"\n  )\n\n\nCumulative hazard (cloglog scale) for anderson data\n\n\n\n\nThis can be fixed by using strata or possibly by other model alterations.\n\n7.7.4 The Stratified Cox Model\n\nIn a stratified Cox model, each stratum, defined by one or more factors, has its own base survival function \\(h_0(t)\\).\nBut the coefficients for each variable not used in the strata definitions are assumed to be the same across strata.\nTo check if this assumption is reasonable one can include interactions with strata and see if they are significant (this may generate a warning and NA lines but these can be ignored).\nSince the sex variable shows possible non-proportionality, we try stratifying on sex.\n\n\nCodeanderson.coxph.strat = \n  coxph(\n    formula = \n      surv ~ rx + logwbc + strata(sex),\n    data = anderson)\n\nsummary(anderson.coxph.strat)\n\nCall:\ncoxph(formula = surv ~ rx + logwbc + strata(sex), data = anderson)\n\n  n= 42, number of events= 30 \n\n         coef exp(coef) se(coef)     z Pr(&gt;|z|)    \nrxnew  -0.998     0.369    0.474 -2.11    0.035 *  \nlogwbc  1.454     4.279    0.344  4.22  2.4e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\nrxnew      0.369      2.713     0.146     0.932\nlogwbc     4.279      0.234     2.180     8.398\n\nConcordance= 0.812  (se = 0.059 )\nLikelihood ratio test= 32.1  on 2 df,   p=1e-07\nWald test            = 22.8  on 2 df,   p=1e-05\nScore (logrank) test = 30.8  on 2 df,   p=2e-07\n\n\nLet’s compare this to a model fit only on the subset of males:\n\nCodeanderson.coxph.male = \n  coxph(\n    formula = surv ~ rx + logwbc,\n    subset = sex == \"male\",\n    data = anderson)\n\nsummary(anderson.coxph.male)\n\nCall:\ncoxph(formula = surv ~ rx + logwbc, data = anderson, subset = sex == \n    \"male\")\n\n  n= 20, number of events= 14 \n\n         coef exp(coef) se(coef)     z Pr(&gt;|z|)   \nrxnew  -1.978     0.138    0.739 -2.68   0.0075 **\nlogwbc  1.743     5.713    0.536  3.25   0.0011 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\nrxnew      0.138      7.227    0.0325     0.589\nlogwbc     5.713      0.175    1.9991    16.328\n\nConcordance= 0.905  (se = 0.043 )\nLikelihood ratio test= 29.2  on 2 df,   p=5e-07\nWald test            = 15.3  on 2 df,   p=5e-04\nScore (logrank) test = 26.4  on 2 df,   p=2e-06\n\n\n\nCodeanderson.coxph.female = \n  coxph(\n    formula = \n      surv ~ rx + logwbc,\n    subset = sex == \"female\",\n    data = anderson)\n\nsummary(anderson.coxph.female)\n\nCall:\ncoxph(formula = surv ~ rx + logwbc, data = anderson, subset = sex == \n    \"female\")\n\n  n= 22, number of events= 16 \n\n         coef exp(coef) se(coef)     z Pr(&gt;|z|)  \nrxnew  -0.311     0.733    0.564 -0.55    0.581  \nlogwbc  1.206     3.341    0.503  2.40    0.017 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\nrxnew      0.733      1.365     0.243      2.21\nlogwbc     3.341      0.299     1.245      8.96\n\nConcordance= 0.692  (se = 0.085 )\nLikelihood ratio test= 6.65  on 2 df,   p=0.04\nWald test            = 6.36  on 2 df,   p=0.04\nScore (logrank) test = 6.74  on 2 df,   p=0.03\n\n\nThe coefficients of treatment look different. Are they statistically different?\n\nCodeanderson.coxph.strat.intxn = \n  coxph(\n    formula = surv ~ strata(sex) * (rx + logwbc),\n    data = anderson)\n\nanderson.coxph.strat.intxn |&gt; summary()\n\nCall:\ncoxph(formula = surv ~ strata(sex) * (rx + logwbc), data = anderson)\n\n  n= 42, number of events= 30 \n\n                         coef exp(coef) se(coef)     z Pr(&gt;|z|)  \nrxnew                  -0.311     0.733    0.564 -0.55    0.581  \nlogwbc                  1.206     3.341    0.503  2.40    0.017 *\nstrata(sex)male:rxnew  -1.667     0.189    0.930 -1.79    0.073 .\nstrata(sex)male:logwbc  0.537     1.710    0.735  0.73    0.465  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                       exp(coef) exp(-coef) lower .95 upper .95\nrxnew                      0.733      1.365    0.2427      2.21\nlogwbc                     3.341      0.299    1.2452      8.96\nstrata(sex)male:rxnew      0.189      5.294    0.0305      1.17\nstrata(sex)male:logwbc     1.710      0.585    0.4048      7.23\n\nConcordance= 0.797  (se = 0.058 )\nLikelihood ratio test= 35.8  on 4 df,   p=3e-07\nWald test            = 21.7  on 4 df,   p=2e-04\nScore (logrank) test = 33.1  on 4 df,   p=1e-06\n\n\n\nCodeanova(\n  anderson.coxph.strat.intxn,\n  anderson.coxph.strat)\n\n\n\nloglik\nChisq\nDf\nPr(&gt;|Chi|)\n\n\n\n-53.85\nNA\nNA\nNA\n\n\n-55.73\n3.766\n2\n0.1521\n\n\n\n\n\nWe don’t have enough evidence to tell the difference between these two models.\n\n7.7.5 Conclusions\n\nWe chose to use a stratified model because of the apparent non-proportionality of the hazard for the sex variable.\nWhen we fit interactions with the strata variable, we did not get an improved model (via the likelihood ratio test).\nSo we use the stratifed model with coefficients that are the same across strata.\n\n7.7.6 Another Modeling Approach\n\nWe used an additive model without interactions and saw that we might need to stratify by sex.\nInstead, we could try to improve the model’s functional form - maybe the interaction of treatment and sex is real, and after fitting that we might not need separate hazard functions.\nEither approach may work.\n\n\nCodeanderson.coxph.intxn = \n  coxph(\n    formula = surv ~ (rx + logwbc) * sex,\n    data = anderson)\n\nanderson.coxph.intxn |&gt; summary()\n\nCall:\ncoxph(formula = surv ~ (rx + logwbc) * sex, data = anderson)\n\n  n= 42, number of events= 30 \n\n                  coef exp(coef) se(coef)     z Pr(&gt;|z|)  \nrxnew          -0.3748    0.6874   0.5545 -0.68    0.499  \nlogwbc          1.0637    2.8971   0.4726  2.25    0.024 *\nsexmale        -2.8052    0.0605   2.0323 -1.38    0.167  \nrxnew:sexmale  -2.1782    0.1132   0.9109 -2.39    0.017 *\nlogwbc:sexmale  1.2303    3.4223   0.6301  1.95    0.051 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n               exp(coef) exp(-coef) lower .95 upper .95\nrxnew             0.6874      1.455   0.23185     2.038\nlogwbc            2.8971      0.345   1.14730     7.315\nsexmale           0.0605     16.531   0.00113     3.248\nrxnew:sexmale     0.1132      8.830   0.01899     0.675\nlogwbc:sexmale    3.4223      0.292   0.99539    11.766\n\nConcordance= 0.861  (se = 0.036 )\nLikelihood ratio test= 57  on 5 df,   p=5e-11\nWald test            = 35.6  on 5 df,   p=1e-06\nScore (logrank) test = 57.1  on 5 df,   p=5e-11\n\n\n\nCodecox.zph(anderson.coxph.intxn)\n\n           chisq df    p\nrx         0.136  1 0.71\nlogwbc     1.652  1 0.20\nsex        1.266  1 0.26\nrx:sex     0.149  1 0.70\nlogwbc:sex 0.102  1 0.75\nGLOBAL     3.747  5 0.59"
  },
  {
    "objectID": "coxph-model-building.html#time-varying-covariates",
    "href": "coxph-model-building.html#time-varying-covariates",
    "title": "\n7  Building Cox Proportional Hazards models\n",
    "section": "\n7.8 Time-varying covariates",
    "text": "7.8 Time-varying covariates\n(adapted from Klein and Moeschberger 2003, §9.2: https://link.springer.com/book/10.1007/b97377)\n\n7.8.1 Motivating example: back to the leukemia dataset\n\n# load the data:\ndata(bmt, package = 'KMsurv')\nbmt |&gt; as_tibble() |&gt; print(n = 5)\n\n# A tibble: 137 × 22\n  group    t1    t2    d1    d2    d3    ta    da    tc    dc    tp    dp    z1\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     1  2081  2081     0     0     0    67     1   121     1    13     1    26\n2     1  1602  1602     0     0     0  1602     0   139     1    18     1    21\n3     1  1496  1496     0     0     0  1496     0   307     1    12     1    26\n4     1  1462  1462     0     0     0    70     1    95     1    13     1    17\n5     1  1433  1433     0     0     0  1433     0   236     1    12     1    32\n# ℹ 132 more rows\n# ℹ 9 more variables: z2 &lt;int&gt;, z3 &lt;int&gt;, z4 &lt;int&gt;, z5 &lt;int&gt;, z6 &lt;int&gt;,\n#   z7 &lt;int&gt;, z8 &lt;int&gt;, z9 &lt;int&gt;, z10 &lt;int&gt;\n\n\nThis dataset comes from the Copelan et al. (1991) study of allogenic bone marrow transplant therapy for acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).\nOutcomes (endpoints)\n\nThe main endpoint is disease-free survival (t2 and d3) for the three risk groups, “ALL”, “AML Low Risk”, and “AML High Risk”.\nPossible intermediate events\n\ngraft vs. host disease (GVHD), an immunological rejection response to the transplant (bad)\nacute (AGVHD)\nchronic (CGVHD)\nplatelet recovery, a return of platelet count to normal levels (good)\n\nOne or the other, both in either order, or neither may occur.\nCovariates\n\nWe are interested in possibly using the covariates z1-z10 to adjust for other factors.\nIn addition, the time-varying covariates for acute GVHD, chronic GVHD, and platelet recovery may be useful.\nPreprocessing\nWe add reformat the data before analysis:\n\nCode# reformat the data:\nbmt1 = \n  bmt |&gt; \n  as_tibble() |&gt; \n  mutate(\n    id = 1:n(), # will be used to connect multiple records for the same individual\n    \n    group =  group |&gt; \n      case_match(\n        1 ~ \"ALL\",\n        2 ~ \"Low Risk AML\",\n        3 ~ \"High Risk AML\") |&gt; \n      factor(levels = c(\"ALL\", \"Low Risk AML\", \"High Risk AML\")),\n    \n    `patient age` = z1,\n    \n    `donor age` = z2,\n    \n    `patient sex` = z3 |&gt; \n      case_match(\n        0 ~ \"Female\",\n        1 ~ \"Male\"),\n    \n    `donor sex` = z4 |&gt; \n      case_match(\n        0 ~ \"Female\",\n        1 ~ \"Male\"),\n    \n    `Patient CMV Status` = z5 |&gt; \n      case_match(\n        0 ~ \"CMV Negative\",\n        1 ~ \"CMV Positive\"),\n    \n    `Donor CMV Status` = z6 |&gt; \n      case_match(\n        0 ~ \"CMV Negative\",\n        1 ~ \"CMV Positive\"),\n    \n    `Waiting Time to Transplant` = z7,\n    \n    FAB = z8 |&gt; \n      case_match(\n        1 ~ \"Grade 4 Or 5 (AML only)\",\n        0 ~ \"Other\") |&gt; \n      factor() |&gt; \n      relevel(ref = \"Other\"),\n    \n    hospital = z9 |&gt;  # `z9` is hospital\n      case_match(\n        1 ~ \"Ohio State University\",\n        2 ~ \"Alferd\",\n        3 ~ \"St. Vincent\",\n        4 ~ \"Hahnemann\") |&gt; \n      factor() |&gt; \n      relevel(ref = \"Ohio State University\"),\n    \n    MTX = (z10 == 1) # a prophylatic treatment for GVHD\n    \n  ) |&gt; \n  select(-(z1:z10)) # don't need these anymore\n\nbmt1 |&gt; \n  select(group, id:MTX) |&gt; \n  print(n = 10)\n\n# A tibble: 137 × 12\n   group    id `patient age` `donor age` `patient sex` `donor sex`\n   &lt;fct&gt; &lt;int&gt;         &lt;int&gt;       &lt;int&gt; &lt;chr&gt;         &lt;chr&gt;      \n 1 ALL       1            26          33 Male          Female     \n 2 ALL       2            21          37 Male          Male       \n 3 ALL       3            26          35 Male          Male       \n 4 ALL       4            17          21 Female        Male       \n 5 ALL       5            32          36 Male          Male       \n 6 ALL       6            22          31 Male          Male       \n 7 ALL       7            20          17 Male          Female     \n 8 ALL       8            22          24 Male          Female     \n 9 ALL       9            18          21 Female        Male       \n10 ALL      10            24          40 Male          Male       \n# ℹ 127 more rows\n# ℹ 6 more variables: `Patient CMV Status` &lt;chr&gt;, `Donor CMV Status` &lt;chr&gt;,\n#   `Waiting Time to Transplant` &lt;int&gt;, FAB &lt;fct&gt;, hospital &lt;fct&gt;, MTX &lt;lgl&gt;\n\n\n\n7.8.2 Time-Dependent Covariates\n\nA time-dependent covariate (“TDC”) is a covariate whose value changes during the course of the study.\nFor variables like age that change in a linear manner with time, we can just use the value at the start.\nBut it may be plausible that when and if GVHD occurs, the risk of relapse or death increases, and when and if platelet recovery occurs, the risk decreases.\n\n7.8.3 Analysis in R\n\nWe form a variable precovery which is = 0 before platelet recovery and is = 1 after platelet recovery, if it occurs.\nFor each subject where platelet recovery occurs, we set up multiple records (lines in the data frame); for example one from t = 0 to the time of platelet recovery, and one from that time to relapse, recovery, or death.\nWe do the same for acute GVHD and chronic GVHD.\nFor each record, the covariates are constant.\n\n\nCodebmt2 = bmt1 |&gt; \n  #set up new long-format data set:\n  tmerge(bmt1, id = id, tstop = t2) |&gt; \n  \n  # the following three steps can be in any order, \n  # and will still produce the same result:\n  #add aghvd as tdc:\n  tmerge(bmt1, id = id, agvhd = tdc(ta)) |&gt; \n  #add cghvd as tdc:\n  tmerge(bmt1, id = id, cgvhd = tdc(tc)) |&gt; \n  #add platelet recovery as tdc:\n  tmerge(bmt1, id = id, precovery = tdc(tp))   \n\nbmt2 = bmt2 |&gt; \n  as_tibble() |&gt; \n  mutate(status = as.numeric((tstop == t2) & d3))\n# status only = 1 if at end of t2 and not censored\n\n\nLet’s see how we’ve rearranged the first row of the data:\n\nCodebmt1 |&gt; \n  filter(id == 1) |&gt; \n  select(id, t1, d1, t2, d2, d3, ta, da, tc, dc, tp, dp)\n\n\n\nid\nt1\nd1\nt2\nd2\nd3\nta\nda\ntc\ndc\ntp\ndp\n\n\n1\n2081\n0\n2081\n0\n0\n67\n1\n121\n1\n13\n1\n\n\n\n\nThe event times for this individual are:\n\n\nt = 0 time of transplant\n\ntp = 13 platelet recovery\n\nta = 67 acute GVHD onset\n\ntc = 121 chronic GVHD onset\n\nt2 = 2081 end of study, patient not relapsed or dead\n\nAfter converting the data to long-format, we have:\n\nCodebmt2 |&gt; \n  select(\n    id,\n    tstart,\n    tstop,\n    agvhd,\n    cgvhd,\n    precovery,\n    status\n  ) |&gt; \n  filter(id == 1)\n\n\n\nid\ntstart\ntstop\nagvhd\ncgvhd\nprecovery\nstatus\n\n\n\n1\n0\n13\n0\n0\n0\n0\n\n\n1\n13\n67\n0\n0\n1\n0\n\n\n1\n67\n121\n1\n0\n1\n0\n\n\n1\n121\n2081\n1\n1\n1\n0\n\n\n\n\n\nNote that status could have been 1 on the last row, indicating that relapse or death occurred; since it is false, the participant must have exited the study without experiencing relapse or death (i.e., they were censored).\n\n7.8.4 Event sequences\nLet:\n\nA = acute GVHD\nC = chronic GVHD\nP = platelet recovery\n\nEach of the eight possible combinations of A or not-A, with C or not-C, with P or not-P occurs in this data set.\n\nA always occurs before C, and P always occurs before C, if both occur.\nThus there are ten event sequences in the data set: None, A, C, P, AC, AP, PA, PC, APC, and PAC.\nIn general, there could be as many as \\(1+3+(3)(2)+6=16\\) sequences, but our domain knowledge tells us that some are missing: CA, CP, CAP, CPA, PCA, PC, PAC\nDifferent subjects could have 1, 2, 3, or 4 intervals, depending on which of acute GVHD, chronic GVHD, and/or platelet recovery occurred.\nThe final interval for any subject has status = 1 if the subject relapsed or died at that time; otherwise status = 0.\nAny earlier intervals have status = 0.\nEven though there might be multiple lines per ID in the dataset, there is never more than one event, so no alterations need be made in the estimation procedures or in the interpretation of the output.\nThe function tmerge in the survival package eases the process of constructing the new long-format dataset.\n\n7.8.5 Model with Time-Fixed Covariates\n\nCodebmt1 = \n  bmt1 |&gt; \n  mutate(surv = Surv(t2,d3))\n\nbmt_coxph_TF = coxph(\n  formula = surv ~ group + `patient age`*`donor age` + FAB,\n  data = bmt1)\nsummary(bmt_coxph_TF)\n\nCall:\ncoxph(formula = surv ~ group + `patient age` * `donor age` + \n    FAB, data = bmt1)\n\n  n= 137, number of events= 83 \n\n                                coef exp(coef)  se(coef)     z Pr(&gt;|z|)    \ngroupLow Risk AML          -1.090648  0.335999  0.354279 -3.08  0.00208 ** \ngroupHigh Risk AML         -0.403905  0.667707  0.362777 -1.11  0.26555    \n`patient age`              -0.081639  0.921605  0.036107 -2.26  0.02376 *  \n`donor age`                -0.084587  0.918892  0.030097 -2.81  0.00495 ** \nFABGrade 4 Or 5 (AML only)  0.837416  2.310388  0.278464  3.01  0.00264 ** \n`patient age`:`donor age`   0.003159  1.003164  0.000951  3.32  0.00089 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                           exp(coef) exp(-coef) lower .95 upper .95\ngroupLow Risk AML              0.336      2.976     0.168     0.673\ngroupHigh Risk AML             0.668      1.498     0.328     1.360\n`patient age`                  0.922      1.085     0.859     0.989\n`donor age`                    0.919      1.088     0.866     0.975\nFABGrade 4 Or 5 (AML only)     2.310      0.433     1.339     3.988\n`patient age`:`donor age`      1.003      0.997     1.001     1.005\n\nConcordance= 0.665  (se = 0.033 )\nLikelihood ratio test= 32.8  on 6 df,   p=1e-05\nWald test            = 33  on 6 df,   p=1e-05\nScore (logrank) test = 35.8  on 6 df,   p=3e-06\n\nCodedrop1(bmt_coxph_TF, test = \"Chisq\")\n\n\n\n\nDf\nAIC\nLRT\nPr(&gt;Chi)\n\n\n\n\nNA\n725.8\nNA\nNA\n\n\ngroup\n2\n734.3\n12.511\n0.0019\n\n\nFAB\n1\n733.0\n9.216\n0.0024\n\n\n\npatient age:donor age\n\n1\n733.3\n9.514\n0.0020\n\n\n\n\n\n\nCodebmt1$mres = \n  bmt_coxph_TF |&gt; \n  update(. ~ . - `donor age`) |&gt; \n  residuals(type=\"martingale\")\n\nbmt1 |&gt; \n  ggplot(aes(x = `donor age`, y = mres)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", aes(col = \"loess\")) +\n  geom_smooth(method = 'lm', aes(col = \"lm\")) +\n  theme_classic() +\n  xlab(\"Donor age\") +\n  ylab(\"Martingale Residuals\") +\n  guides(col=guide_legend(title = \"\"))\n\n\nMartingale residuals for Donor age\n\n\n\n\nA more complex functional form for donor age seems warranted; left as an exercise for the reader.\nNow we will add the time-varying covariates:\n\nCode# add counting process formulation of Surv():\nbmt2 = \n  bmt2 |&gt; \n  mutate(\n    surv = \n      Surv(\n        time = tstart,\n        time2 = tstop,\n        event = status,\n        type = \"counting\"))\n\n\n\n7.8.6 Model with Time-Dependent Covariates\n\nCodebmt_coxph_TV = coxph(\n  formula = \n    surv ~ \n    group + `patient age`*`donor age` + FAB + agvhd + cgvhd + precovery,\n  data = bmt2)\n\nsummary(bmt_coxph_TV)\n\nCall:\ncoxph(formula = surv ~ group + `patient age` * `donor age` + \n    FAB + agvhd + cgvhd + precovery, data = bmt2)\n\n  n= 341, number of events= 83 \n\n                                coef exp(coef)  se(coef)     z Pr(&gt;|z|)   \ngroupLow Risk AML          -1.038514  0.353980  0.358220 -2.90   0.0037 **\ngroupHigh Risk AML         -0.380481  0.683533  0.374867 -1.01   0.3101   \n`patient age`              -0.073351  0.929275  0.035956 -2.04   0.0413 * \n`donor age`                -0.076406  0.926440  0.030196 -2.53   0.0114 * \nFABGrade 4 Or 5 (AML only)  0.805700  2.238263  0.284273  2.83   0.0046 **\nagvhd                       0.150565  1.162491  0.306848  0.49   0.6237   \ncgvhd                      -0.116136  0.890354  0.289046 -0.40   0.6878   \nprecovery                  -0.941123  0.390190  0.347861 -2.71   0.0068 **\n`patient age`:`donor age`   0.002895  1.002899  0.000944  3.07   0.0022 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                           exp(coef) exp(-coef) lower .95 upper .95\ngroupLow Risk AML              0.354      2.825     0.175     0.714\ngroupHigh Risk AML             0.684      1.463     0.328     1.425\n`patient age`                  0.929      1.076     0.866     0.997\n`donor age`                    0.926      1.079     0.873     0.983\nFABGrade 4 Or 5 (AML only)     2.238      0.447     1.282     3.907\nagvhd                          1.162      0.860     0.637     2.121\ncgvhd                          0.890      1.123     0.505     1.569\nprecovery                      0.390      2.563     0.197     0.772\n`patient age`:`donor age`      1.003      0.997     1.001     1.005\n\nConcordance= 0.702  (se = 0.028 )\nLikelihood ratio test= 40.3  on 9 df,   p=7e-06\nWald test            = 42.4  on 9 df,   p=3e-06\nScore (logrank) test = 47.2  on 9 df,   p=4e-07\n\n\nPlatelet recovery is highly significant.\nNeither acute GVHD (agvhd) nor chronic GVHD (cgvhd) has a statistically significant effect here, nor are they significant in models with the other one removed.\n\nCodeupdate(bmt_coxph_TV, .~.-agvhd) |&gt; summary()\n\nCall:\ncoxph(formula = surv ~ group + `patient age` + `donor age` + \n    FAB + cgvhd + precovery + `patient age`:`donor age`, data = bmt2)\n\n  n= 341, number of events= 83 \n\n                                coef exp(coef)  se(coef)     z Pr(&gt;|z|)   \ngroupLow Risk AML          -1.049870  0.349983  0.356727 -2.94   0.0032 **\ngroupHigh Risk AML         -0.417049  0.658988  0.365348 -1.14   0.2537   \n`patient age`              -0.070749  0.931696  0.035477 -1.99   0.0461 * \n`donor age`                -0.075693  0.927101  0.030075 -2.52   0.0118 * \nFABGrade 4 Or 5 (AML only)  0.807035  2.241253  0.283437  2.85   0.0044 **\ncgvhd                      -0.095393  0.909015  0.285979 -0.33   0.7387   \nprecovery                  -0.983653  0.373942  0.338170 -2.91   0.0036 **\n`patient age`:`donor age`   0.002859  1.002863  0.000936  3.05   0.0023 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                           exp(coef) exp(-coef) lower .95 upper .95\ngroupLow Risk AML              0.350      2.857     0.174     0.704\ngroupHigh Risk AML             0.659      1.517     0.322     1.349\n`patient age`                  0.932      1.073     0.869     0.999\n`donor age`                    0.927      1.079     0.874     0.983\nFABGrade 4 Or 5 (AML only)     2.241      0.446     1.286     3.906\ncgvhd                          0.909      1.100     0.519     1.592\nprecovery                      0.374      2.674     0.193     0.726\n`patient age`:`donor age`      1.003      0.997     1.001     1.005\n\nConcordance= 0.701  (se = 0.027 )\nLikelihood ratio test= 40  on 8 df,   p=3e-06\nWald test            = 42.4  on 8 df,   p=1e-06\nScore (logrank) test = 47.2  on 8 df,   p=1e-07\n\nCodeupdate(bmt_coxph_TV, .~.-cgvhd) |&gt; summary()\n\nCall:\ncoxph(formula = surv ~ group + `patient age` + `donor age` + \n    FAB + agvhd + precovery + `patient age`:`donor age`, data = bmt2)\n\n  n= 341, number of events= 83 \n\n                                coef exp(coef)  se(coef)     z Pr(&gt;|z|)   \ngroupLow Risk AML          -1.019638  0.360725  0.355311 -2.87   0.0041 **\ngroupHigh Risk AML         -0.381356  0.682935  0.374568 -1.02   0.3086   \n`patient age`              -0.073189  0.929426  0.035890 -2.04   0.0414 * \n`donor age`                -0.076753  0.926118  0.030121 -2.55   0.0108 * \nFABGrade 4 Or 5 (AML only)  0.811716  2.251769  0.284012  2.86   0.0043 **\nagvhd                       0.131621  1.140676  0.302623  0.43   0.6636   \nprecovery                  -0.946697  0.388021  0.347265 -2.73   0.0064 **\n`patient age`:`donor age`   0.002904  1.002908  0.000943  3.08   0.0021 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                           exp(coef) exp(-coef) lower .95 upper .95\ngroupLow Risk AML              0.361      2.772     0.180     0.724\ngroupHigh Risk AML             0.683      1.464     0.328     1.423\n`patient age`                  0.929      1.076     0.866     0.997\n`donor age`                    0.926      1.080     0.873     0.982\nFABGrade 4 Or 5 (AML only)     2.252      0.444     1.291     3.929\nagvhd                          1.141      0.877     0.630     2.064\nprecovery                      0.388      2.577     0.196     0.766\n`patient age`:`donor age`      1.003      0.997     1.001     1.005\n\nConcordance= 0.701  (se = 0.027 )\nLikelihood ratio test= 40.1  on 8 df,   p=3e-06\nWald test            = 42.1  on 8 df,   p=1e-06\nScore (logrank) test = 47.1  on 8 df,   p=1e-07\n\n\nLet’s drop them both:\n\nCodebmt_coxph_TV2 = update(bmt_coxph_TV, . ~ . - agvhd -cgvhd)\nbmt_coxph_TV2 |&gt; summary()\n\nCall:\ncoxph(formula = surv ~ group + `patient age` + `donor age` + \n    FAB + precovery + `patient age`:`donor age`, data = bmt2)\n\n  n= 341, number of events= 83 \n\n                                coef exp(coef)  se(coef)     z Pr(&gt;|z|)   \ngroupLow Risk AML          -1.032520  0.356108  0.353202 -2.92   0.0035 **\ngroupHigh Risk AML         -0.413888  0.661075  0.365209 -1.13   0.2571   \n`patient age`              -0.070965  0.931495  0.035453 -2.00   0.0453 * \n`donor age`                -0.076052  0.926768  0.030007 -2.53   0.0113 * \nFABGrade 4 Or 5 (AML only)  0.811926  2.252242  0.283231  2.87   0.0041 **\nprecovery                  -0.983505  0.373998  0.337997 -2.91   0.0036 **\n`patient age`:`donor age`   0.002872  1.002876  0.000936  3.07   0.0021 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                           exp(coef) exp(-coef) lower .95 upper .95\ngroupLow Risk AML              0.356      2.808     0.178     0.712\ngroupHigh Risk AML             0.661      1.513     0.323     1.352\n`patient age`                  0.931      1.074     0.869     0.999\n`donor age`                    0.927      1.079     0.874     0.983\nFABGrade 4 Or 5 (AML only)     2.252      0.444     1.293     3.924\nprecovery                      0.374      2.674     0.193     0.725\n`patient age`:`donor age`      1.003      0.997     1.001     1.005\n\nConcordance= 0.7  (se = 0.027 )\nLikelihood ratio test= 39.9  on 7 df,   p=1e-06\nWald test            = 42.2  on 7 df,   p=5e-07\nScore (logrank) test = 47.1  on 7 df,   p=5e-08"
  },
  {
    "objectID": "coxph-model-building.html#recurrent-events",
    "href": "coxph-model-building.html#recurrent-events",
    "title": "\n7  Building Cox Proportional Hazards models\n",
    "section": "\n7.9 Recurrent Events",
    "text": "7.9 Recurrent Events\n(Adapted from Kleinbaum and Klein, Ch 8)\n\nSometimes an appropriate analysis requires consideration of recurrent events.\nA patient with arthritis may have more than one flareup. The same is true of many recurring-remitting diseases.\nIn this case, we have more than one line in the data frame, but each line may have an event.\nWe have to use a “robust” variance estimator to account for correlation of time-to-events within a patient.\n\n\n7.9.1 Bladder Cancer Data Set\nThe bladder cancer dataset from Kleinbaum and Klein contains recurrent event outcome information for eighty-six cancer patients followed for the recurrence of bladder cancer tumor after transurethral surgical excision (Byar and Green 1980). The exposure of interest is the effect of the drug treatment of thiotepa. Control variables are the initial number and initial size of tumors. The data layout is suitable for a counting processes approach.\nThis drug is still a possible choice for some patients. Another therapeutic choice is Bacillus Calmette-Guerin (BCG), a live bacterium related to cow tuberculosis.\nData dictionary\n\nVariables in the bladder dataset\n\nVariable\nDefinition\n\n\n\nid\nPatient unique ID\n\n\nstatus\nfor each time interval: 1 = recurred, 0 = censored\n\n\ninterval\n1 = first recurrence, etc.\n\n\nintime\n`tstop - tstart (all times in months)\n\n\ntstart\nstart of interval\n\n\ntstop\nend of interval\n\n\ntx\ntreatment code, 1 = thiotepa\n\n\nnum\nnumber of initial tumors\n\n\nsize\nsize of initial tumors (cm)\n\n\n\n\nThere are 85 patients and 190 lines in the dataset, meaning that many patients have more than one line.\nPatient 1 with 0 observation time was removed.\nOf the 85 patients, 47 had at least one recurrence and 38 had none.\n18 patients had exactly one recurrence.\nThere were up to 4 recurrences in a patient.\nOf the 190 intervals, 112 terminated with a recurrence and 78 were censored.\nDifferent intervals for the same patient are correlated.\n\nIs the effective sample size 47 or 112? This might narrow confidence intervals by as much as a factor of \\(\\sqrt{112/47}=1.54\\)\nWhat happens if I have 5 treatment and 5 control values and want to do a t-test and I then duplicate the 10 values as if the sample size was 20? This falsely narrows confidence intervals by a factor of \\(\\sqrt{2}=1.41\\).\n\n\nCodebladder = \n  paste0(\n    \"http://web1.sph.emory.edu/dkleinb/allDatasets\",\n    \"/surv2datasets/bladder.dta\") |&gt; \n  read_dta() |&gt; \n  as_tibble()\n\nbladder = bladder[-1,]  #remove subject with 0 observation time\nprint(bladder)\n\n\n\nCodebladder = \n  bladder |&gt; \n  mutate(\n    surv = \n      Surv(\n        time = start,\n        time2 = stop,\n        event = event,\n        type = \"counting\"))\n\nbladder.cox1 = coxph(\n  formula = surv~tx+num+size,\n  data = bladder)\n\n#results with biased variance-covariance matrix:\nsummary(bladder.cox1)\n\nCall:\ncoxph(formula = surv ~ tx + num + size, data = bladder)\n\n  n= 190, number of events= 112 \n\n        coef exp(coef) se(coef)     z Pr(&gt;|z|)    \ntx   -0.4116    0.6626   0.1999 -2.06  0.03947 *  \nnum   0.1637    1.1778   0.0478  3.43  0.00061 ***\nsize -0.0411    0.9598   0.0703 -0.58  0.55897    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     exp(coef) exp(-coef) lower .95 upper .95\ntx       0.663      1.509     0.448      0.98\nnum      1.178      0.849     1.073      1.29\nsize     0.960      1.042     0.836      1.10\n\nConcordance= 0.624  (se = 0.032 )\nLikelihood ratio test= 14.7  on 3 df,   p=0.002\nWald test            = 15.9  on 3 df,   p=0.001\nScore (logrank) test = 16.2  on 3 df,   p=0.001\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe likelihood ratio and score tests assume independence of observations within a cluster. The Wald and robust score tests do not.\n\n\nadding cluster = id\n\nIf we add cluster= id to the call to coxph, the coefficient estimates don’t change, but we get an additional column in the summary() output: robust se:\n\nCodebladder.cox2 = coxph(\n  formula = surv ~ tx + num + size,\n  cluster = id,\n  data = bladder)\n\n#unbiased though this reduces power:\nsummary(bladder.cox2)\n\nCall:\ncoxph(formula = surv ~ tx + num + size, data = bladder, cluster = id)\n\n  n= 190, number of events= 112 \n\n        coef exp(coef) se(coef) robust se     z Pr(&gt;|z|)   \ntx   -0.4116    0.6626   0.1999    0.2488 -1.65   0.0980 . \nnum   0.1637    1.1778   0.0478    0.0584  2.80   0.0051 **\nsize -0.0411    0.9598   0.0703    0.0742 -0.55   0.5799   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     exp(coef) exp(-coef) lower .95 upper .95\ntx       0.663      1.509     0.407      1.08\nnum      1.178      0.849     1.050      1.32\nsize     0.960      1.042     0.830      1.11\n\nConcordance= 0.624  (se = 0.031 )\nLikelihood ratio test= 14.7  on 3 df,   p=0.002\nWald test            = 11.2  on 3 df,   p=0.01\nScore (logrank) test = 16.2  on 3 df,   p=0.001,   Robust = 10.8  p=0.01\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not).\n\n\nrobust se is larger than se, and accounts for the repeated observations from the same individuals:\n\nCoderound(bladder.cox2$naive.var, 4)\n\n\n\n0.0400\n-0.0014\n0.0000\n\n\n-0.0014\n0.0023\n0.0007\n\n\n0.0000\n0.0007\n0.0049\n\n\n\nCoderound(bladder.cox2$var, 4)\n\n\n\n0.0619\n-0.0026\n-0.0004\n\n\n-0.0026\n0.0034\n0.0013\n\n\n-0.0004\n0.0013\n0.0055\n\n\n\n\nThese are the ratios of correct confidence intervals to naive ones:\n\nCodewith(bladder.cox2, diag(var)/diag(naive.var)) |&gt; sqrt()\n\n[1] 1.244 1.223 1.056\n\n\nWe might try dropping the non-significant size variable:\n\nCode#remove non-significant size variable:\nbladder.cox3 = bladder.cox2 |&gt; update(. ~ . - size)\nsummary(bladder.cox3)\n\nCall:\ncoxph(formula = surv ~ tx + num, data = bladder, cluster = id)\n\n  n= 190, number of events= 112 \n\n       coef exp(coef) se(coef) robust se     z Pr(&gt;|z|)   \ntx  -0.4117    0.6625   0.2003    0.2515 -1.64   0.1017   \nnum  0.1700    1.1853   0.0465    0.0564  3.02   0.0026 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    exp(coef) exp(-coef) lower .95 upper .95\ntx      0.663      1.509     0.405      1.08\nnum     1.185      0.844     1.061      1.32\n\nConcordance= 0.623  (se = 0.031 )\nLikelihood ratio test= 14.3  on 2 df,   p=8e-04\nWald test            = 10.2  on 2 df,   p=0.006\nScore (logrank) test = 15.8  on 2 df,   p=4e-04,   Robust = 10.6  p=0.005\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not).\n\n\n\nWays to check PH assumption:\n\ncloglog\nschoenfeld residuals\ninteraction with time\n\n\n\n\n\n\n\n\nGrambsch, Patricia M, and Terry M Therneau. 1994. “Proportional Hazards Tests and Diagnostics Based on Weighted Residuals.” Biometrika 81 (3): 515–26."
  },
  {
    "objectID": "parametric-survival-models.html#parametric-survival-models",
    "href": "parametric-survival-models.html#parametric-survival-models",
    "title": "\n8  Parametric survival models\n",
    "section": "\n8.1 Parametric Survival Models",
    "text": "8.1 Parametric Survival Models\n\n8.1.1 Exponential Distribution\n\nThe exponential distribution is the basic distribution for survival analysis.\n\n\\[\n\\begin{aligned}\nf(t) &= \\lambda e^{-\\lambda t}\\\\\n\\text{log}\\left\\{f(t)\\right\\} &= \\text{log}\\left\\{\\lambda\\right\\}-\\lambda t\\\\\nF(t) &= 1-e^{-\\lambda t}\\\\\nS(t)&= e^{-\\lambda t}\\\\\nH(t)&=\\text{log}\\left\\{S(t)\\right\\} = -\\lambda t\\\\\nh(t) &= \\lambda\\\\\n\\text{E}(T) &= \\lambda^{-1}\n\\end{aligned}\n\\]\n\n8.1.2 Weibull Distribution\nUsing the Kalbfleisch and Prentice (2002) notation:\n\\[\n\\begin{aligned}\nf(t)&= \\lambda p (\\lambda t)^{p-1}e^{-(\\lambda t)^p}\\\\\nF(t)&=1 - e^{-(\\lambda t)^p}\\\\\nS(t)&=e^{-(\\lambda t)^p}\\\\\nh(t)&=\\lambda p (\\lambda t)^{p-1}\\\\\nH(t)&=(\\lambda t)^p\\\\\n\\text{log}\\left\\{H(t)\\right\\} &= p \\text{log}\\left\\{\\lambda t\\right\\} = p \\text{log}\\left\\{\\lambda\\right\\} + p \\text{log}\\left\\{t\\right\\}\\\\\n\\text{E}(T) &= \\lambda^{-1} \\cdot \\Gamma\\left(1 + \\frac{1}{p}\\right)\n\\end{aligned}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nRecall from calculus:\n\n\\(\\Gamma(t) \\stackrel{\\text{def}}{=}\\int_{u=0}^{\\infty}u^{t-1}e^{-u}du\\)\n\\(\\Gamma(t) = (t-1)!\\) for integers \\(t \\in \\mathbb Z\\)\nIt is implemented by the gamma() function in R.\n\n\n\n\n\n\n\n\nHere are some Weibull density functions, with \\(\\lambda = 1\\) and \\(p\\) varying:\n\nCodelibrary(ggplot2)\nlambda = 1\nggplot() +\n  geom_function(\n    aes(col = \"0.25\"),\n    fun = \\(x) dweibull(x, shape = 0.25, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"0.5\"),\n    fun = \\(x) dweibull(x, shape = 0.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1\"),\n    fun = \\(x) dweibull(x, shape = 1, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1.5\"),\n    fun = \\(x) dweibull(x, shape = 1.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"2\"),\n    fun = \\(x) dweibull(x, shape = 2, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"5\"),\n    fun = \\(x) dweibull(x, shape = 5, scale = 1/lambda)) +\n  theme_bw() + \n  xlim(0, 2.5) +\n  ylab(\"f(t)\") +\n  theme(axis.title.y = element_text(angle=0)) +\n  theme(legend.position=\"bottom\") +\n  guides(\n    col = \n      guide_legend(\n        title = \"p\",\n        label.theme = \n          element_text(\n            size = 12)))\n\n\nDensity functions for Weibull distribution\n\n\n\n\nProperties of Weibull hazard functions\n\nWhen \\(p=1\\), the Weibull distribution simplifies to the exponential distribution\nWhen \\(p &gt; 1\\), the hazard is increasing\nWhen \\(p &lt; 1\\), the hazard is decreasing\n\nIn HW: prove these properties\nThis distribution provides more flexibility than the exponential.\nHere are some Weibull hazard functions, with \\(\\lambda = 1\\) and \\(p\\) varying:\n\nCodelibrary(ggplot2)\nlibrary(eha)\nlambda = 1\n\nggplot() +\n  geom_function(\n    aes(col = \"0.25\"),\n    fun = \\(x) hweibull(x, shape = 0.25, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"0.5\"),\n    fun = \\(x) hweibull(x, shape = 0.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1\"),\n    fun = \\(x) hweibull(x, shape = 1, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1.5\"),\n    fun = \\(x) hweibull(x, shape = 1.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"2\"),\n    fun = \\(x) hweibull(x, shape = 2, scale = 1/lambda)) +\n  theme_bw() + \n  xlim(0, 2.5) + \n  ylab(\"h(t)\") +\n  theme(axis.title.y = element_text(angle=0)) +\n  theme(legend.position=\"bottom\") +\n  guides(\n    col = \n      guide_legend(\n        title = \"p\",\n        label.theme = \n          element_text(\n            size = 12)))\n\n\nHazard functions for Weibull distribution\n\n\n\n\n\nCodelibrary(ggplot2)\nlambda = 1\n\nggplot() +\n  geom_function(\n    aes(col = \"0.25\"),\n    fun = \\(x) pweibull(lower = FALSE, x, shape = 0.25, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"0.5\"),\n    fun = \\(x) pweibull(lower = FALSE, x, shape = 0.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1\"),\n    fun = \\(x) pweibull(lower = FALSE, x, shape = 1, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1.5\"),\n    fun = \\(x) pweibull(lower = FALSE, x, shape = 1.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"2\"),\n    fun = \\(x) pweibull(lower = FALSE, x, shape = 2, scale = 1/lambda)) +\n  theme_bw() + \n  xlim(0, 2.5) + \n  ylab(\"S(t)\") +\n  theme(axis.title.y = element_text(angle=0)) +\n  theme(legend.position=\"bottom\") +\n  guides(\n    col = \n      guide_legend(\n        title = \"p\",\n        label.theme = \n          element_text(\n            size = 12)))\n\n\nSurvival functions for Weibull distribution\n\n\n\n\n\n8.1.3 Exponential Regression\nFor each subject \\(i\\), define a linear predictor:\n\\[\n\\begin{aligned}\n\\eta(\\boldsymbol x) &= \\beta_0 + (\\beta_1x_1 + \\dots + \\beta_p x_p)\\\\\nh(t|\\boldsymbol x) &= \\text{exp}\\left\\{\\eta(\\boldsymbol x)\\right\\}\\\\\nh_0 &\\stackrel{\\text{def}}{=} h(t|\\boldsymbol 0)\\\\\n&= \\text{exp}\\left\\{\\eta(\\boldsymbol 0)\\right\\}\\\\\n&= \\text{exp}\\left\\{\\beta_0 + (\\beta_1 \\cdot 0 + \\dots + \\beta_p \\cdot 0)\\right\\}\\\\\n&= \\text{exp}\\left\\{\\beta_0 + 0\\right\\}\\\\\n&= \\text{exp}\\left\\{\\beta_0\\right\\}\\\\\n\\end{aligned}\n\\]\nWe let the linear predictor have a constant term, and when there are no additional predictors the hazard is \\(\\lambda = \\text{exp}\\left\\{\\beta_0\\right\\}\\). This has a log link as in a generalized linear model. Since the hazard does not depend on \\(t\\), the hazards are (trivially) proportional.\n\n8.1.4 Accelerated Failure Time\nPreviously, we assumed the hazards were proportional; that is, the covariates multiplied the baseline hazard function:\n\\[\n\\begin{aligned}\nh(T=t|X=x)\n&\\stackrel{\\text{def}}{=} p(T=t|X=x,T \\ge t)\\\\\n&= h(t|X=0)\\cdot \\text{exp}\\left\\{\\eta(x)\\right\\}\\\\\n&= h(t|X=0)\\cdot \\theta(x)\\\\\n&= h_0(t)\\cdot \\theta(x)\n\\end{aligned}\n\\]\nand correspondingly,\n\\[\n\\begin{aligned}\nH(t|x)\n&= \\theta(x)H_0(t)\\\\\nS(t|x)\n&= \\text{exp}\\left\\{-H(t|x)\\right\\}\\\\\n&= \\text{exp}\\left\\{-\\theta(x)\\cdot H_0(t)\\right\\}\\\\\n&= \\left(\\text{exp}\\left\\{- H_0(t)\\right\\}\\right)^{\\theta(x)}\\\\\n&= \\left(S_0(t)\\right)^{\\theta(x)}\\\\\n\\end{aligned}\n\\]\nAn alternative modeling assumption would be \\[S(t|X=x)=S_0(t\\cdot \\theta(x))\\] where \\(\\theta(x)=\\text{exp}\\left\\{\\eta(x)\\right\\}\\), \\(\\eta(x) =\\beta_1x_1+\\cdots+\\beta_px_p\\), and \\(S_0(t)=P(T\\ge t|X=0)\\) is the base survival function.\nThen\n\\[\n\\begin{aligned}\nE(T|X=x)\n&= \\int_{t=0}^{\\infty} S(t|x)dt\\\\\n&= \\int_{t=0}^{\\infty} S_0(t\\cdot \\theta(x))dt\\\\\n&= \\int_{u=0}^{\\infty} S_0(u)du \\cdot \\theta(x)^{-1}\\\\\n&= \\theta(x)^{-1} \\cdot \\int_{u=0}^{\\infty} S_0(u)du\\\\\n&= \\theta(x)^{-1} \\cdot \\text{E}(T|X=0)\\\\\n\\end{aligned}\n\\] So the mean of \\(T\\) given \\(X=x\\) is the baseline mean divided by \\(\\theta(x) = \\text{exp}\\left\\{\\eta(x)\\right\\}\\).\nThis modeling strategy is called an accelerated failure time model, because covariates cause uniform acceleration (or slowing) of failure times.\nAdditionally:\n\\[\n\\begin{aligned}\nH(t|x) &= H_0(\\theta(x)\\cdot t)\\\\\nh(t|x) &= \\theta(x) \\cdot h_0(\\theta(x)\\cdot t)\n\\end{aligned}\n\\]\nIf the base distribution is exponential with parameter \\(\\lambda\\) then\n\\[\n\\begin{aligned}\nS(t|x)\n&= \\text{exp}\\left\\{-\\lambda \\cdot t \\theta(x)\\right\\}\\\\\n&= [\\text{exp}\\left\\{-\\lambda t\\right\\}]^{\\theta(x)}\\\\\n\\end{aligned}\n\\]\nwhich is an exponential model with base hazard multiplied by \\(\\theta(x)\\), which is also the proportional hazards model.\n\nIn terms of the log survival time \\(Y=\\text{log}\\left\\{T\\right\\}\\) the model can be written as\n\\[\n\\begin{aligned}\nY&=\\alpha-\\eta+W\\\\\n\\alpha&= -\\text{log}\\left\\{\\lambda\\right\\}\n\\end{aligned}\n\\]\nwhere \\(W\\) has the extreme value distribution. The estimated parameter \\(\\lambda\\) is the intercept and the other coefficients are those of \\(\\eta\\), which will be the opposite sign of those for coxph.\n\nFor a Weibull distribution, the hazard function and the survival function are\n\\[\n\\begin{aligned}\nh(t)&=\\lambda p (\\lambda t)^{p-1}\\\\\nS(t)&=e^{-(\\lambda t)^p}\n\\end{aligned}\n\\]\nWe can construct a proportional hazards model by using a linear predictor \\(\\eta_i\\) without constant term and letting \\(\\theta_i=e^{\\eta_i}\\) we have\n\\[\n\\begin{aligned}\nh(t)&=\\lambda p (\\lambda t)^{p-1}\\theta_i\n\\end{aligned}\n\\]\nA distribution with \\(h(t)=\\lambda p (\\lambda t)^{p-1}\\theta_i\\) is a Weibull distribution with parameters \\(\\lambda^*=\\lambda \\theta_i^{1/p}\\) and \\(p\\) so the survival function is\n\\[\n\\begin{aligned}\nS^*(t)&=e^{-(\\lambda^* t)^p}\\\\\n&=e^{-(\\lambda \\theta^{1/p} t)^p}\\\\\n&= S(t\\theta^{1/p})\n\\end{aligned}\n\\]\nso this is also an accelerated failure time model.\n\nIn terms of the log survival time \\(Y=\\text{log}\\left\\{T\\right\\}\\) the model can be written as\n\\[\n\\begin{aligned}\nY&=\\alpha-\\sigma\\eta+\\sigma W\\\\\n\\alpha&= -\\text{log}\\left\\{\\lambda\\right\\}\\\\\n\\sigma &= 1/p\n\\end{aligned}\n\\]\nwhere \\(W\\) has the extreme value distribution. The estimated parameter \\(\\lambda\\) is the intercept and the other coefficients are those of \\(\\eta\\), which will be the opposite sign of those for coxph.\n\nThese AFT models are log-linear, meaning that the linear predictor has a log link. The exponential and the Weibull are the only log-linear models that are simultaneously proportional hazards models. Other parametric distributions can be used for survival regression either as a proportional hazards model or as an accelerated failure time model.\n\n8.1.5 Dataset: Leukemia treatments\nRemission survival times on 42 leukemia patients, half on new treatment, half on standard treatment.\nThis is the same data as the drug6mp data from KMsurv, but with two other variables and without the pairing.\n\nCodelibrary(haven)\nlibrary(survival)\nanderson = \n  paste0(\n    \"http://web1.sph.emory.edu/dkleinb/allDatasets\",\n    \"/surv2datasets/anderson.dta\") |&gt; \n  read_dta() |&gt; \n  mutate(\n    status = status |&gt; \n      case_match(\n        1 ~ \"relapse\",\n        0 ~ \"censored\"\n      ),\n    sex = sex |&gt; \n      case_match(\n        0 ~ \"female\",\n        1 ~ \"male\"\n      ),\n    \n    rx = rx |&gt; \n      case_match(\n        0 ~ \"new\",\n        1 ~ \"standard\"\n      ),\n    \n    surv = Surv(time = survt,event = (status == \"relapse\"))\n  ) \n\nprint(anderson)\n\n\nCox semi-parametric model\n\nCodeanderson.cox0 = coxph(\n  formula = surv ~ rx,\n  data = anderson)\nsummary(anderson.cox0)\n\nCall:\ncoxph(formula = surv ~ rx, data = anderson)\n\n  n= 42, number of events= 30 \n\n             coef exp(coef) se(coef)      z  Pr(&gt;|z|)    \nrxstandard 1.5721    4.8169   0.4124 3.8122 0.0001378 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n           exp(coef) exp(-coef) lower .95 upper .95\nrxstandard    4.8169     0.2076    2.1465    10.809\n\nConcordance= 0.69  (se = 0.041 )\nLikelihood ratio test= 16.35  on 1 df,   p=5e-05\nWald test            = 14.53  on 1 df,   p=1e-04\nScore (logrank) test = 17.25  on 1 df,   p=3e-05\n\n\nWeibull parametric model\n\nCodeanderson.weib &lt;- survreg(\n  formula = surv ~ rx,\n  data = anderson,\n  dist = \"weibull\")\nsummary(anderson.weib)\n\n\nCall:\nsurvreg(formula = surv ~ rx, data = anderson, dist = \"weibull\")\n              Value Std. Error      z        p\n(Intercept)  3.5157     0.2518 13.963  &lt; 2e-16\nrxstandard  -1.2673     0.3106 -4.080 4.51e-05\nLog(scale)  -0.3117     0.1473 -2.116   0.0343\n\nScale= 0.7322 \n\nWeibull distribution\nLoglik(model)= -106.6   Loglik(intercept only)= -116.4\n    Chisq= 19.65 on 1 degrees of freedom, p= 9.3e-06 \nNumber of Newton-Raphson Iterations: 5 \nn= 42 \n\n\nExponential parametric model\n\nCodeanderson.exp &lt;- survreg(\n  formula = surv ~ rx,\n  data = anderson,\n  dist = \"exp\")\nsummary(anderson.exp)\n\n\nCall:\nsurvreg(formula = surv ~ rx, data = anderson, dist = \"exp\")\n              Value Std. Error      z        p\n(Intercept)  3.6861     0.3333 11.058  &lt; 2e-16\nrxstandard  -1.5266     0.3984 -3.832 0.000127\n\nScale fixed at 1 \n\nExponential distribution\nLoglik(model)= -108.5   Loglik(intercept only)= -116.8\n    Chisq= 16.49 on 1 degrees of freedom, p= 4.9e-05 \nNumber of Newton-Raphson Iterations: 4 \nn= 42 \n\n\nDiagnostic - complementary log-log survival plot\n\nCodelibrary(survminer)\nsurvfit(\n  formula = surv ~ rx,\n  data = anderson) |&gt; \n  ggsurvplot(fun = \"cloglog\")\n\n\n\n\nIf the cloglog plot is linear, then a Weibull model may be ok."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Casella, George, and Roger Berger. 2002. Statistical Inference.\n2nd ed. Brooks/cole.\n\n\nDobson, Annette J, and Adrian G Barnett. 2018. An Introduction to\nGeneralized Linear Models. 4th ed. CRC press. https://doi.org/10.1201/9781315182780.\n\n\nGrambsch, Patricia M, and Terry M Therneau. 1994. “Proportional\nHazards Tests and Diagnostics Based on Weighted Residuals.”\nBiometrika 81 (3): 515–26.\n\n\nMcLachlan, Geoffrey J, and Thriyambakam Krishnan. 2007. The EM\nAlgorithm and Extensions. 2nd ed. John Wiley & Sons. https://doi.org/10.1002/9780470191613.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, and Charles E\nMcCulloch. 2012. Regression Methods in Biostatistics: Linear,\nLogistic, Survival, and Repeated Measures Models. 2nd ed. Springer.\nhttps://doi.org/10.1007/978-1-4614-1353-0."
  },
  {
    "objectID": "probability.html#random-variables",
    "href": "probability.html#random-variables",
    "title": "Appendix A — Probability",
    "section": "A.1 Random variables",
    "text": "A.1 Random variables\n\nA.1.1 Types of random variables\n\nDefinition A.1 (binary variable) A binary variable is a random variable which has only two possible values in its range."
  },
  {
    "objectID": "probability.html#characteristics-of-probability-distributions",
    "href": "probability.html#characteristics-of-probability-distributions",
    "title": "Appendix A — Probability",
    "section": "A.2 Characteristics of probability distributions",
    "text": "A.2 Characteristics of probability distributions\n\nDefinition A.2 (Density function) The density function \\(f(t)\\) for a random variable \\(T\\) at value \\(t\\) can be defined as the derivative of the cumulative probability function \\(P(T\\le t)\\); that is:\n\\[f(t) \\stackrel{\\text{def}}{=}\\frac{d}{dt} \\Pr(T\\le t)\\]\n\n\nDefinition A.3 (Hazard function) The hazard function for a random variable \\(T\\) at value \\(t\\) is the conditional density of \\(T\\) at \\(t\\), given \\(T\\ge t\\); that is:\n\\[h(t) \\stackrel{\\text{def}}{=}p(T=t|T\\ge t)\\]\nIf \\(T\\) represents the time at which an event occurs, then \\(h(t)\\) is the probability that the event occurs at time \\(t\\), given that it has not occurred prior to time \\(t\\).\n\n\nDefinition A.4 (Variance) The variance of a random variable \\(X\\) is the expectation of the squared difference between \\(X\\) and \\(\\mathbb{E}\\left[X\\right]\\); that is:\n\\[\n\\text{Var}\\left(X\\right) \\stackrel{\\text{def}}{=}\\mathbb{E}\\left[(X-\\mathbb{E}\\left[X\\right])^2\\right]\n\\]\n\n\nTheorem A.1 (Alternative expression for variance) \\[\\text{Var}\\left(X\\right)=\\mathbb{E}\\left[X^2\\right] - \\left(\\mathbb{E}\\left[X\\right]\\right)^2\\]\n\nProof. By linearity of expectation, we have:\n\\[\n\\begin{aligned}\n\\text{Var}\\left(X\\right)\n&\\stackrel{\\text{def}}{=}\\mathbb{E}\\left[(X-\\mathbb{E}\\left[X\\right])^2\\right]\\\\\n&=\\mathbb{E}\\left[X^2 - 2X\\mathbb{E}\\left[X\\right] + \\left(\\mathbb{E}\\left[X\\right]\\right)^2\\right]\\\\\n&=\\mathbb{E}\\left[X^2\\right] - \\mathbb{E}\\left[2X\\mathbb{E}\\left[X\\right]\\right] + \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[X\\right]\\right)^2\\right]\\\\\n&=\\mathbb{E}\\left[X^2\\right] - 2\\mathbb{E}\\left[X\\right]\\mathbb{E}\\left[X\\right] + \\left(\\mathbb{E}\\left[X\\right]\\right)^2\\\\\n&=\\mathbb{E}\\left[X^2\\right] - \\left(\\mathbb{E}\\left[X\\right]\\right)^2\\\\\n\\end{aligned}\n\\]\n\n\n\nDefinition A.5 (Precision) The precision of a random variable \\(X\\), often denoted \\(\\tau(X)\\), \\(\\tau_X\\), or shorthanded as \\(\\tau\\), is the inverse of that random variable’s variance; that is:\n\\[\\tau(X) \\stackrel{\\text{def}}{=}\\left(\\text{Var}\\left(X\\right)\\right)^{-1}\\]"
  },
  {
    "objectID": "probability.html#key-probability-distributions",
    "href": "probability.html#key-probability-distributions",
    "title": "Appendix A — Probability",
    "section": "A.3 Key probability distributions",
    "text": "A.3 Key probability distributions\n\nDefinition A.6 (Bernoulli distribution) The Bernoulli distribution family for a random variable \\(X\\) is defined as:\n\\[\\Pr(X=x) = \\left\\{{\\pi, x=1}\\atop{1-\\pi, x=0}\\right.\\]"
  },
  {
    "objectID": "estimation.html#estimands-estimates-and-estimators",
    "href": "estimation.html#estimands-estimates-and-estimators",
    "title": "Appendix B — Estimation",
    "section": "B.1 Estimands, estimates, and estimators",
    "text": "B.1 Estimands, estimates, and estimators\n\n\nDefinition B.1 (Estimand) An estimand is an unknown quantity whose value we want to know.\n\nExample B.1 (Mean height of students) If we are trying to determine the mean height of students at our school, then the population mean is our estimand.\n\nIn statistical contexts, most estimands are parameters of probabilistic models, or functions of model parameters.\n\n\n\n\n\n\nNotation for estimands\n\n\n\nModel paramaters and other estimands are often symbolized using lower-case Greek letters: \\(\\alpha, \\beta, \\gamma, \\delta\\), etc.\n\n\n\n \n\nDefinition B.2 (Estimate/estimated value) In statistics, an estimate or estimated value is an informed guess of an estimand’s value, based on observed data.\n\nExample B.2 (Mean height of students) Suppose we measure the heights of 50 random students from our school, and the sample mean was 175cm. We might use 175cm as an estimate of the population mean.\n\n\n\n\nDefinition B.3 (Estimator) An estimator is a function \\(\\hat\\theta(X_1,...X_n)\\), whose input is a set of random variables \\(X_1,...,X_n\\), and whose corresponding observed value \\(\\hat\\theta(x_1,...x_n)\\) is used as an estimate.\n\n\n\n\n\n\n\nNotation for estimators\n\n\n\nEstimators are often symbolized by placing a ^ (“hat”) symbol on top of the corresponding estimand; for example, \\(\\hat\\theta\\).\n\n\n\nExample B.3 (Mean height of students) If we want to estimate the mean height of students at our university, which we will represent as \\(\\mu\\), we might measure the heights of \\(n= 50\\) randomly sampled students as random variables \\(X_1,...,X_n\\). Then we could use the function\n\\[\\hat\\mu(X_1,...,X_n) = \\frac{1}{n} \\sum_{i=1}^n X_i \\stackrel{\\text{def}}{=}\\bar X\\]\nas an estimator to produce an estimate \\(\\hat\\mu = \\bar x\\) of \\(\\mu\\).\nAnother estimator would be just the height of the first student sampled:\n\\[\\hat\\mu^{(2)}(X_1,...,X_n) = X_1\\]\nA third possible estimator would be the mean of all sampled students’ heights, except for the two most extreme; that is, if we re-order the observations \\(X_{(1)} = \\min_{i\\in 1:n} X_i\\), \\(X_{(2)} = \\min_{i\\in \\{1:n\\} - \\arg X_{(1)}} X_i\\), …, \\(X_{(n)} = \\max_{i\\in 1:n} X_i\\), then we could define the estimator:\n\\[\\hat\\mu^{(3)}(X_1,...,X_n) = \\frac{1}{n}\\sum_{i=2}^{n-1} X_{(i)}\\]\nWhich of these estimators is best? It depends on how we evaluate them (see Section B.2 below).\n\n\n\n\n\n\n\nContrasting estimands, estimates, and estimators\n\n\n\nIt’s helpful to keep in mind the mathematical type of each estimation concept:\n\nestimands are numbers (or vector of numbers)\nestimates are also numbers (or vectors)\nestimators are functions of random variables, so they are also random variables"
  },
  {
    "objectID": "estimation.html#sec-est-accuracy",
    "href": "estimation.html#sec-est-accuracy",
    "title": "Appendix B — Estimation",
    "section": "B.2 Accuracy of estimators",
    "text": "B.2 Accuracy of estimators\nTo determine which estimator is best, we need to define best. “Accuracy” is usually most important; easy computation is usually secondary.\n\nDefinition B.8 (Accuracy) The accuracy of an estimator for a given estimand does not have a consensus formal definition, but all of the usual candidates are related to the distributions of the errors made by the resulting estimates.\n\nDefinition B.4 (Error) The error of an estimate \\(\\hat\\theta\\), often denoted \\(\\epsilon(\\hat\\theta)\\), is the difference between the estimate and its estimand \\(\\theta\\); that is:\n\\[\\epsilon(\\hat\\theta) \\stackrel{\\text{def}}{=}\\hat\\theta- \\theta\\]\n\nSome frequently-used measures of accuracy include:\n\nDefinition B.5 (Mean squared error) The mean squared error of an estimator \\(\\hat\\theta\\), denoted \\(\\text{MSE}\\left(\\hat\\theta\\right)\\), is the expectation of the square of the error1:\n\\[\\text{MSE}\\left(\\hat\\theta\\right) \\stackrel{\\text{def}}{=}\\mathbb{E}\\left[(\\epsilon(\\hat\\theta))^2\\right]\\]\n\n\nDefinition B.6 (Mean absolute error) The mean absolute error of an estimator is the expectation of the absolute value of the error:\n\\[\n\\text{MAE}\\left(\\hat\\theta\\right) \\stackrel{\\text{def}}{=}\\mathbb{E}\\left[\\left|\\epsilon(\\hat\\theta)\\right|\\right]\\]\n\n\nDefinition B.7 (Bias) The bias of an estimator \\(\\hat\\theta\\) for an estimand \\(\\theta\\) is the expected value of the error:\n\\[\\text{Bias}\\left(\\hat\\theta\\right) \\stackrel{\\text{def}}{=}\\mathbb{E}\\left[\\epsilon(\\hat\\theta)\\right] \\tag{B.1}\\]\n\n\nTheorem B.1 (Expressions for bias) The following expressions are equivalent to the definition of bias:\n\\[\n\\begin{aligned}\n\\text{Bias}\\left(\\hat\\theta\\right)\n&\\stackrel{\\text{def}}{=}\\mathbb{E}\\left[\\epsilon(\\hat\\theta)\\right]\\\\\n&= \\mathbb{E}\\left[\\hat\\theta- \\theta\\right]\\\\\n&=\\mathbb{E}\\left[\\hat\\theta\\right] - \\mathbb{E}\\left[\\theta\\right]\\\\\n&=\\mathbb{E}\\left[\\hat\\theta\\right] - \\theta\n\\end{aligned}\n\\]\nThe third equality is by the linearity of expectation.\n\n\n\nTheorem B.2 (MSE = Bias2 + Variance) For any one-dimensional estimator \\(\\hat\\theta\\):\n\\[\\text{MSE}\\left(\\hat\\theta\\right) = \\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2 + \\text{Var}\\left(\\hat\\theta\\right)\\]\n\nProof. Let’s start by expanding each term of the right-hand side:\n\\[\n\\begin{aligned}\n\\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2\n&=\\left(\\mathbb{E}\\left[\\hat\\theta\\right] - \\theta\\right)^2\\\\\n&=\\left(\\mathbb{E}\\left[\\hat\\theta\\right]\\right)^2 - 2\\mathbb{E}\\left[\\hat\\theta\\right]\\theta+\\theta^2\\\\\n\\end{aligned}\n\\]\n\\[\\text{Var}\\left(\\hat\\theta\\right) = \\mathbb{E}\\left[\\hat\\theta^2\\right] - \\left(\\mathbb{E}\\left[\\hat\\theta\\right]\\right)^2\\\\\\]\nNow, add them together and simplify:\n\\[\n\\begin{aligned}\n\\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2 + \\text{Var}\\left(\\hat\\theta\\right)\n&=\\left(\\mathbb{E}\\left[\\hat\\theta\\right]\\right)^2 - 2\\mathbb{E}\\left[\\hat\\theta\\right]\\theta+\\theta^2 + \\mathbb{E}\\left[\\hat\\theta^2\\right] - \\left(\\mathbb{E}\\left[\\hat\\theta\\right]\\right)^2\\\\\n&=\\mathbb{E}\\left[\\hat\\theta^2\\right] - 2\\mathbb{E}\\left[\\hat\\theta\\right]\\theta+\\theta^2\\\\\n\\end{aligned}\n\\]\nNow let’s expand the left-hand side to reach the same expression:\n\\[\n\\begin{aligned}\n\\text{MSE}\\left(\\hat\\theta\\right)\n&= \\mathbb{E}\\left[(\\epsilon(\\hat\\theta))^2\\right]\\\\\n&= \\mathbb{E}\\left[(\\hat\\theta- \\theta)^2\\right]\\\\\n&= \\mathbb{E}\\left[\\hat\\theta^2 - 2\\hat\\theta\\theta- \\theta^2\\right]\\\\\n&=\\mathbb{E}\\left[\\hat\\theta^2\\right] - \\mathbb{E}\\left[2\\hat\\theta\\theta\\right]+\\mathbb{E}\\left[\\theta^2\\right]\\\\\n&=\\mathbb{E}\\left[\\hat\\theta^2\\right] - 2\\mathbb{E}\\left[\\hat\\theta\\right]\\theta+\\theta^2\\\\\n\\end{aligned}\n\\]\n\\(\\text{MSE}\\left(\\hat\\theta\\right)\\) and \\(\\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2 + \\text{Var}\\left(\\hat\\theta\\right)\\) both equal \\(\\mathbb{E}\\left[\\hat\\theta^2\\right] - 2\\mathbb{E}\\left[\\hat\\theta\\right]\\theta+\\theta^2\\). Equality is transitive, so \\(\\text{MSE}\\left(\\hat\\theta\\right)\\) and \\(\\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2 + \\text{Var}\\left(\\hat\\theta\\right)\\) are equal to each other:\n\\[\\text{MSE}\\left(\\hat\\theta\\right) = \\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2 + \\text{Var}\\left(\\hat\\theta\\right)\\]\n\n\n\nUnbiased estimators\n\nDefinition B.9 (unbiased estimator) An estimator \\(\\hat\\theta\\) is unbiased if \\(\\text{Bias}\\left(\\hat\\theta\\right) = 0\\).\n\n\nTheorem B.3 (properties of unbiased estimators) If \\(\\hat\\theta\\) is unbiased, then:\n\\[\\mathbb{E}\\left[\\hat\\theta\\right] = \\theta \\tag{B.2}\\] \\[\\text{MSE}\\left(\\hat\\theta\\right) = \\text{Var}\\left(\\hat\\theta\\right) \\tag{B.3}\\]\n\nProof. \n\n\nIf \\(\\hat\\theta\\) is unbiased, then:\n\n\\[\n\\begin{aligned}\n\\text{Bias}\\left(\\hat\\theta\\right) &= 0\\\\\n\\mathbb{E}\\left[\\hat\\theta\\right] - \\theta &= 0\\\\\n\\mathbb{E}\\left[\\hat\\theta\\right] &= \\theta\n\\end{aligned}\n\\]\n\nIf \\(\\hat\\theta\\) is unbiased, then:\n\n\\[\n\\begin{aligned}\n\\text{MSE}\\left(\\hat\\theta\\right)\n&= \\mathbb{E}\\left[\\left(\\epsilon(\\hat\\theta)\\right)^2\\right]\\\\\n&= \\mathbb{E}\\left[\\left(\\hat\\theta- \\theta\\right)^2\\right]\\\\\n&= \\mathbb{E}\\left[\\left(\\hat\\theta- \\mathbb{E}\\left[\\hat\\theta\\right]\\right)^2\\right]\\\\\n&\\stackrel{\\text{def}}{=}\\text{Var}\\left(\\hat\\theta\\right)\n\\end{aligned}\n\\]\n(Alternative proof of 2) We could have started from Theorem B.2 instead:\n\\[\n\\begin{aligned}\n\\text{MSE}\\left(\\hat\\theta\\right)\n&= \\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2 + \\text{Var}\\left(\\hat\\theta\\right)\\\\\n&= \\left(0\\right)^2 + \\text{Var}\\left(\\hat\\theta\\right)\\\\\n&= 0 + \\text{Var}\\left(\\hat\\theta\\right)\\\\\n&= \\text{Var}\\left(\\hat\\theta\\right)\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "estimation.html#footnotes",
    "href": "estimation.html#footnotes",
    "title": "Appendix B — Estimation",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Does_exactly_what_it_says_on_the_tin↩︎"
  },
  {
    "objectID": "intro-MLEs.html#maximum-likelihood-inference-for-univariate-gaussian-models",
    "href": "intro-MLEs.html#maximum-likelihood-inference-for-univariate-gaussian-models",
    "title": "Appendix C — Introduction to Maximum Likelihood Inference",
    "section": "\nC.1 Maximum likelihood inference for univariate Gaussian models",
    "text": "C.1 Maximum likelihood inference for univariate Gaussian models\nSuppose \\(X_{1},\\ldots,X_{n} \\sim_{iid}N\\left( \\mu,\\ \\sigma^{2} \\right)\\). Let \\(X = \\left( X_{1},\\ldots,X_{n} \\right)^{\\top}\\) be these random variables in vector format. Let \\(x_{i}\\) and \\(x\\) denote the corresponding observed data. Let \\(\\theta = \\left( \\mu,\\sigma^{2} \\right)^{\\top}\\) be the vector of parameters. Let \\(\\Theta\\) denote the parameters as a random vector.\nThen the log-likelihood \\(\\ell \\stackrel{\\text{def}}{=}\\ell(X;\\theta) \\stackrel{\\text{def}}{=}p\\left( X = x \\mid \\Theta = \\theta \\right)\\) is:\n\\[\n\\begin{aligned}\n\\ell\n&\\propto - \\frac{n}{2}\\text{log}\\left\\{\\sigma^{2}\\right\\} - \\frac{1}{2}\\sum_{i = 1}^{n}\\frac{\\left( x_{i} - \\mu \\right)^{2}}{\\sigma^{2}}\\\\\n&= - \\frac{n}{2}\\text{log}\\left\\{\\sigma^{2}\\right\\} - \\frac{1}{2\\sigma^{2}}\\sum_{i = 1}^{n}{x_{i}^{2} - 2x_{i}\\mu + \\mu^{2}}\n\\end{aligned}\n\\]\n\nC.1.1 MLE of \\(\\mu\\):\nThen:\n\\[\\frac{d\\ell}{d\\mu} = - \\frac{1}{2}\\sum_{i = 1}^{n}\\frac{- 2\\left( x_{i} - \\mu \\right)}{\\sigma^{2}}\\]\n\\[= \\frac{1}{\\sigma^{2}}\\left\\lbrack \\left( \\sum_{i = 1}^{n}x_{i} \\right) - n\\mu \\right\\rbrack\\]\nIf \\(\\frac{d\\ell}{d\\mu} = 0\\), then \\(\\mu = \\overline{x} \\stackrel{\\text{def}}{=}\\frac{1}{n}\\sum_{i = 1}^{n}x_{i}\\).\n\\[\\frac{d^{2}\\ell}{(d\\mu)^{2}} = \\frac{- n}{\\sigma^{2}} &lt; 0\\]\nSo \\({\\widehat{\\mu}}_{ML} = \\overline{x}\\).\n\nC.1.2 MLE of \\(\\sigma^{2}\\)\n\n\n\n\n\n\n\nReparametrizing the Gaussian distribution\n\n\n\nWhen solving for \\({\\widehat{\\sigma}}_{ML}\\), you can treat \\(\\sigma^{2}\\) as an atomic variable (don’t differentiate with respect to \\(\\sigma\\) or things get messy). In fact, you can replace \\(\\sigma^{2}\\) with \\(1/\\tau\\) and differentiate with respect to \\(\\tau\\) instead, and the process might be even easier.\n\n\n\\[\\frac{d\\ell}{d\\sigma^{2}} = \\frac{d}{d\\sigma^{2}}\\left( - \\frac{n}{2}\\text{log}\\left\\{\\sigma^{2}\\right\\} - \\frac{1}{2}\\sum_{i = 1}^{n}\\frac{\\left( x_{i} - \\mu \\right)^{2}}{\\sigma^{2}} \\right)\\ \\]\n\\[= - \\frac{n}{2}\\left( \\sigma^{2} \\right)^{- 1} + \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2}\\]\nIf \\(\\frac{d\\ell}{d\\sigma^{2}} = 0\\), then:\n\\[\\frac{n}{2}\\left( \\sigma^{2} \\right)^{- 1} = \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2}\\]\n\\[\\sigma^{2} = \\frac{1}{n}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2}\\]\nWe plug in \\({\\widehat{\\mu}}_{ML} = \\overline{x}\\) to maximize globally (a technique called profiling):\n\\[\\sigma_{ML}^{2} = \\frac{1}{n}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}\\]\nNow:\n\\[\n\\begin{aligned}\n\\frac{d^{2}\\ell}{\\left( d\\sigma^{2} \\right)^{2}}\n&= \\frac{d}{d\\sigma^{2}}\\left\\{ - \\frac{n}{2}\\left( \\sigma^{2} \\right)^{- 1} + \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2} \\right\\}\\\\\n&= \\left\\{ - \\frac{n}{2}\\frac{d}{d\\sigma^{2}}\\left( \\sigma^{2} \\right)^{- 1} + \\frac{1}{2}\\frac{d}{d\\sigma^{2}}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2} \\right\\}\\\\\n&= \\left\\{ \\frac{n}{2}\\left( \\sigma^{2} \\right)^{- 2} - \\left( \\sigma^{2} \\right)^{- 3}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2} \\right\\}\\\\\n&= \\left( \\sigma^{2} \\right)^{- 2}\\left\\{ \\frac{n}{2} - \\left( \\sigma^{2} \\right)^{- 1}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2} \\right\\}\n\\end{aligned}\n\\]\nEvaluated at \\(\\mu = \\overline{x},\\sigma^{2} = \\frac{1}{n}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}\\), we have:\n\\[\n\\begin{aligned}\n\\frac{d^{2}\\ell}{\\left( d\\sigma^{2} \\right)^{2}}\n&= \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}\\left\\{ \\frac{n}{2} - \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 1}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2} \\right\\}\\\\\n&= \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}\\left\\{ \\frac{n}{2} - \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 1}n{\\widehat{\\sigma}}^{2} \\right\\}\\\\\n&= \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}\\left\\{ \\frac{n}{2} - n \\right\\}\\\\\n&= \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}n\\left\\{ \\frac{1}{2} - 1 \\right\\}\\\\\n&= \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}n\\left( - \\frac{1}{2} \\right) &lt; 0\n\\end{aligned}\n\\]\nFinally, we have:\n\\[\n\\begin{aligned}\n\\frac{d^{2}\\ell}{d\\mu\\ d\\sigma^{2}}\n&= \\frac{d}{d\\mu}\\left\\{ - \\frac{n}{2}\\left( \\sigma^{2} \\right)^{- 1} + \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2} \\right\\}\\\\\n&= \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\frac{d}{d\\mu}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2}\\\\\n&= \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}{- 2(x_{i} - \\mu)}\\\\\n&= - \\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}{(x_{i} - \\mu)}\n\\end{aligned}\n\\]\nEvaluated at \\(\\mu = \\widehat{\\mu} = \\overline{x},\\sigma^{2} = {\\widehat{\\sigma}}^{2} = \\frac{1}{n}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}\\), we have:\n\\[\\frac{d^{2}\\ell}{d\\mu\\ d\\sigma^{2}} = - \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}\\left( n\\overline{x} - n\\overline{x} \\right) = 0\\]\n\nC.1.3 Covariance matrix of MLEs\nThe score function\nLet \\(\\theta\\) be the vector of all parameters; here, \\(\\theta = \\left( \\mu,\\sigma^{2} \\right)^{\\top}\\).\nLet \\(\\ell^{'}(x,\\theta) \\stackrel{\\text{def}}{=}\\frac{d}{d\\theta}\\ell(x,\\theta) = \\left( \\begin{array}{r} \\frac{d}{d\\mu}\\ell(\\theta;x) \\\\ \\frac{d}{d\\sigma^{2}}\\ell(\\theta;x) \\end{array} \\right) = \\left( \\begin{array}{r} \\ell_{\\mu}^{'}(\\theta;x) \\\\ \\ell_{\\sigma^{2}}^{'}(\\theta;x) \\end{array} \\right)\\).\n\\(\\ell^{'}(x,\\theta)\\) is the function we set equal to 0 and solve to find the MLE:\n\\[{\\widehat{\\theta}}_{ML} = \\left\\{ \\theta:\\ell^{'}(x,\\theta) = 0 \\right\\}\\]\nThe function \\(\\ell^{'}(x,\\theta)\\) is so central that it has its own name, the “score” or “gradient” function. Statisticians also often skip writing the arguments \\((x,\\theta)\\), so \\(\\ell^{'} \\stackrel{\\text{def}}{=}\\ell^{'}(x,\\theta)\\).1 Some statisticians use \\(U\\) or \\(S\\) instead of \\(\\ell^{'}\\). I prefer \\(\\ell^{'}\\). Why use up extra letters?\nThe (Fisher) (expected) information matrix\nThe variance of \\(\\ell^{'}(x,\\theta)\\), \\({Cov}\\left\\{ \\ell^{'}(x,\\theta) \\right\\}\\), is also very important; we call it the “expected information matrix”, “Fisher information matrix”, or just “information matrix”, and we represent it using the symbol \\(\\mathfrak{I}\\) (\\frakturI in Unicode, \\mathfrak{I} in LaTeX).\nReview of variances and covariances\nVariances and covariances of one-dimensional random variables\nFor a one-dimensional random variables \\(X\\),\n\\[{Var}(X) \\stackrel{\\text{def}}{=}Ε\\left\\lbrack \\left( X - Ε\\lbrack X\\rbrack \\right)^{2} \\right\\rbrack = Ε\\left\\lbrack X^{2} \\right\\rbrack - \\left( Ε\\lbrack X\\rbrack \\right)^{2}\\]\nFor any two-dimensional random variables, \\(X,Y\\):\n\\[{Cov}(X,Y) = E\\left\\lbrack (X - ΕX)(Y - ΕY) \\right\\rbrack = Ε\\lbrack XY\\rbrack - E\\lbrack X\\rbrack E\\lbrack Y\\rbrack\\]\nTherefore, \\({Var}{(X)} = {Cov}(X,X) = Ε\\lbrack XX\\rbrack - E\\lbrack X\\rbrack E\\lbrack X\\rbrack = Ε\\left\\lbrack X^{2} \\right\\rbrack - \\left( E\\lbrack X\\rbrack \\right)^{2}\\)\n\\[\\mathfrak{I \\stackrel{\\text{def}}{=}I(}\\theta) \\stackrel{\\text{def}}{=}{Cov}\\left( \\ell^{'}|\\theta \\right) = Ε\\left\\lbrack \\ell^{'}{\\ell^{'}}^{\\top} \\right\\rbrack - Ε\\left\\lbrack \\ell^{'} \\right\\rbrack\\ Ε\\left\\lbrack \\ell^{'} \\right\\rbrack^{\\top}\\]\nSometimes we write \\({Cov}{(X)} \\stackrel{\\text{def}}{=}{Cov}(X,X) = {Var}(X)\\).\nVariances and covariances of \\(p \\times 1\\) random vectors\nNow, for a \\(p \\times 1\\) dimensional random vector \\(X\\),\n\\[\n\\begin{aligned}\n\\text{Var}(X)\n&\\stackrel{\\text{def}}{=}\\text{Cov}(X)\\\\\n&\\stackrel{\\text{def}}{=}E\\left\\lbrack \\left( X - E\\lbrack X\\rbrack \\right)^{\\top}\\left( X - E\\lbrack X\\rbrack \\right) \\right\\rbrack\\\\\n&= E\\left\\lbrack \\left( X^{\\top} - E\\lbrack X\\rbrack^{\\top} \\right)\\left( X - E\\lbrack X\\rbrack \\right) \\right\\rbrack\\\\\n&= E\\left\\lbrack X^{\\top}X - E\\lbrack X\\rbrack^{\\top}X - X^{\\top}E\\lbrack X\\rbrack + E\\lbrack X\\rbrack^{\\top}E\\lbrack X\\rbrack \\right\\rbrack\\\\\n&= E\\left\\lbrack X^{\\top}X \\right\\rbrack - E\\lbrack X\\rbrack^{\\top}E\\lbrack X\\rbrack - {E\\lbrack X\\rbrack}^{\\top}E\\lbrack X\\rbrack + E\\lbrack X\\rbrack^{\\top}E\\lbrack X\\rbrack\\\\\n&= E\\left\\lbrack X^{\\top}X \\right\\rbrack - 2{E\\lbrack X\\rbrack}^{\\top}E\\lbrack X\\rbrack + E\\lbrack X\\rbrack^{\\top}E\\lbrack X\\rbrack\\\\\n&= E\\left\\lbrack X^{\\top}X \\right\\rbrack - {E\\lbrack X\\rbrack}^{\\top}E\\lbrack X\\rbrack\n\\end{aligned}\n\\]\nThe elements of \\(\\mathfrak{I}\\) are:\n\\[\\left\\{ \\mathfrak{I}_{ij} \\stackrel{\\text{def}}{=}{Cov}\\left( {\\ell^{'}}_{i},{\\ell^{'}}_{j} \\right) = Ε\\left\\lbrack \\ell_{i}^{'}\\ell_{j}^{'} \\right\\rbrack - Ε\\left\\lbrack {\\ell^{'}}_{i} \\right\\rbrack Ε\\left\\lbrack {\\ell^{'}}_{j} \\right\\rbrack \\right\\}\\]\nIn our motivating example, \\(i,j \\in \\left\\{ \\mu,\\sigma^{2} \\right\\}\\). Here,\n\\[\n\\begin{aligned}\nΕ[\\ell']\n&\\stackrel{\\text{def}}{=}\\int_{x \\in \\mathcal R(x)}{\\ell'(x,\\theta) p(X = x | \\theta)dx}\\\\\n&= \\int_{x\\in \\mathcal R(x)}{\\left( \\frac{d}{d\\theta}\\text{log}\\left\\{p\\left( X = x \\mid \\theta \\right)\\right\\} \\right)\\ p\\left( X = x \\mid \\theta \\right)\\ dx}\\\\\n&= \\int_{x \\in \\mathcal R(x)}{\\frac{\\frac{d}{d\\theta}p\\left( X = x \\mid \\theta \\right)}{p\\left( X = x \\mid \\theta \\right)}p\\left( X = x \\mid \\theta \\right)\\ dx\n}\\\\\n&= \\int_{x \\in \\mathcal R(x)}{\\frac{d}{d\\theta}p\\left( X = x \\mid \\theta \\right)\\ dx}\n\\end{aligned}\n\\]\nAnd similarly\n\\[Ε\\left\\lbrack \\ell^{'}{\\ell^{'}}^{\\top} \\right\\rbrack \\stackrel{\\text{def}}{=}\\int_{x \\in R(x)}^{}{\\ell^{'}(x,\\theta)\\ell^{'}(x,\\theta)^{\\top}\\ p\\left( X = x \\mid \\theta \\right)\\ dx}\\]\nNote that \\(Ε\\left\\lbrack \\ell^{'} \\right\\rbrack\\) and \\(Ε\\left\\lbrack \\ell^{'}{\\ell^{'}}^{\\top} \\right\\rbrack\\) are functions of \\(\\theta\\) but not of \\(x\\); the expectation operator removed \\(x\\).\nAlso note that for most of the distributions you are familiar with (including Gaussian, binomial, Poisson, exponential),\n\\[Ε\\left\\lbrack \\ell^{'} \\right\\rbrack = 0\\]\nSo\n\\[\\mathfrak{I =}Ε\\left\\lbrack \\ell^{'}{\\ell^{'}}^{\\top} \\right\\rbrack\\]\nMoreover, for those distributions (called the “exponential family”), we have:\n\\[\\mathfrak{I = -}Ε\\left\\lbrack \\ell^{''} \\right\\rbrack = Ε\\left\\lbrack - \\ell^{''} \\right\\rbrack\\]\n(see Dobson and Barnett 4e, §3.17), where\n\\[\\ell^{''} \\stackrel{\\text{def}}{=}\\frac{d}{d\\theta}\\ell^{'(x,\\theta)^{\\top}} = \\frac{d}{d\\theta}\\frac{d}{d\\theta^{\\top}}\\ell(x,\\theta)\\]\nis the \\(p \\times p\\) matrix whose elements are:\n\\[\\ell_{ij}^{''} \\stackrel{\\text{def}}{=}\\frac{d}{d\\theta_{i}}\\frac{d}{d\\theta_{j}}\\text{log}\\left\\{ p\\left( X = x \\mid \\theta \\right)\\right\\}\\]\n\\(\\ell^{''}\\) could be called the “Hessian” of the log-likelihood function.\nSometimes, we use \\(I(\\theta;x) \\stackrel{\\text{def}}{=}- \\ell^{''}\\) (note the standard-font “I” here). \\(I(\\theta;x)\\) is the observed information matrix (Negative Hessian).\n\n\n\n\n\n\nKey point\n\n\n\nThe asymptotics of MLEs gives us \\({\\widehat{\\theta}}_{ML} \\sim N\\left( \\theta,\\mathfrak{I}^{- 1}(\\theta) \\right)\\), approximately, for large sample sizes.\n\n\nWe can estimate \\(\\mathfrak{I}^{- 1}(\\theta)\\) by working out \\(- Ε\\left\\lbrack \\ell^{''} \\right\\rbrack\\) or \\(Ε\\left\\lbrack \\ell^{'}{\\ell^{'}}^{\\top} \\right\\rbrack\\) and plugging in \\({\\widehat{\\theta}}_{ML}\\), but sometimes we instead use \\(I\\left( {\\widehat{\\theta}}_{ML};x \\right)\\) for convenience; there are some cases where it’s provably better according to some criteria (Efron & Hinkley 1978).\nNote that later, when we are trying to find MLEs for likelihoods that we can’t easily differentiate, we will “hill-climb” using the Newton-Raphson algorithm:\n\\[\n\\begin{aligned}\n\\widehat{\\theta}\n&\\leftarrow \\widehat{\\theta} + \\left\\lbrack I\\left( \\widehat{\\theta},y \\right) \\right\\rbrack^{- 1}\\ell^{'}\\left( y,\\widehat{\\theta} \\right)\\\\\n&= \\widehat{\\theta} - \\left\\lbrack \\ell^{''}\\left( y,\\widehat{\\theta} \\right) \\right\\rbrack^{- 1}\\ell^{'}\\left( y,\\widehat{\\theta} \\right)\n\\end{aligned}\n\\]\nHere, for computational simplicity, we will sometimes use \\(\\mathfrak{I}^{- 1}(\\theta)\\) in place of \\(I\\left( \\widehat{\\theta},y \\right)\\); doing so is called “Fisher scoring” or the “method of scoring”. Note that this is the opposite of the substitution that we are making for estimating the variance of the MLE; this time we should technically use the observed information but we use the expected information instead.\nThere’s also an “empirical information matrix” (see McLachlan and Krishnan 2007).\n\\[I_{e}(\\theta,y) = \\sum_{i = 1}^{n}{\\ell_{i}^{'}\\ {\\ell_{i}^{'}}^{\\top}} - \\frac{1}{n}\\ell^{'}{\\ell^{'}}^{\\top}\\]\nwhere \\(\\ell_{i}\\) is the log-likelihood of the ith observation. Note that \\(\\ell^{'} = \\sum_{i = 1}^{n}\\ell_{i}^{'}\\).\n\\(\\frac{1}{n}I_{e}(\\theta,y)\\) is the sample equivalent of\n\\[\\mathfrak{I \\stackrel{\\text{def}}{=}I(}\\theta) \\stackrel{\\text{def}}{=}{Cov}\\left( \\ell^{'}|\\theta \\right) = Ε\\left\\lbrack \\ell^{'}{\\ell^{'}}^{\\top} \\right\\rbrack - Ε\\left\\lbrack \\ell^{'} \\right\\rbrack\\ Ε\\left\\lbrack \\ell^{'} \\right\\rbrack^{\\top}\\]\n\\[\\left\\{ \\mathfrak{I}_{jk} \\stackrel{\\text{def}}{=}{Cov}\\left( {\\ell^{'}}_{j},{\\ell^{'}}_{k} \\right) = Ε\\left\\lbrack \\ell_{j}^{'}\\ell_{k}^{'} \\right\\rbrack - Ε\\left\\lbrack {\\ell^{'}}_{j} \\right\\rbrack Ε\\left\\lbrack {\\ell^{'}}_{k} \\right\\rbrack \\right\\}\\]\n\\(I_{e}(\\theta,y)\\) is sometimes computationally easier to compute for Newton-Raphson-type maximization algorithms.\nBack to our Gaussian example:\n\\[I = \\begin{bmatrix}\n\\frac{n}{\\sigma^{2}} & 0 \\\\\n0 & \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}n\\left( - \\frac{1}{2} \\right)\n\\end{bmatrix} = \\begin{bmatrix}\na & 0 \\\\\n0 & d\n\\end{bmatrix}\\]\nSo:\n\\[I^{- 1} = \\frac{1}{ad}\\begin{bmatrix}\nd & 0 \\\\\n0 & a\n\\end{bmatrix} = \\begin{bmatrix}\n\\frac{1}{a} & 0 \\\\\n0 & \\frac{1}{d}\n\\end{bmatrix}\\]\n\\[I^{- 1} = \\begin{bmatrix}\n\\frac{{\\widehat{\\sigma}}^{2}}{n} & 0 \\\\\n0 & \\frac{{2\\left( {\\widehat{\\sigma}}^{2} \\right)}^{2}}{n}\n\\end{bmatrix}\\]\nSee Casella and Berger (2002) p322, example 7.2.12.\nTo prove it’s a maximum, need:\n\n\\(\\ell^{'} = 0\\)\nAt least one diagonal element of \\(\\mathcal{l''}\\) is negative.\nDeterminant of \\(\\mathcal{l''}\\) is positive.\n\nC.1.4 Confidence intervals for MLEs\n\nC.1.5 p-values and hypothesis tests for MLEs\n\nC.1.6 Likelihood ratio tests for MLEs\n[We haven’t gone over this yet]\nLog-likelihood ratio tests (Dobson & Barnett 5.7)\n\\[2\\left( \\mathcal{l -}\\ell_{0} \\right) \\sim \\chi^{2}(p - q)\\]\n\nC.1.7 Prediction intervals for MLEs\n\\[\\overline{X} \\in \\left\\lbrack \\widehat{\\mu} \\pm z_{1 - \\alpha\\text{/}2}\\frac{\\sigma}{m} \\right\\rbrack\\]\nWhere \\(m\\) is the sample size of the new data to be predicted (typically 1, except for binary outcomes, where it needs to be bigger for prediction intervals to make sense)\n\n\n\n\n\n\nCasella, George, and Roger Berger. 2002. Statistical Inference. 2nd ed. Brooks/cole.\n\n\nDobson, Annette J, and Adrian G Barnett. 2018. An Introduction to Generalized Linear Models. 4th ed. CRC press. https://doi.org/10.1201/9781315182780.\n\n\nMcLachlan, Geoffrey J, and Thriyambakam Krishnan. 2007. The EM Algorithm and Extensions. 2nd ed. John Wiley & Sons. https://doi.org/10.1002/9780470191613."
  },
  {
    "objectID": "intro-MLEs.html#footnotes",
    "href": "intro-MLEs.html#footnotes",
    "title": "Appendix C — Introduction to Maximum Likelihood Inference",
    "section": "",
    "text": "I might sometimes switch the order of \\(x,\\) \\(\\theta\\); this is unintentional and not meaningful.↩︎"
  }
]